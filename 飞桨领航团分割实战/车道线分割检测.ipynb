{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "VisuaDL中mIOU变化趋势或训练日志\n",
    "（请在此处插入图片）断点续训图片\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/768af9bb78f84f36891b72e50651c48b570c4174880640ceb675a4c4205db6d0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "infer/4346.png图片车道线分割效果图\n",
    "（请在此处插入图片）\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/689b50a70bb04d4c82d61206610f7165bac6a3c9c6e24f72a11b978fc0a4544c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "：[第十六届全国大学生智能车竞赛线上资格赛：车道线检测](https://aistudio.baidu.com/aistudio/competition/detail/68)比赛提交结果\n",
    "（请在此处插入图片）\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/415e83a45c564f329ed91db9e9e0e38313ec262835914118a70ab1817ab69ddb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleSeg进行车道线分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 解压数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip data/data82548/智能车数据集.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!git clone https://gitee.com/paddlepaddle/PaddleSeg.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## [准备数据集](https://gitee.com/paddlepaddle/PaddleSeg/blob/release/v2.0/docs/data_prepare.md)\n",
    "PaddleSeg目前支持CityScapes、ADE20K、Pascal VOC等数据集的加载，在加载数据集时，如若本地不存在对应数据，则会自动触发下载(除Cityscapes数据集)。\n",
    "这里可以直接使用比赛提供的脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run make_list.py\n",
    "!python make_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 类别不均衡的处理\n",
    "\n",
    "在图像分割任务中，经常出现类别分布不均匀的情况，例如：工业产品的瑕疵检测、道路提取及病变区域提取等。\n",
    "\n",
    "针对这个问题，PaddleSeg主要提供了带权的softmax loss和Lovasz loss两种解决方案。\n",
    "\n",
    "### [](https://gitee.com/paddlepaddle/PaddleSeg/blob/release/v0.6.0/docs/loss_select.md#weighted-softmax-loss)Weighted softmax loss\n",
    "\n",
    "Weighted softmax loss是按类别设置不同权重的softmax loss。\n",
    "\n",
    "通过设置`cfg.SOLVER.CROSS_ENTROPY_WEIGHT`参数进行使用。  \n",
    "默认为None. 如果设置为'dynamic'，会根据每个batch中各个类别的数目，动态调整类别权重。 也可以设置一个静态权重(list的方式)，比如有3类，每个类别权重可以设置为\\[0.1, 2.0, 0.9\\]。\n",
    "```\n",
    "SOLVER:\n",
    "    LR: 0.005\n",
    "    LR_POLICY: \"poly\"\n",
    "    OPTIMIZER: \"sgd\"\n",
    "    NUM_EPOCHS: 40\n",
    "    CROSS_ENTROPY_WEIGHT: [0.1, 2, 0.5, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2 ,2, 2] #每一个类别loss权重\n",
    "```\n",
    "### [](https://gitee.com/paddlepaddle/PaddleSeg/blob/release/v2.0/docs/lovasz_loss.md#lovasz-loss)Lovasz loss\n",
    "Lovasz loss基于子模损失(submodular losses)的凸Lovasz扩展，对神经网络的mean IoU损失进行优化。Lovasz loss根据分割目标的类别数量可分为两种：lovasz hinge loss和lovasz softmax loss. 其中lovasz hinge loss适用于二分类问题，lovasz softmax loss适用于多分类问题。该工作发表在CVPR 2018上，可点击[参考文献](https://gitee.com/paddlepaddle/PaddleSeg/blob/release/v2.0/docs/lovasz_loss.md#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE)查看具体原理。\n",
    "\n",
    "需要注意的是，通常的直接训练方式不一定管用，PaddleSeg推荐的是另外2种训练方式：\n",
    "- （1）与cross entropy loss或bce loss(binary cross-entropy loss)加权结合使用。\n",
    "- （2）先使用cross entropy loss或bce loss进行训练，再使用lovasz softmax loss或lovasz hinge loss进行finetuning。\n",
    "通过`coef`参数对不同loss进行权重配比，从而灵活地进行训练调参。\n",
    "\n",
    "```python\n",
    "loss:\n",
    "  types:\n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: LovaszSoftmaxLoss\n",
    "      coef: [0.8, 0.2]\n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: LovaszSoftmaxLoss\n",
    "      coef: [0.8, 0.2]\n",
    "  coef: [1, 0.4]\n",
    "\n",
    "SOLVER:\n",
    "    LR: 0.005\n",
    "    LR_POLICY: \"poly\"\n",
    "    OPTIMIZER: \"sgd\"\n",
    "    NUM_EPOCHS: 40\n",
    "    CROSS_ENTROPY_WEIGHT: dynamic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/PaddleSeg\n"
     ]
    }
   ],
   "source": [
    "%cd PaddleSeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可以选择任意config文件重新训练\n",
    "!python train.py \\\n",
    "       --config configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml \\\n",
    "       --do_eval \\\n",
    "       --use_vdl \\\n",
    "       --save_interval 750 \\\n",
    "       --save_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2021-04-13 17:28:42 [INFO]\t\n",
      "------------Environment Information-------------\n",
      "platform: Linux-4.13.0-36-generic-x86_64-with-debian-stretch-sid\n",
      "Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\n",
      "Paddle compiled with cuda: True\n",
      "NVCC: Cuda compilation tools, release 10.1, V10.1.243\n",
      "cudnn: 7.6\n",
      "GPUs used: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "GPU: ['GPU 0: Tesla V100-SXM2-16GB']\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\n",
      "PaddlePaddle: 2.0.1\n",
      "OpenCV: 4.1.1\n",
      "------------------------------------------------\n",
      "2021-04-13 17:28:42 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "SOLVER:\n",
      "  CROSS_ENTROPY_WEIGHT: dynamic\n",
      "  LR: 0.005\n",
      "  LR_POLICY: poly\n",
      "  NUM_EPOCHS: 40\n",
      "  OPTIMIZER: sgd\n",
      "batch_size: 4\n",
      "iters: 20000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.0025\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  - 0.4\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "    type: HRNet_W18\n",
      "  backbone_indices:\n",
      "  - 0\n",
      "  type: OCRNet\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: train\n",
      "  num_classes: 15\n",
      "  train_path: /home/aistudio/train_list.txt\n",
      "  transforms:\n",
      "  - max_scale_factor: 2.0\n",
      "    min_scale_factor: 0.5\n",
      "    scale_step_size: 0.25\n",
      "    type: ResizeStepScaling\n",
      "  - max_rotation: 30\n",
      "    type: RandomRotation\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - crop_size:\n",
      "    - 1024\n",
      "    - 512\n",
      "    type: RandomPaddingCrop\n",
      "  - type: RandomBlur\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: val\n",
      "  num_classes: 15\n",
      "  transforms:\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/val_list.txt\n",
      "------------------------------------------------\n",
      "W0413 17:28:42.403793  4253 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0413 17:28:42.403846  4253 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-04-13 17:28:46 [INFO]\tLoading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "2021-04-13 17:28:46,264 - INFO - Lock 140521126073680 acquired on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 17:28:46,265 - INFO - Lock 140521126073680 released on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 17:28:47 [INFO]\tThere are 1525/1525 variables loaded into HRNet.\n",
      "2021-04-13 17:28:48 [INFO]\tResume model from output/iter_12750\n",
      "2021-04-13 17:29:03 [INFO]\t[TRAIN] epoch=15, iter=12760/20000, loss=0.2117, lr=0.001002, batch_cost=1.3671, reader_cost=0.11303, ips=2.9260 samples/sec | ETA 02:44:57\n",
      "2021-04-13 17:29:14 [INFO]\t[TRAIN] epoch=15, iter=12770/20000, loss=0.2073, lr=0.001001, batch_cost=1.1786, reader_cost=0.00010, ips=3.3937 samples/sec | ETA 02:22:01\n",
      "2021-04-13 17:29:25 [INFO]\t[TRAIN] epoch=15, iter=12780/20000, loss=0.1802, lr=0.000999, batch_cost=1.0709, reader_cost=0.00009, ips=3.7350 samples/sec | ETA 02:08:52\n",
      "2021-04-13 17:29:37 [INFO]\t[TRAIN] epoch=15, iter=12790/20000, loss=0.1907, lr=0.000998, batch_cost=1.1583, reader_cost=0.00010, ips=3.4534 samples/sec | ETA 02:19:11\n",
      "2021-04-13 17:29:48 [INFO]\t[TRAIN] epoch=15, iter=12800/20000, loss=0.1618, lr=0.000997, batch_cost=1.1379, reader_cost=0.00009, ips=3.5153 samples/sec | ETA 02:16:32\n",
      "2021-04-13 17:29:59 [INFO]\t[TRAIN] epoch=15, iter=12810/20000, loss=0.1786, lr=0.000996, batch_cost=1.1002, reader_cost=0.00009, ips=3.6357 samples/sec | ETA 02:11:50\n",
      "2021-04-13 17:30:11 [INFO]\t[TRAIN] epoch=15, iter=12820/20000, loss=0.1922, lr=0.000994, batch_cost=1.1849, reader_cost=0.00010, ips=3.3758 samples/sec | ETA 02:21:47\n",
      "2021-04-13 17:30:22 [INFO]\t[TRAIN] epoch=15, iter=12830/20000, loss=0.1953, lr=0.000993, batch_cost=1.0897, reader_cost=0.00009, ips=3.6708 samples/sec | ETA 02:10:13\n",
      "2021-04-13 17:30:33 [INFO]\t[TRAIN] epoch=15, iter=12840/20000, loss=0.2127, lr=0.000992, batch_cost=1.0970, reader_cost=0.00009, ips=3.6463 samples/sec | ETA 02:10:54\n",
      "2021-04-13 17:30:43 [INFO]\t[TRAIN] epoch=15, iter=12850/20000, loss=0.1749, lr=0.000991, batch_cost=1.0604, reader_cost=0.00010, ips=3.7721 samples/sec | ETA 02:06:21\n",
      "2021-04-13 17:30:54 [INFO]\t[TRAIN] epoch=15, iter=12860/20000, loss=0.2149, lr=0.000989, batch_cost=1.0674, reader_cost=0.00009, ips=3.7474 samples/sec | ETA 02:07:01\n",
      "2021-04-13 17:31:05 [INFO]\t[TRAIN] epoch=15, iter=12870/20000, loss=0.2198, lr=0.000988, batch_cost=1.0898, reader_cost=0.00009, ips=3.6704 samples/sec | ETA 02:09:30\n",
      "2021-04-13 17:31:16 [INFO]\t[TRAIN] epoch=15, iter=12880/20000, loss=0.1677, lr=0.000987, batch_cost=1.1283, reader_cost=0.00010, ips=3.5450 samples/sec | ETA 02:13:53\n",
      "2021-04-13 17:31:28 [INFO]\t[TRAIN] epoch=15, iter=12890/20000, loss=0.1920, lr=0.000986, batch_cost=1.1601, reader_cost=0.00009, ips=3.4478 samples/sec | ETA 02:17:28\n",
      "2021-04-13 17:31:38 [INFO]\t[TRAIN] epoch=15, iter=12900/20000, loss=0.1835, lr=0.000984, batch_cost=1.0611, reader_cost=0.00010, ips=3.7697 samples/sec | ETA 02:05:33\n",
      "2021-04-13 17:31:49 [INFO]\t[TRAIN] epoch=15, iter=12910/20000, loss=0.1996, lr=0.000983, batch_cost=1.0814, reader_cost=0.00010, ips=3.6989 samples/sec | ETA 02:07:47\n",
      "2021-04-13 17:32:00 [INFO]\t[TRAIN] epoch=15, iter=12920/20000, loss=0.1859, lr=0.000982, batch_cost=1.1094, reader_cost=0.00009, ips=3.6057 samples/sec | ETA 02:10:54\n",
      "2021-04-13 17:32:11 [INFO]\t[TRAIN] epoch=15, iter=12930/20000, loss=0.1897, lr=0.000981, batch_cost=1.0806, reader_cost=0.00010, ips=3.7017 samples/sec | ETA 02:07:19\n",
      "2021-04-13 17:32:22 [INFO]\t[TRAIN] epoch=15, iter=12940/20000, loss=0.2340, lr=0.000979, batch_cost=1.1417, reader_cost=0.00009, ips=3.5035 samples/sec | ETA 02:14:20\n",
      "2021-04-13 17:32:34 [INFO]\t[TRAIN] epoch=15, iter=12950/20000, loss=0.1826, lr=0.000978, batch_cost=1.1143, reader_cost=0.00012, ips=3.5898 samples/sec | ETA 02:10:55\n",
      "2021-04-13 17:32:44 [INFO]\t[TRAIN] epoch=15, iter=12960/20000, loss=0.1999, lr=0.000977, batch_cost=1.0634, reader_cost=0.00009, ips=3.7617 samples/sec | ETA 02:04:46\n",
      "2021-04-13 17:32:55 [INFO]\t[TRAIN] epoch=15, iter=12970/20000, loss=0.1930, lr=0.000976, batch_cost=1.0553, reader_cost=0.00009, ips=3.7902 samples/sec | ETA 02:03:39\n",
      "2021-04-13 17:33:06 [INFO]\t[TRAIN] epoch=15, iter=12980/20000, loss=0.2108, lr=0.000974, batch_cost=1.1026, reader_cost=0.00009, ips=3.6277 samples/sec | ETA 02:09:00\n",
      "2021-04-13 17:33:17 [INFO]\t[TRAIN] epoch=15, iter=12990/20000, loss=0.1451, lr=0.000973, batch_cost=1.1002, reader_cost=0.00010, ips=3.6356 samples/sec | ETA 02:08:32\n",
      "2021-04-13 17:33:29 [INFO]\t[TRAIN] epoch=15, iter=13000/20000, loss=0.1768, lr=0.000972, batch_cost=1.1727, reader_cost=0.00011, ips=3.4109 samples/sec | ETA 02:16:48\n",
      "2021-04-13 17:33:39 [INFO]\t[TRAIN] epoch=15, iter=13010/20000, loss=0.1714, lr=0.000971, batch_cost=1.0625, reader_cost=0.00010, ips=3.7647 samples/sec | ETA 02:03:46\n",
      "2021-04-13 17:33:50 [INFO]\t[TRAIN] epoch=15, iter=13020/20000, loss=0.2049, lr=0.000969, batch_cost=1.1135, reader_cost=0.00009, ips=3.5924 samples/sec | ETA 02:09:31\n",
      "2021-04-13 17:34:02 [INFO]\t[TRAIN] epoch=15, iter=13030/20000, loss=0.1911, lr=0.000968, batch_cost=1.1214, reader_cost=0.00009, ips=3.5671 samples/sec | ETA 02:10:15\n",
      "2021-04-13 17:34:12 [INFO]\t[TRAIN] epoch=15, iter=13040/20000, loss=0.2186, lr=0.000967, batch_cost=1.0878, reader_cost=0.00010, ips=3.6771 samples/sec | ETA 02:06:11\n",
      "2021-04-13 17:34:24 [INFO]\t[TRAIN] epoch=15, iter=13050/20000, loss=0.2100, lr=0.000966, batch_cost=1.1335, reader_cost=0.00009, ips=3.5289 samples/sec | ETA 02:11:17\n",
      "2021-04-13 17:34:35 [INFO]\t[TRAIN] epoch=15, iter=13060/20000, loss=0.2125, lr=0.000964, batch_cost=1.1449, reader_cost=0.00009, ips=3.4938 samples/sec | ETA 02:12:25\n",
      "2021-04-13 17:34:46 [INFO]\t[TRAIN] epoch=15, iter=13070/20000, loss=0.2163, lr=0.000963, batch_cost=1.0918, reader_cost=0.00010, ips=3.6636 samples/sec | ETA 02:06:06\n",
      "2021-04-13 17:34:57 [INFO]\t[TRAIN] epoch=15, iter=13080/20000, loss=0.2275, lr=0.000962, batch_cost=1.0695, reader_cost=0.00010, ips=3.7401 samples/sec | ETA 02:03:20\n",
      "2021-04-13 17:35:08 [INFO]\t[TRAIN] epoch=15, iter=13090/20000, loss=0.1763, lr=0.000961, batch_cost=1.1157, reader_cost=0.00009, ips=3.5851 samples/sec | ETA 02:08:29\n",
      "2021-04-13 17:35:20 [INFO]\t[TRAIN] epoch=15, iter=13100/20000, loss=0.2018, lr=0.000959, batch_cost=1.1607, reader_cost=0.00009, ips=3.4463 samples/sec | ETA 02:13:28\n",
      "2021-04-13 17:35:30 [INFO]\t[TRAIN] epoch=15, iter=13110/20000, loss=0.2294, lr=0.000958, batch_cost=1.0798, reader_cost=0.00009, ips=3.7045 samples/sec | ETA 02:03:59\n",
      "2021-04-13 17:35:42 [INFO]\t[TRAIN] epoch=15, iter=13120/20000, loss=0.1736, lr=0.000957, batch_cost=1.1579, reader_cost=0.00009, ips=3.4545 samples/sec | ETA 02:12:46\n",
      "2021-04-13 17:35:53 [INFO]\t[TRAIN] epoch=16, iter=13130/20000, loss=0.1695, lr=0.000956, batch_cost=1.0982, reader_cost=0.00010, ips=3.6423 samples/sec | ETA 02:05:44\n",
      "2021-04-13 17:36:04 [INFO]\t[TRAIN] epoch=16, iter=13140/20000, loss=0.1734, lr=0.000954, batch_cost=1.1276, reader_cost=0.00009, ips=3.5475 samples/sec | ETA 02:08:55\n",
      "2021-04-13 17:36:15 [INFO]\t[TRAIN] epoch=16, iter=13150/20000, loss=0.1840, lr=0.000953, batch_cost=1.1048, reader_cost=0.00010, ips=3.6207 samples/sec | ETA 02:06:07\n",
      "2021-04-13 17:36:26 [INFO]\t[TRAIN] epoch=16, iter=13160/20000, loss=0.1804, lr=0.000952, batch_cost=1.0941, reader_cost=0.00010, ips=3.6560 samples/sec | ETA 02:04:43\n",
      "2021-04-13 17:36:38 [INFO]\t[TRAIN] epoch=16, iter=13170/20000, loss=0.2265, lr=0.000951, batch_cost=1.1360, reader_cost=0.00009, ips=3.5210 samples/sec | ETA 02:09:19\n",
      "2021-04-13 17:36:49 [INFO]\t[TRAIN] epoch=16, iter=13180/20000, loss=0.1771, lr=0.000949, batch_cost=1.1257, reader_cost=0.00010, ips=3.5535 samples/sec | ETA 02:07:57\n",
      "2021-04-13 17:37:00 [INFO]\t[TRAIN] epoch=16, iter=13190/20000, loss=0.1885, lr=0.000948, batch_cost=1.1435, reader_cost=0.00010, ips=3.4981 samples/sec | ETA 02:09:47\n",
      "2021-04-13 17:37:11 [INFO]\t[TRAIN] epoch=16, iter=13200/20000, loss=0.1785, lr=0.000947, batch_cost=1.1083, reader_cost=0.00009, ips=3.6090 samples/sec | ETA 02:05:36\n",
      "2021-04-13 17:37:22 [INFO]\t[TRAIN] epoch=16, iter=13210/20000, loss=0.1872, lr=0.000946, batch_cost=1.0902, reader_cost=0.00009, ips=3.6689 samples/sec | ETA 02:03:22\n",
      "2021-04-13 17:37:34 [INFO]\t[TRAIN] epoch=16, iter=13220/20000, loss=0.1726, lr=0.000944, batch_cost=1.1245, reader_cost=0.00010, ips=3.5572 samples/sec | ETA 02:07:03\n",
      "2021-04-13 17:37:44 [INFO]\t[TRAIN] epoch=16, iter=13230/20000, loss=0.1783, lr=0.000943, batch_cost=1.0820, reader_cost=0.00009, ips=3.6968 samples/sec | ETA 02:02:05\n",
      "2021-04-13 17:37:55 [INFO]\t[TRAIN] epoch=16, iter=13240/20000, loss=0.2014, lr=0.000942, batch_cost=1.0935, reader_cost=0.00010, ips=3.6580 samples/sec | ETA 02:03:11\n",
      "2021-04-13 17:38:06 [INFO]\t[TRAIN] epoch=16, iter=13250/20000, loss=0.1753, lr=0.000941, batch_cost=1.0709, reader_cost=0.00008, ips=3.7350 samples/sec | ETA 02:00:28\n",
      "2021-04-13 17:38:17 [INFO]\t[TRAIN] epoch=16, iter=13260/20000, loss=0.2159, lr=0.000939, batch_cost=1.0675, reader_cost=0.00009, ips=3.7470 samples/sec | ETA 01:59:55\n",
      "2021-04-13 17:38:28 [INFO]\t[TRAIN] epoch=16, iter=13270/20000, loss=0.1850, lr=0.000938, batch_cost=1.1233, reader_cost=0.00009, ips=3.5609 samples/sec | ETA 02:05:59\n",
      "2021-04-13 17:38:39 [INFO]\t[TRAIN] epoch=16, iter=13280/20000, loss=0.2165, lr=0.000937, batch_cost=1.1031, reader_cost=0.00010, ips=3.6263 samples/sec | ETA 02:03:32\n",
      "2021-04-13 17:38:50 [INFO]\t[TRAIN] epoch=16, iter=13290/20000, loss=0.2077, lr=0.000936, batch_cost=1.1275, reader_cost=0.00009, ips=3.5477 samples/sec | ETA 02:06:05\n",
      "2021-04-13 17:39:01 [INFO]\t[TRAIN] epoch=16, iter=13300/20000, loss=0.2046, lr=0.000934, batch_cost=1.0754, reader_cost=0.00009, ips=3.7194 samples/sec | ETA 02:00:05\n",
      "2021-04-13 17:39:12 [INFO]\t[TRAIN] epoch=16, iter=13310/20000, loss=0.2258, lr=0.000933, batch_cost=1.1014, reader_cost=0.00009, ips=3.6316 samples/sec | ETA 02:02:48\n",
      "2021-04-13 17:39:23 [INFO]\t[TRAIN] epoch=16, iter=13320/20000, loss=0.1728, lr=0.000932, batch_cost=1.1159, reader_cost=0.00010, ips=3.5845 samples/sec | ETA 02:04:14\n",
      "2021-04-13 17:39:34 [INFO]\t[TRAIN] epoch=16, iter=13330/20000, loss=0.2507, lr=0.000931, batch_cost=1.1318, reader_cost=0.00010, ips=3.5343 samples/sec | ETA 02:05:48\n",
      "2021-04-13 17:39:46 [INFO]\t[TRAIN] epoch=16, iter=13340/20000, loss=0.2025, lr=0.000929, batch_cost=1.1124, reader_cost=0.00009, ips=3.5958 samples/sec | ETA 02:03:28\n",
      "2021-04-13 17:39:56 [INFO]\t[TRAIN] epoch=16, iter=13350/20000, loss=0.1898, lr=0.000928, batch_cost=1.0880, reader_cost=0.00009, ips=3.6766 samples/sec | ETA 02:00:35\n",
      "2021-04-13 17:40:08 [INFO]\t[TRAIN] epoch=16, iter=13360/20000, loss=0.1785, lr=0.000927, batch_cost=1.1249, reader_cost=0.00009, ips=3.5560 samples/sec | ETA 02:04:29\n",
      "2021-04-13 17:40:19 [INFO]\t[TRAIN] epoch=16, iter=13370/20000, loss=0.2075, lr=0.000926, batch_cost=1.1486, reader_cost=0.00010, ips=3.4825 samples/sec | ETA 02:06:55\n",
      "2021-04-13 17:40:31 [INFO]\t[TRAIN] epoch=16, iter=13380/20000, loss=0.1591, lr=0.000924, batch_cost=1.1501, reader_cost=0.00010, ips=3.4779 samples/sec | ETA 02:06:53\n",
      "2021-04-13 17:40:43 [INFO]\t[TRAIN] epoch=16, iter=13390/20000, loss=0.1886, lr=0.000923, batch_cost=1.2298, reader_cost=0.00010, ips=3.2527 samples/sec | ETA 02:15:28\n",
      "2021-04-13 17:40:54 [INFO]\t[TRAIN] epoch=16, iter=13400/20000, loss=0.1849, lr=0.000922, batch_cost=1.0651, reader_cost=0.00087, ips=3.7555 samples/sec | ETA 01:57:09\n",
      "2021-04-13 17:41:05 [INFO]\t[TRAIN] epoch=16, iter=13410/20000, loss=0.2027, lr=0.000921, batch_cost=1.1639, reader_cost=0.00010, ips=3.4367 samples/sec | ETA 02:07:50\n",
      "2021-04-13 17:41:17 [INFO]\t[TRAIN] epoch=16, iter=13420/20000, loss=0.2051, lr=0.000919, batch_cost=1.1738, reader_cost=0.00009, ips=3.4078 samples/sec | ETA 02:08:43\n",
      "2021-04-13 17:41:28 [INFO]\t[TRAIN] epoch=16, iter=13430/20000, loss=0.1734, lr=0.000918, batch_cost=1.1159, reader_cost=0.00009, ips=3.5844 samples/sec | ETA 02:02:11\n",
      "2021-04-13 17:41:39 [INFO]\t[TRAIN] epoch=16, iter=13440/20000, loss=0.1788, lr=0.000917, batch_cost=1.0905, reader_cost=0.00009, ips=3.6681 samples/sec | ETA 01:59:13\n",
      "2021-04-13 17:41:50 [INFO]\t[TRAIN] epoch=16, iter=13450/20000, loss=0.1864, lr=0.000916, batch_cost=1.1208, reader_cost=0.00011, ips=3.5689 samples/sec | ETA 02:02:21\n",
      "2021-04-13 17:42:02 [INFO]\t[TRAIN] epoch=16, iter=13460/20000, loss=0.1992, lr=0.000914, batch_cost=1.1218, reader_cost=0.00009, ips=3.5658 samples/sec | ETA 02:02:16\n",
      "2021-04-13 17:42:12 [INFO]\t[TRAIN] epoch=16, iter=13470/20000, loss=0.1599, lr=0.000913, batch_cost=1.0807, reader_cost=0.00010, ips=3.7012 samples/sec | ETA 01:57:37\n",
      "2021-04-13 17:42:24 [INFO]\t[TRAIN] epoch=16, iter=13480/20000, loss=0.1845, lr=0.000912, batch_cost=1.1286, reader_cost=0.00045, ips=3.5443 samples/sec | ETA 02:02:38\n",
      "2021-04-13 17:42:35 [INFO]\t[TRAIN] epoch=16, iter=13490/20000, loss=0.2172, lr=0.000911, batch_cost=1.1465, reader_cost=0.00009, ips=3.4889 samples/sec | ETA 02:04:23\n",
      "2021-04-13 17:42:46 [INFO]\t[TRAIN] epoch=16, iter=13500/20000, loss=0.1979, lr=0.000909, batch_cost=1.1422, reader_cost=0.00009, ips=3.5019 samples/sec | ETA 02:03:44\n",
      "2021-04-13 17:42:47 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT32, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT64, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "500/500 [==============================] - 106s 212ms/step - batch_cost: 0.2122 - reader cost: 0.00\n",
      "2021-04-13 17:44:33 [INFO]\t[EVAL] #Images=500 mIoU=0.2030 Acc=0.9884 Kappa=0.6061 \n",
      "2021-04-13 17:44:33 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9898 0.1853 0.5099 0.4729 0.1649 0.2978 0.4044 0.     0.     0.\n",
      " 0.     0.0198 0.     0.     0.    ]\n",
      "2021-04-13 17:44:33 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9935 0.2887 0.7609 0.583  0.2607 0.3863 0.6079 0.     0.     0.\n",
      " 0.     0.2162 0.     0.     0.    ]\n",
      "2021-04-13 17:44:36 [INFO]\t[EVAL] The model with the best validation mIoU (0.2030) was saved at iter 13500.\n",
      "2021-04-13 17:44:47 [INFO]\t[TRAIN] epoch=16, iter=13510/20000, loss=0.2138, lr=0.000908, batch_cost=1.1013, reader_cost=0.00009, ips=3.6319 samples/sec | ETA 01:59:07\n",
      "2021-04-13 17:44:57 [INFO]\t[TRAIN] epoch=16, iter=13520/20000, loss=0.2149, lr=0.000907, batch_cost=1.0601, reader_cost=0.00009, ips=3.7734 samples/sec | ETA 01:54:29\n",
      "2021-04-13 17:45:09 [INFO]\t[TRAIN] epoch=16, iter=13530/20000, loss=0.1902, lr=0.000905, batch_cost=1.1241, reader_cost=0.00009, ips=3.5584 samples/sec | ETA 02:01:12\n",
      "2021-04-13 17:45:21 [INFO]\t[TRAIN] epoch=16, iter=13540/20000, loss=0.2264, lr=0.000904, batch_cost=1.2067, reader_cost=0.00010, ips=3.3148 samples/sec | ETA 02:09:55\n",
      "2021-04-13 17:45:32 [INFO]\t[TRAIN] epoch=16, iter=13550/20000, loss=0.1949, lr=0.000903, batch_cost=1.1248, reader_cost=0.00010, ips=3.5561 samples/sec | ETA 02:00:55\n",
      "2021-04-13 17:45:44 [INFO]\t[TRAIN] epoch=16, iter=13560/20000, loss=0.1758, lr=0.000902, batch_cost=1.1894, reader_cost=0.00010, ips=3.3629 samples/sec | ETA 02:07:39\n",
      "2021-04-13 17:45:55 [INFO]\t[TRAIN] epoch=16, iter=13570/20000, loss=0.1904, lr=0.000900, batch_cost=1.0948, reader_cost=0.00009, ips=3.6537 samples/sec | ETA 01:57:19\n",
      "2021-04-13 17:46:06 [INFO]\t[TRAIN] epoch=16, iter=13580/20000, loss=0.2104, lr=0.000899, batch_cost=1.1176, reader_cost=0.00009, ips=3.5790 samples/sec | ETA 01:59:35\n",
      "2021-04-13 17:46:17 [INFO]\t[TRAIN] epoch=16, iter=13590/20000, loss=0.2158, lr=0.000898, batch_cost=1.1011, reader_cost=0.00009, ips=3.6329 samples/sec | ETA 01:57:37\n",
      "2021-04-13 17:46:28 [INFO]\t[TRAIN] epoch=16, iter=13600/20000, loss=0.1655, lr=0.000897, batch_cost=1.1177, reader_cost=0.00010, ips=3.5787 samples/sec | ETA 01:59:13\n",
      "2021-04-13 17:46:39 [INFO]\t[TRAIN] epoch=16, iter=13610/20000, loss=0.1864, lr=0.000895, batch_cost=1.0568, reader_cost=0.00009, ips=3.7849 samples/sec | ETA 01:52:33\n",
      "2021-04-13 17:46:49 [INFO]\t[TRAIN] epoch=16, iter=13620/20000, loss=0.3498, lr=0.000894, batch_cost=1.0668, reader_cost=0.00011, ips=3.7495 samples/sec | ETA 01:53:26\n",
      "2021-04-13 17:47:00 [INFO]\t[TRAIN] epoch=16, iter=13630/20000, loss=0.1874, lr=0.000893, batch_cost=1.0665, reader_cost=0.08359, ips=3.7507 samples/sec | ETA 01:53:13\n",
      "2021-04-13 17:47:12 [INFO]\t[TRAIN] epoch=16, iter=13640/20000, loss=0.2020, lr=0.000892, batch_cost=1.1485, reader_cost=0.00016, ips=3.4827 samples/sec | ETA 02:01:44\n",
      "2021-04-13 17:47:22 [INFO]\t[TRAIN] epoch=16, iter=13650/20000, loss=0.1742, lr=0.000890, batch_cost=1.0939, reader_cost=0.00009, ips=3.6565 samples/sec | ETA 01:55:46\n",
      "2021-04-13 17:47:34 [INFO]\t[TRAIN] epoch=16, iter=13660/20000, loss=0.1932, lr=0.000889, batch_cost=1.1754, reader_cost=0.00011, ips=3.4032 samples/sec | ETA 02:04:11\n",
      "2021-04-13 17:47:46 [INFO]\t[TRAIN] epoch=16, iter=13670/20000, loss=0.2303, lr=0.000888, batch_cost=1.1507, reader_cost=0.00009, ips=3.4762 samples/sec | ETA 02:01:23\n",
      "2021-04-13 17:47:57 [INFO]\t[TRAIN] epoch=16, iter=13680/20000, loss=0.1833, lr=0.000887, batch_cost=1.0945, reader_cost=0.00009, ips=3.6546 samples/sec | ETA 01:55:17\n",
      "2021-04-13 17:48:07 [INFO]\t[TRAIN] epoch=16, iter=13690/20000, loss=0.1844, lr=0.000885, batch_cost=1.0565, reader_cost=0.00010, ips=3.7862 samples/sec | ETA 01:51:06\n",
      "2021-04-13 17:48:18 [INFO]\t[TRAIN] epoch=16, iter=13700/20000, loss=0.1555, lr=0.000884, batch_cost=1.0960, reader_cost=0.00010, ips=3.6498 samples/sec | ETA 01:55:04\n",
      "2021-04-13 17:48:29 [INFO]\t[TRAIN] epoch=16, iter=13710/20000, loss=0.1936, lr=0.000883, batch_cost=1.1321, reader_cost=0.00011, ips=3.5334 samples/sec | ETA 01:58:40\n",
      "2021-04-13 17:48:40 [INFO]\t[TRAIN] epoch=16, iter=13720/20000, loss=0.1703, lr=0.000882, batch_cost=1.0997, reader_cost=0.00009, ips=3.6372 samples/sec | ETA 01:55:06\n",
      "2021-04-13 17:48:51 [INFO]\t[TRAIN] epoch=16, iter=13730/20000, loss=0.2322, lr=0.000880, batch_cost=1.0729, reader_cost=0.00009, ips=3.7282 samples/sec | ETA 01:52:07\n",
      "2021-04-13 17:49:02 [INFO]\t[TRAIN] epoch=16, iter=13740/20000, loss=0.1720, lr=0.000879, batch_cost=1.1192, reader_cost=0.00009, ips=3.5739 samples/sec | ETA 01:56:46\n",
      "2021-04-13 17:49:14 [INFO]\t[TRAIN] epoch=16, iter=13750/20000, loss=0.1468, lr=0.000878, batch_cost=1.1291, reader_cost=0.00010, ips=3.5426 samples/sec | ETA 01:57:37\n",
      "2021-04-13 17:49:25 [INFO]\t[TRAIN] epoch=16, iter=13760/20000, loss=0.2280, lr=0.000876, batch_cost=1.1261, reader_cost=0.00009, ips=3.5520 samples/sec | ETA 01:57:06\n",
      "2021-04-13 17:49:35 [INFO]\t[TRAIN] epoch=16, iter=13770/20000, loss=0.1869, lr=0.000875, batch_cost=1.0393, reader_cost=0.00010, ips=3.8486 samples/sec | ETA 01:47:54\n",
      "2021-04-13 17:49:46 [INFO]\t[TRAIN] epoch=16, iter=13780/20000, loss=0.1905, lr=0.000874, batch_cost=1.0523, reader_cost=0.00009, ips=3.8012 samples/sec | ETA 01:49:05\n",
      "2021-04-13 17:49:57 [INFO]\t[TRAIN] epoch=16, iter=13790/20000, loss=0.2017, lr=0.000873, batch_cost=1.1077, reader_cost=0.00009, ips=3.6110 samples/sec | ETA 01:54:38\n",
      "2021-04-13 17:50:08 [INFO]\t[TRAIN] epoch=16, iter=13800/20000, loss=0.1998, lr=0.000871, batch_cost=1.0860, reader_cost=0.00009, ips=3.6832 samples/sec | ETA 01:52:13\n",
      "2021-04-13 17:50:19 [INFO]\t[TRAIN] epoch=16, iter=13810/20000, loss=0.1938, lr=0.000870, batch_cost=1.1343, reader_cost=0.00010, ips=3.5263 samples/sec | ETA 01:57:01\n",
      "2021-04-13 17:50:31 [INFO]\t[TRAIN] epoch=16, iter=13820/20000, loss=0.2125, lr=0.000869, batch_cost=1.1408, reader_cost=0.00009, ips=3.5064 samples/sec | ETA 01:57:30\n",
      "2021-04-13 17:50:42 [INFO]\t[TRAIN] epoch=16, iter=13830/20000, loss=0.1870, lr=0.000868, batch_cost=1.1062, reader_cost=0.00009, ips=3.6160 samples/sec | ETA 01:53:45\n",
      "2021-04-13 17:50:53 [INFO]\t[TRAIN] epoch=16, iter=13840/20000, loss=0.1937, lr=0.000866, batch_cost=1.1532, reader_cost=0.00010, ips=3.4686 samples/sec | ETA 01:58:23\n",
      "2021-04-13 17:51:04 [INFO]\t[TRAIN] epoch=16, iter=13850/20000, loss=0.1854, lr=0.000865, batch_cost=1.0712, reader_cost=0.00009, ips=3.7340 samples/sec | ETA 01:49:48\n",
      "2021-04-13 17:51:15 [INFO]\t[TRAIN] epoch=16, iter=13860/20000, loss=0.1631, lr=0.000864, batch_cost=1.1425, reader_cost=0.00011, ips=3.5010 samples/sec | ETA 01:56:55\n",
      "2021-04-13 17:51:27 [INFO]\t[TRAIN] epoch=16, iter=13870/20000, loss=0.1931, lr=0.000863, batch_cost=1.1238, reader_cost=0.00009, ips=3.5595 samples/sec | ETA 01:54:48\n",
      "2021-04-13 17:51:37 [INFO]\t[TRAIN] epoch=16, iter=13880/20000, loss=0.1573, lr=0.000861, batch_cost=1.0074, reader_cost=0.00008, ips=3.9705 samples/sec | ETA 01:42:45\n",
      "2021-04-13 17:51:48 [INFO]\t[TRAIN] epoch=16, iter=13890/20000, loss=0.2072, lr=0.000860, batch_cost=1.1749, reader_cost=0.00009, ips=3.4044 samples/sec | ETA 01:59:38\n",
      "2021-04-13 17:52:00 [INFO]\t[TRAIN] epoch=16, iter=13900/20000, loss=0.3566, lr=0.000859, batch_cost=1.1184, reader_cost=0.00009, ips=3.5766 samples/sec | ETA 01:53:42\n",
      "2021-04-13 17:52:10 [INFO]\t[TRAIN] epoch=16, iter=13910/20000, loss=0.1922, lr=0.000857, batch_cost=1.0550, reader_cost=0.00009, ips=3.7916 samples/sec | ETA 01:47:04\n",
      "2021-04-13 17:52:22 [INFO]\t[TRAIN] epoch=16, iter=13920/20000, loss=0.1669, lr=0.000856, batch_cost=1.1475, reader_cost=0.00009, ips=3.4858 samples/sec | ETA 01:56:16\n",
      "2021-04-13 17:52:33 [INFO]\t[TRAIN] epoch=16, iter=13930/20000, loss=0.1625, lr=0.000855, batch_cost=1.1335, reader_cost=0.00010, ips=3.5290 samples/sec | ETA 01:54:40\n",
      "2021-04-13 17:52:43 [INFO]\t[TRAIN] epoch=16, iter=13940/20000, loss=0.2137, lr=0.000854, batch_cost=1.0387, reader_cost=0.00009, ips=3.8508 samples/sec | ETA 01:44:54\n",
      "2021-04-13 17:52:54 [INFO]\t[TRAIN] epoch=16, iter=13950/20000, loss=0.1885, lr=0.000852, batch_cost=1.1040, reader_cost=0.00010, ips=3.6232 samples/sec | ETA 01:51:19\n",
      "2021-04-13 17:53:06 [INFO]\t[TRAIN] epoch=16, iter=13960/20000, loss=0.1835, lr=0.000851, batch_cost=1.1284, reader_cost=0.00017, ips=3.5449 samples/sec | ETA 01:53:35\n",
      "2021-04-13 17:53:17 [INFO]\t[TRAIN] epoch=16, iter=13970/20000, loss=0.1805, lr=0.000850, batch_cost=1.1414, reader_cost=0.00010, ips=3.5045 samples/sec | ETA 01:54:42\n",
      "2021-04-13 17:53:28 [INFO]\t[TRAIN] epoch=16, iter=13980/20000, loss=0.2088, lr=0.000849, batch_cost=1.0931, reader_cost=0.00010, ips=3.6595 samples/sec | ETA 01:49:40\n",
      "2021-04-13 17:53:39 [INFO]\t[TRAIN] epoch=16, iter=13990/20000, loss=0.2440, lr=0.000847, batch_cost=1.1514, reader_cost=0.00010, ips=3.4741 samples/sec | ETA 01:55:19\n",
      "2021-04-13 17:53:51 [INFO]\t[TRAIN] epoch=16, iter=14000/20000, loss=0.1826, lr=0.000846, batch_cost=1.1359, reader_cost=0.00010, ips=3.5215 samples/sec | ETA 01:53:35\n",
      "2021-04-13 17:54:02 [INFO]\t[TRAIN] epoch=17, iter=14010/20000, loss=0.1965, lr=0.000845, batch_cost=1.0985, reader_cost=0.00010, ips=3.6412 samples/sec | ETA 01:49:40\n",
      "2021-04-13 17:54:14 [INFO]\t[TRAIN] epoch=17, iter=14020/20000, loss=0.2175, lr=0.000844, batch_cost=1.1701, reader_cost=0.00009, ips=3.4186 samples/sec | ETA 01:56:36\n",
      "2021-04-13 17:54:25 [INFO]\t[TRAIN] epoch=17, iter=14030/20000, loss=0.1846, lr=0.000842, batch_cost=1.1385, reader_cost=0.00009, ips=3.5135 samples/sec | ETA 01:53:16\n",
      "2021-04-13 17:54:37 [INFO]\t[TRAIN] epoch=17, iter=14040/20000, loss=0.1677, lr=0.000841, batch_cost=1.2238, reader_cost=0.00009, ips=3.2686 samples/sec | ETA 02:01:33\n",
      "2021-04-13 17:54:49 [INFO]\t[TRAIN] epoch=17, iter=14050/20000, loss=0.2255, lr=0.000840, batch_cost=1.1607, reader_cost=0.00009, ips=3.4463 samples/sec | ETA 01:55:06\n",
      "2021-04-13 17:55:00 [INFO]\t[TRAIN] epoch=17, iter=14060/20000, loss=0.1852, lr=0.000838, batch_cost=1.1219, reader_cost=0.00009, ips=3.5653 samples/sec | ETA 01:51:04\n",
      "2021-04-13 17:55:11 [INFO]\t[TRAIN] epoch=17, iter=14070/20000, loss=0.2029, lr=0.000837, batch_cost=1.1275, reader_cost=0.00009, ips=3.5478 samples/sec | ETA 01:51:25\n",
      "2021-04-13 17:55:23 [INFO]\t[TRAIN] epoch=17, iter=14080/20000, loss=0.2245, lr=0.000836, batch_cost=1.1526, reader_cost=0.00009, ips=3.4703 samples/sec | ETA 01:53:43\n",
      "2021-04-13 17:55:35 [INFO]\t[TRAIN] epoch=17, iter=14090/20000, loss=0.3410, lr=0.000835, batch_cost=1.2024, reader_cost=0.00010, ips=3.3268 samples/sec | ETA 01:58:25\n",
      "2021-04-13 17:55:46 [INFO]\t[TRAIN] epoch=17, iter=14100/20000, loss=0.1888, lr=0.000833, batch_cost=1.0888, reader_cost=0.00009, ips=3.6739 samples/sec | ETA 01:47:03\n",
      "2021-04-13 17:55:57 [INFO]\t[TRAIN] epoch=17, iter=14110/20000, loss=0.1932, lr=0.000832, batch_cost=1.1567, reader_cost=0.00009, ips=3.4581 samples/sec | ETA 01:53:33\n",
      "2021-04-13 17:56:08 [INFO]\t[TRAIN] epoch=17, iter=14120/20000, loss=0.1832, lr=0.000831, batch_cost=1.1090, reader_cost=0.00009, ips=3.6067 samples/sec | ETA 01:48:41\n",
      "2021-04-13 17:56:20 [INFO]\t[TRAIN] epoch=17, iter=14130/20000, loss=0.1882, lr=0.000830, batch_cost=1.1722, reader_cost=0.00010, ips=3.4123 samples/sec | ETA 01:54:41\n",
      "2021-04-13 17:56:31 [INFO]\t[TRAIN] epoch=17, iter=14140/20000, loss=0.1677, lr=0.000828, batch_cost=1.1400, reader_cost=0.00011, ips=3.5088 samples/sec | ETA 01:51:20\n",
      "2021-04-13 17:56:42 [INFO]\t[TRAIN] epoch=17, iter=14150/20000, loss=0.1882, lr=0.000827, batch_cost=1.0834, reader_cost=0.00009, ips=3.6921 samples/sec | ETA 01:45:37\n",
      "2021-04-13 17:56:53 [INFO]\t[TRAIN] epoch=17, iter=14160/20000, loss=0.2054, lr=0.000826, batch_cost=1.1006, reader_cost=0.00009, ips=3.6345 samples/sec | ETA 01:47:07\n",
      "2021-04-13 17:57:05 [INFO]\t[TRAIN] epoch=17, iter=14170/20000, loss=0.1973, lr=0.000824, batch_cost=1.1246, reader_cost=0.00010, ips=3.5568 samples/sec | ETA 01:49:16\n",
      "2021-04-13 17:57:16 [INFO]\t[TRAIN] epoch=17, iter=14180/20000, loss=0.1958, lr=0.000823, batch_cost=1.1020, reader_cost=0.00010, ips=3.6298 samples/sec | ETA 01:46:53\n",
      "2021-04-13 17:57:27 [INFO]\t[TRAIN] epoch=17, iter=14190/20000, loss=0.2076, lr=0.000822, batch_cost=1.1237, reader_cost=0.00009, ips=3.5598 samples/sec | ETA 01:48:48\n",
      "2021-04-13 17:57:38 [INFO]\t[TRAIN] epoch=17, iter=14200/20000, loss=0.2002, lr=0.000821, batch_cost=1.1639, reader_cost=0.00011, ips=3.4369 samples/sec | ETA 01:52:30\n",
      "2021-04-13 17:57:51 [INFO]\t[TRAIN] epoch=17, iter=14210/20000, loss=0.1658, lr=0.000819, batch_cost=1.2081, reader_cost=0.00009, ips=3.3109 samples/sec | ETA 01:56:34\n",
      "2021-04-13 17:58:02 [INFO]\t[TRAIN] epoch=17, iter=14220/20000, loss=0.1955, lr=0.000818, batch_cost=1.1649, reader_cost=0.00009, ips=3.4338 samples/sec | ETA 01:52:13\n",
      "2021-04-13 17:58:13 [INFO]\t[TRAIN] epoch=17, iter=14230/20000, loss=0.2235, lr=0.000817, batch_cost=1.0869, reader_cost=0.00011, ips=3.6801 samples/sec | ETA 01:44:31\n",
      "2021-04-13 17:58:24 [INFO]\t[TRAIN] epoch=17, iter=14240/20000, loss=0.2062, lr=0.000816, batch_cost=1.0859, reader_cost=0.00009, ips=3.6835 samples/sec | ETA 01:44:14\n",
      "2021-04-13 17:58:35 [INFO]\t[TRAIN] epoch=17, iter=14250/20000, loss=0.2042, lr=0.000814, batch_cost=1.1097, reader_cost=0.00011, ips=3.6045 samples/sec | ETA 01:46:20\n",
      "2021-04-13 17:58:35 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 110s 219ms/step - batch_cost: 0.2188 - reader cost: 6.4950e-0\n",
      "2021-04-13 18:00:25 [INFO]\t[EVAL] #Images=500 mIoU=0.2076 Acc=0.9886 Kappa=0.6173 \n",
      "2021-04-13 18:00:25 [INFO]\t[EVAL] Class IoU: \n",
      "[0.99   0.1911 0.5224 0.4912 0.1656 0.2996 0.4147 0.     0.     0.\n",
      " 0.     0.0393 0.     0.     0.    ]\n",
      "2021-04-13 18:00:25 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9938 0.26   0.7458 0.586  0.3146 0.3653 0.6082 0.     0.     0.\n",
      " 0.     0.1908 0.     0.     0.    ]\n",
      "2021-04-13 18:00:28 [INFO]\t[EVAL] The model with the best validation mIoU (0.2076) was saved at iter 14250.\n",
      "2021-04-13 18:00:39 [INFO]\t[TRAIN] epoch=17, iter=14260/20000, loss=0.2051, lr=0.000813, batch_cost=1.1039, reader_cost=0.00009, ips=3.6236 samples/sec | ETA 01:45:36\n",
      "2021-04-13 18:00:50 [INFO]\t[TRAIN] epoch=17, iter=14270/20000, loss=0.1864, lr=0.000812, batch_cost=1.0678, reader_cost=0.00009, ips=3.7462 samples/sec | ETA 01:41:58\n",
      "2021-04-13 18:01:01 [INFO]\t[TRAIN] epoch=17, iter=14280/20000, loss=0.1836, lr=0.000810, batch_cost=1.1282, reader_cost=0.00010, ips=3.5453 samples/sec | ETA 01:47:33\n",
      "2021-04-13 18:01:12 [INFO]\t[TRAIN] epoch=17, iter=14290/20000, loss=0.1970, lr=0.000809, batch_cost=1.1391, reader_cost=0.00011, ips=3.5116 samples/sec | ETA 01:48:24\n",
      "2021-04-13 18:01:24 [INFO]\t[TRAIN] epoch=17, iter=14300/20000, loss=0.1900, lr=0.000808, batch_cost=1.1878, reader_cost=0.00010, ips=3.3677 samples/sec | ETA 01:52:50\n",
      "2021-04-13 18:01:35 [INFO]\t[TRAIN] epoch=17, iter=14310/20000, loss=0.2024, lr=0.000807, batch_cost=1.0866, reader_cost=0.00011, ips=3.6813 samples/sec | ETA 01:43:02\n",
      "2021-04-13 18:01:47 [INFO]\t[TRAIN] epoch=17, iter=14320/20000, loss=0.1882, lr=0.000805, batch_cost=1.1595, reader_cost=0.00010, ips=3.4499 samples/sec | ETA 01:49:45\n",
      "2021-04-13 18:01:59 [INFO]\t[TRAIN] epoch=17, iter=14330/20000, loss=0.2140, lr=0.000804, batch_cost=1.2014, reader_cost=0.00010, ips=3.3294 samples/sec | ETA 01:53:31\n",
      "2021-04-13 18:02:11 [INFO]\t[TRAIN] epoch=17, iter=14340/20000, loss=0.2194, lr=0.000803, batch_cost=1.2122, reader_cost=0.00013, ips=3.2999 samples/sec | ETA 01:54:20\n",
      "2021-04-13 18:02:22 [INFO]\t[TRAIN] epoch=17, iter=14350/20000, loss=0.1669, lr=0.000802, batch_cost=1.0944, reader_cost=0.00008, ips=3.6550 samples/sec | ETA 01:43:03\n",
      "2021-04-13 18:02:33 [INFO]\t[TRAIN] epoch=17, iter=14360/20000, loss=0.1834, lr=0.000800, batch_cost=1.1590, reader_cost=0.00009, ips=3.4511 samples/sec | ETA 01:48:57\n",
      "2021-04-13 18:02:45 [INFO]\t[TRAIN] epoch=17, iter=14370/20000, loss=0.1818, lr=0.000799, batch_cost=1.1413, reader_cost=0.00010, ips=3.5048 samples/sec | ETA 01:47:05\n",
      "2021-04-13 18:02:56 [INFO]\t[TRAIN] epoch=17, iter=14380/20000, loss=0.1576, lr=0.000798, batch_cost=1.1003, reader_cost=0.00009, ips=3.6353 samples/sec | ETA 01:43:03\n",
      "2021-04-13 18:03:08 [INFO]\t[TRAIN] epoch=17, iter=14390/20000, loss=0.1972, lr=0.000796, batch_cost=1.2085, reader_cost=0.00009, ips=3.3099 samples/sec | ETA 01:52:59\n",
      "2021-04-13 18:03:20 [INFO]\t[TRAIN] epoch=17, iter=14400/20000, loss=0.1693, lr=0.000795, batch_cost=1.1816, reader_cost=0.00010, ips=3.3853 samples/sec | ETA 01:50:16\n",
      "2021-04-13 18:03:30 [INFO]\t[TRAIN] epoch=17, iter=14410/20000, loss=0.2235, lr=0.000794, batch_cost=1.0718, reader_cost=0.00009, ips=3.7321 samples/sec | ETA 01:39:51\n",
      "2021-04-13 18:03:42 [INFO]\t[TRAIN] epoch=17, iter=14420/20000, loss=0.1989, lr=0.000793, batch_cost=1.1698, reader_cost=0.00010, ips=3.4194 samples/sec | ETA 01:48:47\n",
      "2021-04-13 18:03:53 [INFO]\t[TRAIN] epoch=17, iter=14430/20000, loss=0.2111, lr=0.000791, batch_cost=1.1233, reader_cost=0.00010, ips=3.5609 samples/sec | ETA 01:44:16\n",
      "2021-04-13 18:04:05 [INFO]\t[TRAIN] epoch=17, iter=14440/20000, loss=0.2269, lr=0.000790, batch_cost=1.1479, reader_cost=0.00009, ips=3.4845 samples/sec | ETA 01:46:22\n",
      "2021-04-13 18:04:15 [INFO]\t[TRAIN] epoch=17, iter=14450/20000, loss=0.1941, lr=0.000789, batch_cost=1.0471, reader_cost=0.00009, ips=3.8199 samples/sec | ETA 01:36:51\n",
      "2021-04-13 18:04:26 [INFO]\t[TRAIN] epoch=17, iter=14460/20000, loss=0.2258, lr=0.000787, batch_cost=1.1245, reader_cost=0.00009, ips=3.5573 samples/sec | ETA 01:43:49\n",
      "2021-04-13 18:04:37 [INFO]\t[TRAIN] epoch=17, iter=14470/20000, loss=0.1775, lr=0.000786, batch_cost=1.0716, reader_cost=0.00009, ips=3.7328 samples/sec | ETA 01:38:45\n",
      "2021-04-13 18:04:48 [INFO]\t[TRAIN] epoch=17, iter=14480/20000, loss=0.1547, lr=0.000785, batch_cost=1.0847, reader_cost=0.00010, ips=3.6876 samples/sec | ETA 01:39:47\n",
      "2021-04-13 18:05:00 [INFO]\t[TRAIN] epoch=17, iter=14490/20000, loss=0.1980, lr=0.000784, batch_cost=1.1932, reader_cost=0.00010, ips=3.3523 samples/sec | ETA 01:49:34\n",
      "2021-04-13 18:05:10 [INFO]\t[TRAIN] epoch=17, iter=14500/20000, loss=0.2013, lr=0.000782, batch_cost=1.0081, reader_cost=0.00010, ips=3.9680 samples/sec | ETA 01:32:24\n",
      "2021-04-13 18:05:23 [INFO]\t[TRAIN] epoch=17, iter=14510/20000, loss=0.2035, lr=0.000781, batch_cost=1.2844, reader_cost=0.10308, ips=3.1142 samples/sec | ETA 01:57:31\n",
      "2021-04-13 18:05:33 [INFO]\t[TRAIN] epoch=17, iter=14520/20000, loss=0.1729, lr=0.000780, batch_cost=1.0370, reader_cost=0.00010, ips=3.8572 samples/sec | ETA 01:34:42\n",
      "2021-04-13 18:05:45 [INFO]\t[TRAIN] epoch=17, iter=14530/20000, loss=0.1912, lr=0.000779, batch_cost=1.1307, reader_cost=0.00010, ips=3.5375 samples/sec | ETA 01:43:05\n",
      "2021-04-13 18:05:55 [INFO]\t[TRAIN] epoch=17, iter=14540/20000, loss=0.1870, lr=0.000777, batch_cost=1.0830, reader_cost=0.00009, ips=3.6935 samples/sec | ETA 01:38:33\n",
      "2021-04-13 18:06:07 [INFO]\t[TRAIN] epoch=17, iter=14550/20000, loss=0.1832, lr=0.000776, batch_cost=1.1170, reader_cost=0.00009, ips=3.5810 samples/sec | ETA 01:41:27\n",
      "2021-04-13 18:06:18 [INFO]\t[TRAIN] epoch=17, iter=14560/20000, loss=0.1638, lr=0.000775, batch_cost=1.1233, reader_cost=0.00009, ips=3.5611 samples/sec | ETA 01:41:50\n",
      "2021-04-13 18:06:29 [INFO]\t[TRAIN] epoch=17, iter=14570/20000, loss=0.1838, lr=0.000773, batch_cost=1.1599, reader_cost=0.00009, ips=3.4487 samples/sec | ETA 01:44:58\n",
      "2021-04-13 18:06:41 [INFO]\t[TRAIN] epoch=17, iter=14580/20000, loss=0.2016, lr=0.000772, batch_cost=1.1543, reader_cost=0.00009, ips=3.4652 samples/sec | ETA 01:44:16\n",
      "2021-04-13 18:06:52 [INFO]\t[TRAIN] epoch=17, iter=14590/20000, loss=0.1842, lr=0.000771, batch_cost=1.1230, reader_cost=0.00094, ips=3.5620 samples/sec | ETA 01:41:15\n",
      "2021-04-13 18:07:03 [INFO]\t[TRAIN] epoch=17, iter=14600/20000, loss=0.1770, lr=0.000770, batch_cost=1.1133, reader_cost=0.00010, ips=3.5930 samples/sec | ETA 01:40:11\n",
      "2021-04-13 18:07:14 [INFO]\t[TRAIN] epoch=17, iter=14610/20000, loss=0.2379, lr=0.000768, batch_cost=1.0799, reader_cost=0.00009, ips=3.7039 samples/sec | ETA 01:37:00\n",
      "2021-04-13 18:07:25 [INFO]\t[TRAIN] epoch=17, iter=14620/20000, loss=0.1520, lr=0.000767, batch_cost=1.0607, reader_cost=0.00009, ips=3.7710 samples/sec | ETA 01:35:06\n",
      "2021-04-13 18:07:36 [INFO]\t[TRAIN] epoch=17, iter=14630/20000, loss=0.1818, lr=0.000766, batch_cost=1.1333, reader_cost=0.00010, ips=3.5295 samples/sec | ETA 01:41:25\n",
      "2021-04-13 18:07:48 [INFO]\t[TRAIN] epoch=17, iter=14640/20000, loss=0.2160, lr=0.000764, batch_cost=1.2448, reader_cost=0.00009, ips=3.2135 samples/sec | ETA 01:51:11\n",
      "2021-04-13 18:07:59 [INFO]\t[TRAIN] epoch=17, iter=14650/20000, loss=0.1652, lr=0.000763, batch_cost=1.0904, reader_cost=0.00010, ips=3.6682 samples/sec | ETA 01:37:13\n",
      "2021-04-13 18:08:10 [INFO]\t[TRAIN] epoch=17, iter=14660/20000, loss=0.1533, lr=0.000762, batch_cost=1.0759, reader_cost=0.00009, ips=3.7179 samples/sec | ETA 01:35:45\n",
      "2021-04-13 18:08:21 [INFO]\t[TRAIN] epoch=17, iter=14670/20000, loss=0.1759, lr=0.000761, batch_cost=1.0610, reader_cost=0.00009, ips=3.7700 samples/sec | ETA 01:34:15\n",
      "2021-04-13 18:08:32 [INFO]\t[TRAIN] epoch=17, iter=14680/20000, loss=0.2881, lr=0.000759, batch_cost=1.1401, reader_cost=0.00010, ips=3.5085 samples/sec | ETA 01:41:05\n",
      "2021-04-13 18:08:44 [INFO]\t[TRAIN] epoch=17, iter=14690/20000, loss=0.1861, lr=0.000758, batch_cost=1.1602, reader_cost=0.00009, ips=3.4478 samples/sec | ETA 01:42:40\n",
      "2021-04-13 18:08:55 [INFO]\t[TRAIN] epoch=17, iter=14700/20000, loss=0.1901, lr=0.000757, batch_cost=1.0769, reader_cost=0.00008, ips=3.7143 samples/sec | ETA 01:35:07\n",
      "2021-04-13 18:09:06 [INFO]\t[TRAIN] epoch=17, iter=14710/20000, loss=0.2165, lr=0.000755, batch_cost=1.1665, reader_cost=0.00009, ips=3.4290 samples/sec | ETA 01:42:50\n",
      "2021-04-13 18:09:17 [INFO]\t[TRAIN] epoch=17, iter=14720/20000, loss=0.1984, lr=0.000754, batch_cost=1.0545, reader_cost=0.00009, ips=3.7933 samples/sec | ETA 01:32:47\n",
      "2021-04-13 18:09:27 [INFO]\t[TRAIN] epoch=17, iter=14730/20000, loss=0.1747, lr=0.000753, batch_cost=1.0723, reader_cost=0.00009, ips=3.7302 samples/sec | ETA 01:34:11\n",
      "2021-04-13 18:09:38 [INFO]\t[TRAIN] epoch=17, iter=14740/20000, loss=0.1911, lr=0.000752, batch_cost=1.0966, reader_cost=0.00010, ips=3.6478 samples/sec | ETA 01:36:07\n",
      "2021-04-13 18:09:50 [INFO]\t[TRAIN] epoch=17, iter=14750/20000, loss=0.2253, lr=0.000750, batch_cost=1.1417, reader_cost=0.00010, ips=3.5036 samples/sec | ETA 01:39:53\n",
      "2021-04-13 18:10:02 [INFO]\t[TRAIN] epoch=17, iter=14760/20000, loss=0.1657, lr=0.000749, batch_cost=1.1710, reader_cost=0.00009, ips=3.4158 samples/sec | ETA 01:42:16\n",
      "2021-04-13 18:10:13 [INFO]\t[TRAIN] epoch=17, iter=14770/20000, loss=0.1897, lr=0.000748, batch_cost=1.1585, reader_cost=0.00027, ips=3.4528 samples/sec | ETA 01:40:58\n",
      "2021-04-13 18:10:26 [INFO]\t[TRAIN] epoch=17, iter=14780/20000, loss=0.1949, lr=0.000746, batch_cost=1.2641, reader_cost=0.00010, ips=3.1643 samples/sec | ETA 01:49:58\n",
      "2021-04-13 18:10:38 [INFO]\t[TRAIN] epoch=17, iter=14790/20000, loss=0.2032, lr=0.000745, batch_cost=1.1675, reader_cost=0.00120, ips=3.4260 samples/sec | ETA 01:41:22\n",
      "2021-04-13 18:10:50 [INFO]\t[TRAIN] epoch=17, iter=14800/20000, loss=0.2088, lr=0.000744, batch_cost=1.2107, reader_cost=0.00009, ips=3.3038 samples/sec | ETA 01:44:55\n",
      "2021-04-13 18:11:00 [INFO]\t[TRAIN] epoch=17, iter=14810/20000, loss=0.1536, lr=0.000743, batch_cost=1.0240, reader_cost=0.00010, ips=3.9064 samples/sec | ETA 01:28:34\n",
      "2021-04-13 18:11:11 [INFO]\t[TRAIN] epoch=17, iter=14820/20000, loss=0.1537, lr=0.000741, batch_cost=1.1086, reader_cost=0.00009, ips=3.6082 samples/sec | ETA 01:35:42\n",
      "2021-04-13 18:11:22 [INFO]\t[TRAIN] epoch=17, iter=14830/20000, loss=0.1853, lr=0.000740, batch_cost=1.1527, reader_cost=0.00009, ips=3.4702 samples/sec | ETA 01:39:19\n",
      "2021-04-13 18:11:33 [INFO]\t[TRAIN] epoch=17, iter=14840/20000, loss=0.1968, lr=0.000739, batch_cost=1.0808, reader_cost=0.00009, ips=3.7010 samples/sec | ETA 01:32:56\n",
      "2021-04-13 18:11:44 [INFO]\t[TRAIN] epoch=17, iter=14850/20000, loss=0.2128, lr=0.000737, batch_cost=1.1169, reader_cost=0.00009, ips=3.5814 samples/sec | ETA 01:35:51\n",
      "2021-04-13 18:11:56 [INFO]\t[TRAIN] epoch=17, iter=14860/20000, loss=0.1869, lr=0.000736, batch_cost=1.1786, reader_cost=0.00009, ips=3.3939 samples/sec | ETA 01:40:58\n",
      "2021-04-13 18:12:08 [INFO]\t[TRAIN] epoch=17, iter=14870/20000, loss=0.2000, lr=0.000735, batch_cost=1.1334, reader_cost=0.00009, ips=3.5292 samples/sec | ETA 01:36:54\n",
      "2021-04-13 18:12:19 [INFO]\t[TRAIN] epoch=18, iter=14880/20000, loss=0.1813, lr=0.000734, batch_cost=1.1006, reader_cost=0.00009, ips=3.6344 samples/sec | ETA 01:33:54\n",
      "2021-04-13 18:12:29 [INFO]\t[TRAIN] epoch=18, iter=14890/20000, loss=0.1998, lr=0.000732, batch_cost=1.0618, reader_cost=0.00008, ips=3.7672 samples/sec | ETA 01:30:25\n",
      "2021-04-13 18:12:40 [INFO]\t[TRAIN] epoch=18, iter=14900/20000, loss=0.2044, lr=0.000731, batch_cost=1.1184, reader_cost=0.00009, ips=3.5766 samples/sec | ETA 01:35:03\n",
      "2021-04-13 18:12:52 [INFO]\t[TRAIN] epoch=18, iter=14910/20000, loss=0.2010, lr=0.000730, batch_cost=1.1709, reader_cost=0.00009, ips=3.4163 samples/sec | ETA 01:39:19\n",
      "2021-04-13 18:13:04 [INFO]\t[TRAIN] epoch=18, iter=14920/20000, loss=0.1863, lr=0.000728, batch_cost=1.2225, reader_cost=0.00008, ips=3.2721 samples/sec | ETA 01:43:30\n",
      "2021-04-13 18:13:15 [INFO]\t[TRAIN] epoch=18, iter=14930/20000, loss=0.1770, lr=0.000727, batch_cost=1.0965, reader_cost=0.00010, ips=3.6479 samples/sec | ETA 01:32:39\n",
      "2021-04-13 18:13:26 [INFO]\t[TRAIN] epoch=18, iter=14940/20000, loss=0.2217, lr=0.000726, batch_cost=1.1166, reader_cost=0.00010, ips=3.5822 samples/sec | ETA 01:34:10\n",
      "2021-04-13 18:13:37 [INFO]\t[TRAIN] epoch=18, iter=14950/20000, loss=0.2171, lr=0.000725, batch_cost=1.1026, reader_cost=0.00010, ips=3.6277 samples/sec | ETA 01:32:48\n",
      "2021-04-13 18:13:49 [INFO]\t[TRAIN] epoch=18, iter=14960/20000, loss=0.1981, lr=0.000723, batch_cost=1.1075, reader_cost=0.00009, ips=3.6117 samples/sec | ETA 01:33:01\n",
      "2021-04-13 18:13:59 [INFO]\t[TRAIN] epoch=18, iter=14970/20000, loss=0.2197, lr=0.000722, batch_cost=1.0462, reader_cost=0.00009, ips=3.8233 samples/sec | ETA 01:27:42\n",
      "2021-04-13 18:14:11 [INFO]\t[TRAIN] epoch=18, iter=14980/20000, loss=0.1831, lr=0.000721, batch_cost=1.2055, reader_cost=0.00010, ips=3.3181 samples/sec | ETA 01:40:51\n",
      "2021-04-13 18:14:22 [INFO]\t[TRAIN] epoch=18, iter=14990/20000, loss=0.2162, lr=0.000719, batch_cost=1.1258, reader_cost=0.00009, ips=3.5532 samples/sec | ETA 01:34:00\n",
      "2021-04-13 18:14:33 [INFO]\t[TRAIN] epoch=18, iter=15000/20000, loss=0.1781, lr=0.000718, batch_cost=1.0526, reader_cost=0.00009, ips=3.8000 samples/sec | ETA 01:27:43\n",
      "2021-04-13 18:14:33 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 112s 224ms/step - batch_cost: 0.2241 - reader cost: 0.001\n",
      "2021-04-13 18:16:25 [INFO]\t[EVAL] #Images=500 mIoU=0.2109 Acc=0.9880 Kappa=0.6192 \n",
      "2021-04-13 18:16:25 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9894 0.1995 0.525  0.4879 0.1663 0.3084 0.4186 0.     0.     0.\n",
      " 0.     0.0683 0.     0.     0.    ]\n",
      "2021-04-13 18:16:25 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9944 0.2898 0.7031 0.6066 0.2657 0.3886 0.5546 0.     0.     0.\n",
      " 0.     0.3283 0.     0.     0.    ]\n",
      "2021-04-13 18:16:28 [INFO]\t[EVAL] The model with the best validation mIoU (0.2109) was saved at iter 15000.\n",
      "2021-04-13 18:16:39 [INFO]\t[TRAIN] epoch=18, iter=15010/20000, loss=0.1940, lr=0.000717, batch_cost=1.0933, reader_cost=0.00010, ips=3.6586 samples/sec | ETA 01:30:55\n",
      "2021-04-13 18:16:50 [INFO]\t[TRAIN] epoch=18, iter=15020/20000, loss=0.2031, lr=0.000715, batch_cost=1.1060, reader_cost=0.00008, ips=3.6166 samples/sec | ETA 01:31:47\n",
      "2021-04-13 18:17:01 [INFO]\t[TRAIN] epoch=18, iter=15030/20000, loss=0.1894, lr=0.000714, batch_cost=1.1158, reader_cost=0.00011, ips=3.5848 samples/sec | ETA 01:32:25\n",
      "2021-04-13 18:17:13 [INFO]\t[TRAIN] epoch=18, iter=15040/20000, loss=0.2125, lr=0.000713, batch_cost=1.1311, reader_cost=0.00010, ips=3.5365 samples/sec | ETA 01:33:30\n",
      "2021-04-13 18:17:24 [INFO]\t[TRAIN] epoch=18, iter=15050/20000, loss=0.1529, lr=0.000712, batch_cost=1.0953, reader_cost=0.00008, ips=3.6519 samples/sec | ETA 01:30:21\n",
      "2021-04-13 18:17:35 [INFO]\t[TRAIN] epoch=18, iter=15060/20000, loss=0.1956, lr=0.000710, batch_cost=1.1598, reader_cost=0.00010, ips=3.4488 samples/sec | ETA 01:35:29\n",
      "2021-04-13 18:17:46 [INFO]\t[TRAIN] epoch=18, iter=15070/20000, loss=0.2373, lr=0.000709, batch_cost=1.1041, reader_cost=0.00009, ips=3.6228 samples/sec | ETA 01:30:43\n",
      "2021-04-13 18:17:57 [INFO]\t[TRAIN] epoch=18, iter=15080/20000, loss=0.2078, lr=0.000708, batch_cost=1.0731, reader_cost=0.00009, ips=3.7274 samples/sec | ETA 01:27:59\n",
      "2021-04-13 18:18:08 [INFO]\t[TRAIN] epoch=18, iter=15090/20000, loss=0.1862, lr=0.000706, batch_cost=1.1348, reader_cost=0.00010, ips=3.5247 samples/sec | ETA 01:32:52\n",
      "2021-04-13 18:18:21 [INFO]\t[TRAIN] epoch=18, iter=15100/20000, loss=0.2063, lr=0.000705, batch_cost=1.2192, reader_cost=0.00010, ips=3.2807 samples/sec | ETA 01:39:34\n",
      "2021-04-13 18:18:32 [INFO]\t[TRAIN] epoch=18, iter=15110/20000, loss=0.1973, lr=0.000704, batch_cost=1.1657, reader_cost=0.00009, ips=3.4316 samples/sec | ETA 01:35:00\n",
      "2021-04-13 18:18:44 [INFO]\t[TRAIN] epoch=18, iter=15120/20000, loss=0.2166, lr=0.000703, batch_cost=1.1418, reader_cost=0.00010, ips=3.5034 samples/sec | ETA 01:32:51\n",
      "2021-04-13 18:18:55 [INFO]\t[TRAIN] epoch=18, iter=15130/20000, loss=0.2309, lr=0.000701, batch_cost=1.1631, reader_cost=0.00010, ips=3.4390 samples/sec | ETA 01:34:24\n",
      "2021-04-13 18:19:07 [INFO]\t[TRAIN] epoch=18, iter=15140/20000, loss=0.1922, lr=0.000700, batch_cost=1.1416, reader_cost=0.00009, ips=3.5040 samples/sec | ETA 01:32:28\n",
      "2021-04-13 18:19:17 [INFO]\t[TRAIN] epoch=18, iter=15150/20000, loss=0.1719, lr=0.000699, batch_cost=1.0697, reader_cost=0.00010, ips=3.7395 samples/sec | ETA 01:26:27\n",
      "2021-04-13 18:19:29 [INFO]\t[TRAIN] epoch=18, iter=15160/20000, loss=0.1725, lr=0.000697, batch_cost=1.1270, reader_cost=0.00008, ips=3.5493 samples/sec | ETA 01:30:54\n",
      "2021-04-13 18:19:40 [INFO]\t[TRAIN] epoch=18, iter=15170/20000, loss=0.1901, lr=0.000696, batch_cost=1.1692, reader_cost=0.00010, ips=3.4213 samples/sec | ETA 01:34:07\n",
      "2021-04-13 18:19:51 [INFO]\t[TRAIN] epoch=18, iter=15180/20000, loss=0.1915, lr=0.000695, batch_cost=1.0714, reader_cost=0.00008, ips=3.7335 samples/sec | ETA 01:26:04\n",
      "2021-04-13 18:20:03 [INFO]\t[TRAIN] epoch=18, iter=15190/20000, loss=0.1946, lr=0.000693, batch_cost=1.1787, reader_cost=0.00009, ips=3.3937 samples/sec | ETA 01:34:29\n",
      "2021-04-13 18:20:14 [INFO]\t[TRAIN] epoch=18, iter=15200/20000, loss=0.1903, lr=0.000692, batch_cost=1.0707, reader_cost=0.00009, ips=3.7360 samples/sec | ETA 01:25:39\n",
      "2021-04-13 18:20:25 [INFO]\t[TRAIN] epoch=18, iter=15210/20000, loss=0.2020, lr=0.000691, batch_cost=1.1188, reader_cost=0.00009, ips=3.5754 samples/sec | ETA 01:29:18\n",
      "2021-04-13 18:20:36 [INFO]\t[TRAIN] epoch=18, iter=15220/20000, loss=0.1719, lr=0.000690, batch_cost=1.1280, reader_cost=0.00009, ips=3.5461 samples/sec | ETA 01:29:51\n",
      "2021-04-13 18:20:47 [INFO]\t[TRAIN] epoch=18, iter=15230/20000, loss=0.1685, lr=0.000688, batch_cost=1.0701, reader_cost=0.00009, ips=3.7379 samples/sec | ETA 01:25:04\n",
      "2021-04-13 18:20:58 [INFO]\t[TRAIN] epoch=18, iter=15240/20000, loss=0.1683, lr=0.000687, batch_cost=1.0782, reader_cost=0.00016, ips=3.7100 samples/sec | ETA 01:25:32\n",
      "2021-04-13 18:21:09 [INFO]\t[TRAIN] epoch=18, iter=15250/20000, loss=0.2120, lr=0.000686, batch_cost=1.1103, reader_cost=0.00009, ips=3.6025 samples/sec | ETA 01:27:54\n",
      "2021-04-13 18:21:20 [INFO]\t[TRAIN] epoch=18, iter=15260/20000, loss=0.1774, lr=0.000684, batch_cost=1.1010, reader_cost=0.00012, ips=3.6332 samples/sec | ETA 01:26:58\n",
      "2021-04-13 18:21:31 [INFO]\t[TRAIN] epoch=18, iter=15270/20000, loss=0.2053, lr=0.000683, batch_cost=1.1085, reader_cost=0.00011, ips=3.6085 samples/sec | ETA 01:27:23\n",
      "2021-04-13 18:21:42 [INFO]\t[TRAIN] epoch=18, iter=15280/20000, loss=0.1877, lr=0.000682, batch_cost=1.0773, reader_cost=0.00010, ips=3.7131 samples/sec | ETA 01:24:44\n",
      "2021-04-13 18:21:52 [INFO]\t[TRAIN] epoch=18, iter=15290/20000, loss=0.1778, lr=0.000680, batch_cost=1.0658, reader_cost=0.00009, ips=3.7531 samples/sec | ETA 01:23:39\n",
      "2021-04-13 18:22:04 [INFO]\t[TRAIN] epoch=18, iter=15300/20000, loss=0.1790, lr=0.000679, batch_cost=1.1481, reader_cost=0.00010, ips=3.4839 samples/sec | ETA 01:29:56\n",
      "2021-04-13 18:22:16 [INFO]\t[TRAIN] epoch=18, iter=15310/20000, loss=0.1852, lr=0.000678, batch_cost=1.2114, reader_cost=0.00011, ips=3.3021 samples/sec | ETA 01:34:41\n",
      "2021-04-13 18:22:27 [INFO]\t[TRAIN] epoch=18, iter=15320/20000, loss=0.2257, lr=0.000677, batch_cost=1.1348, reader_cost=0.00010, ips=3.5247 samples/sec | ETA 01:28:31\n",
      "2021-04-13 18:22:38 [INFO]\t[TRAIN] epoch=18, iter=15330/20000, loss=0.1830, lr=0.000675, batch_cost=1.0929, reader_cost=0.00009, ips=3.6598 samples/sec | ETA 01:25:04\n",
      "2021-04-13 18:22:49 [INFO]\t[TRAIN] epoch=18, iter=15340/20000, loss=0.1889, lr=0.000674, batch_cost=1.0790, reader_cost=0.00009, ips=3.7070 samples/sec | ETA 01:23:48\n",
      "2021-04-13 18:23:00 [INFO]\t[TRAIN] epoch=18, iter=15350/20000, loss=0.2037, lr=0.000673, batch_cost=1.1668, reader_cost=0.00010, ips=3.4283 samples/sec | ETA 01:30:25\n",
      "2021-04-13 18:23:12 [INFO]\t[TRAIN] epoch=18, iter=15360/20000, loss=0.1858, lr=0.000671, batch_cost=1.1365, reader_cost=0.00010, ips=3.5196 samples/sec | ETA 01:27:53\n",
      "2021-04-13 18:23:24 [INFO]\t[TRAIN] epoch=18, iter=15370/20000, loss=0.2158, lr=0.000670, batch_cost=1.1912, reader_cost=0.00010, ips=3.3579 samples/sec | ETA 01:31:55\n",
      "2021-04-13 18:23:35 [INFO]\t[TRAIN] epoch=18, iter=15380/20000, loss=0.2144, lr=0.000669, batch_cost=1.1507, reader_cost=0.05542, ips=3.4762 samples/sec | ETA 01:28:36\n",
      "2021-04-13 18:23:47 [INFO]\t[TRAIN] epoch=18, iter=15390/20000, loss=0.1877, lr=0.000667, batch_cost=1.1313, reader_cost=0.00067, ips=3.5359 samples/sec | ETA 01:26:55\n",
      "2021-04-13 18:23:57 [INFO]\t[TRAIN] epoch=18, iter=15400/20000, loss=0.2080, lr=0.000666, batch_cost=1.0312, reader_cost=0.00009, ips=3.8791 samples/sec | ETA 01:19:03\n",
      "2021-04-13 18:24:08 [INFO]\t[TRAIN] epoch=18, iter=15410/20000, loss=0.1776, lr=0.000665, batch_cost=1.0836, reader_cost=0.00010, ips=3.6915 samples/sec | ETA 01:22:53\n",
      "2021-04-13 18:24:20 [INFO]\t[TRAIN] epoch=18, iter=15420/20000, loss=0.2456, lr=0.000664, batch_cost=1.2701, reader_cost=0.00010, ips=3.1493 samples/sec | ETA 01:36:57\n",
      "2021-04-13 18:24:32 [INFO]\t[TRAIN] epoch=18, iter=15430/20000, loss=0.1911, lr=0.000662, batch_cost=1.1484, reader_cost=0.00008, ips=3.4832 samples/sec | ETA 01:27:28\n",
      "2021-04-13 18:24:43 [INFO]\t[TRAIN] epoch=18, iter=15440/20000, loss=0.1731, lr=0.000661, batch_cost=1.1033, reader_cost=0.00010, ips=3.6254 samples/sec | ETA 01:23:51\n",
      "2021-04-13 18:24:54 [INFO]\t[TRAIN] epoch=18, iter=15450/20000, loss=0.1733, lr=0.000660, batch_cost=1.1313, reader_cost=0.00010, ips=3.5357 samples/sec | ETA 01:25:47\n",
      "2021-04-13 18:25:05 [INFO]\t[TRAIN] epoch=18, iter=15460/20000, loss=0.2247, lr=0.000658, batch_cost=1.1168, reader_cost=0.00009, ips=3.5817 samples/sec | ETA 01:24:30\n",
      "2021-04-13 18:25:17 [INFO]\t[TRAIN] epoch=18, iter=15470/20000, loss=0.1759, lr=0.000657, batch_cost=1.1641, reader_cost=0.00009, ips=3.4362 samples/sec | ETA 01:27:53\n",
      "2021-04-13 18:25:29 [INFO]\t[TRAIN] epoch=18, iter=15480/20000, loss=0.2777, lr=0.000656, batch_cost=1.1488, reader_cost=0.00009, ips=3.4820 samples/sec | ETA 01:26:32\n",
      "2021-04-13 18:25:40 [INFO]\t[TRAIN] epoch=18, iter=15490/20000, loss=0.1961, lr=0.000654, batch_cost=1.1826, reader_cost=0.00010, ips=3.3825 samples/sec | ETA 01:28:53\n",
      "2021-04-13 18:25:52 [INFO]\t[TRAIN] epoch=18, iter=15500/20000, loss=0.2223, lr=0.000653, batch_cost=1.1289, reader_cost=0.00008, ips=3.5432 samples/sec | ETA 01:24:40\n",
      "2021-04-13 18:26:03 [INFO]\t[TRAIN] epoch=18, iter=15510/20000, loss=0.2252, lr=0.000652, batch_cost=1.1340, reader_cost=0.00009, ips=3.5273 samples/sec | ETA 01:24:51\n",
      "2021-04-13 18:26:15 [INFO]\t[TRAIN] epoch=18, iter=15520/20000, loss=0.1549, lr=0.000651, batch_cost=1.1939, reader_cost=0.00010, ips=3.3503 samples/sec | ETA 01:29:08\n",
      "2021-04-13 18:26:26 [INFO]\t[TRAIN] epoch=18, iter=15530/20000, loss=0.1863, lr=0.000649, batch_cost=1.0735, reader_cost=0.00009, ips=3.7263 samples/sec | ETA 01:19:58\n",
      "2021-04-13 18:26:37 [INFO]\t[TRAIN] epoch=18, iter=15540/20000, loss=0.1548, lr=0.000648, batch_cost=1.0879, reader_cost=0.00009, ips=3.6768 samples/sec | ETA 01:20:52\n",
      "2021-04-13 18:26:48 [INFO]\t[TRAIN] epoch=18, iter=15550/20000, loss=0.2280, lr=0.000647, batch_cost=1.1280, reader_cost=0.00010, ips=3.5460 samples/sec | ETA 01:23:39\n",
      "2021-04-13 18:26:59 [INFO]\t[TRAIN] epoch=18, iter=15560/20000, loss=0.2104, lr=0.000645, batch_cost=1.1329, reader_cost=0.00009, ips=3.5307 samples/sec | ETA 01:23:50\n",
      "2021-04-13 18:27:10 [INFO]\t[TRAIN] epoch=18, iter=15570/20000, loss=0.2103, lr=0.000644, batch_cost=1.1034, reader_cost=0.00009, ips=3.6251 samples/sec | ETA 01:21:28\n",
      "2021-04-13 18:27:21 [INFO]\t[TRAIN] epoch=18, iter=15580/20000, loss=0.1818, lr=0.000643, batch_cost=1.1237, reader_cost=0.00010, ips=3.5597 samples/sec | ETA 01:22:46\n",
      "2021-04-13 18:27:33 [INFO]\t[TRAIN] epoch=18, iter=15590/20000, loss=0.1816, lr=0.000641, batch_cost=1.1265, reader_cost=0.00009, ips=3.5509 samples/sec | ETA 01:22:47\n",
      "2021-04-13 18:27:44 [INFO]\t[TRAIN] epoch=18, iter=15600/20000, loss=0.2145, lr=0.000640, batch_cost=1.1151, reader_cost=0.00008, ips=3.5870 samples/sec | ETA 01:21:46\n",
      "2021-04-13 18:27:55 [INFO]\t[TRAIN] epoch=18, iter=15610/20000, loss=0.1958, lr=0.000639, batch_cost=1.1238, reader_cost=0.00009, ips=3.5593 samples/sec | ETA 01:22:13\n",
      "2021-04-13 18:28:06 [INFO]\t[TRAIN] epoch=18, iter=15620/20000, loss=0.1703, lr=0.000637, batch_cost=1.1157, reader_cost=0.00015, ips=3.5852 samples/sec | ETA 01:21:26\n",
      "2021-04-13 18:28:17 [INFO]\t[TRAIN] epoch=18, iter=15630/20000, loss=0.1976, lr=0.000636, batch_cost=1.1011, reader_cost=0.00010, ips=3.6327 samples/sec | ETA 01:20:11\n",
      "2021-04-13 18:28:29 [INFO]\t[TRAIN] epoch=18, iter=15640/20000, loss=0.1998, lr=0.000635, batch_cost=1.1312, reader_cost=0.00009, ips=3.5361 samples/sec | ETA 01:22:11\n",
      "2021-04-13 18:28:40 [INFO]\t[TRAIN] epoch=18, iter=15650/20000, loss=0.1871, lr=0.000633, batch_cost=1.0899, reader_cost=0.00009, ips=3.6700 samples/sec | ETA 01:19:01\n",
      "2021-04-13 18:28:51 [INFO]\t[TRAIN] epoch=18, iter=15660/20000, loss=0.2041, lr=0.000632, batch_cost=1.1152, reader_cost=0.00010, ips=3.5868 samples/sec | ETA 01:20:39\n",
      "2021-04-13 18:29:02 [INFO]\t[TRAIN] epoch=18, iter=15670/20000, loss=0.1787, lr=0.000631, batch_cost=1.1281, reader_cost=0.00009, ips=3.5459 samples/sec | ETA 01:21:24\n",
      "2021-04-13 18:29:12 [INFO]\t[TRAIN] epoch=18, iter=15680/20000, loss=0.1631, lr=0.000630, batch_cost=1.0552, reader_cost=0.00009, ips=3.7908 samples/sec | ETA 01:15:58\n",
      "2021-04-13 18:29:23 [INFO]\t[TRAIN] epoch=18, iter=15690/20000, loss=0.1965, lr=0.000628, batch_cost=1.0695, reader_cost=0.00009, ips=3.7400 samples/sec | ETA 01:16:49\n",
      "2021-04-13 18:29:34 [INFO]\t[TRAIN] epoch=18, iter=15700/20000, loss=0.1911, lr=0.000627, batch_cost=1.1028, reader_cost=0.00010, ips=3.6271 samples/sec | ETA 01:19:02\n",
      "2021-04-13 18:29:45 [INFO]\t[TRAIN] epoch=18, iter=15710/20000, loss=0.2164, lr=0.000626, batch_cost=1.1119, reader_cost=0.00009, ips=3.5974 samples/sec | ETA 01:19:30\n",
      "2021-04-13 18:29:56 [INFO]\t[TRAIN] epoch=18, iter=15720/20000, loss=0.1913, lr=0.000624, batch_cost=1.1014, reader_cost=0.00009, ips=3.6317 samples/sec | ETA 01:18:34\n",
      "2021-04-13 18:30:08 [INFO]\t[TRAIN] epoch=18, iter=15730/20000, loss=0.2203, lr=0.000623, batch_cost=1.1362, reader_cost=0.00010, ips=3.5206 samples/sec | ETA 01:20:51\n",
      "2021-04-13 18:30:19 [INFO]\t[TRAIN] epoch=18, iter=15740/20000, loss=0.1932, lr=0.000622, batch_cost=1.1520, reader_cost=0.00009, ips=3.4722 samples/sec | ETA 01:21:47\n",
      "2021-04-13 18:30:31 [INFO]\t[TRAIN] epoch=18, iter=15750/20000, loss=0.2253, lr=0.000620, batch_cost=1.1644, reader_cost=0.00009, ips=3.4354 samples/sec | ETA 01:22:28\n",
      "2021-04-13 18:30:31 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 106s 212ms/step - batch_cost: 0.2121 - reader cost: 0.00\n",
      "2021-04-13 18:32:17 [INFO]\t[EVAL] #Images=500 mIoU=0.2148 Acc=0.9883 Kappa=0.6184 \n",
      "2021-04-13 18:32:17 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9898 0.1983 0.5218 0.4919 0.1706 0.3108 0.4205 0.     0.     0.\n",
      " 0.     0.1184 0.     0.     0.    ]\n",
      "2021-04-13 18:32:17 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9943 0.2804 0.7294 0.6005 0.2446 0.3832 0.6083 0.     0.     0.\n",
      " 0.     0.2494 0.     0.     0.    ]\n",
      "2021-04-13 18:32:20 [INFO]\t[EVAL] The model with the best validation mIoU (0.2148) was saved at iter 15750.\n",
      "2021-04-13 18:32:32 [INFO]\t[TRAIN] epoch=19, iter=15760/20000, loss=0.1850, lr=0.000619, batch_cost=1.1375, reader_cost=0.00011, ips=3.5166 samples/sec | ETA 01:20:22\n",
      "2021-04-13 18:32:44 [INFO]\t[TRAIN] epoch=19, iter=15770/20000, loss=0.1980, lr=0.000618, batch_cost=1.1883, reader_cost=0.00008, ips=3.3663 samples/sec | ETA 01:23:46\n",
      "2021-04-13 18:32:56 [INFO]\t[TRAIN] epoch=19, iter=15780/20000, loss=0.2217, lr=0.000616, batch_cost=1.2271, reader_cost=0.00010, ips=3.2596 samples/sec | ETA 01:26:18\n",
      "2021-04-13 18:33:07 [INFO]\t[TRAIN] epoch=19, iter=15790/20000, loss=0.1797, lr=0.000615, batch_cost=1.1518, reader_cost=0.00009, ips=3.4730 samples/sec | ETA 01:20:48\n",
      "2021-04-13 18:33:18 [INFO]\t[TRAIN] epoch=19, iter=15800/20000, loss=0.1742, lr=0.000614, batch_cost=1.0597, reader_cost=0.00010, ips=3.7746 samples/sec | ETA 01:14:10\n",
      "2021-04-13 18:33:29 [INFO]\t[TRAIN] epoch=19, iter=15810/20000, loss=0.1899, lr=0.000612, batch_cost=1.1290, reader_cost=0.00010, ips=3.5431 samples/sec | ETA 01:18:50\n",
      "2021-04-13 18:33:41 [INFO]\t[TRAIN] epoch=19, iter=15820/20000, loss=0.1955, lr=0.000611, batch_cost=1.1700, reader_cost=0.00010, ips=3.4187 samples/sec | ETA 01:21:30\n",
      "2021-04-13 18:33:52 [INFO]\t[TRAIN] epoch=19, iter=15830/20000, loss=0.1917, lr=0.000610, batch_cost=1.0808, reader_cost=0.00010, ips=3.7010 samples/sec | ETA 01:15:06\n",
      "2021-04-13 18:34:03 [INFO]\t[TRAIN] epoch=19, iter=15840/20000, loss=0.2370, lr=0.000609, batch_cost=1.0899, reader_cost=0.00009, ips=3.6702 samples/sec | ETA 01:15:33\n",
      "2021-04-13 18:34:15 [INFO]\t[TRAIN] epoch=19, iter=15850/20000, loss=0.2179, lr=0.000607, batch_cost=1.1953, reader_cost=0.00009, ips=3.3464 samples/sec | ETA 01:22:40\n",
      "2021-04-13 18:34:26 [INFO]\t[TRAIN] epoch=19, iter=15860/20000, loss=0.2200, lr=0.000606, batch_cost=1.1013, reader_cost=0.00010, ips=3.6320 samples/sec | ETA 01:15:59\n",
      "2021-04-13 18:34:37 [INFO]\t[TRAIN] epoch=19, iter=15870/20000, loss=0.2135, lr=0.000605, batch_cost=1.0952, reader_cost=0.00008, ips=3.6523 samples/sec | ETA 01:15:23\n",
      "2021-04-13 18:34:48 [INFO]\t[TRAIN] epoch=19, iter=15880/20000, loss=0.1814, lr=0.000603, batch_cost=1.1478, reader_cost=0.00011, ips=3.4849 samples/sec | ETA 01:18:48\n",
      "2021-04-13 18:34:59 [INFO]\t[TRAIN] epoch=19, iter=15890/20000, loss=0.2084, lr=0.000602, batch_cost=1.0791, reader_cost=0.00009, ips=3.7067 samples/sec | ETA 01:13:55\n",
      "2021-04-13 18:35:10 [INFO]\t[TRAIN] epoch=19, iter=15900/20000, loss=0.1982, lr=0.000601, batch_cost=1.1051, reader_cost=0.00008, ips=3.6196 samples/sec | ETA 01:15:30\n",
      "2021-04-13 18:35:21 [INFO]\t[TRAIN] epoch=19, iter=15910/20000, loss=0.2000, lr=0.000599, batch_cost=1.1228, reader_cost=0.00009, ips=3.5624 samples/sec | ETA 01:16:32\n",
      "2021-04-13 18:35:32 [INFO]\t[TRAIN] epoch=19, iter=15920/20000, loss=0.1558, lr=0.000598, batch_cost=1.0561, reader_cost=0.00010, ips=3.7876 samples/sec | ETA 01:11:48\n",
      "2021-04-13 18:35:43 [INFO]\t[TRAIN] epoch=19, iter=15930/20000, loss=0.1918, lr=0.000597, batch_cost=1.0936, reader_cost=0.00009, ips=3.6576 samples/sec | ETA 01:14:11\n",
      "2021-04-13 18:35:54 [INFO]\t[TRAIN] epoch=19, iter=15940/20000, loss=0.2008, lr=0.000595, batch_cost=1.1592, reader_cost=0.00009, ips=3.4506 samples/sec | ETA 01:18:26\n",
      "2021-04-13 18:36:05 [INFO]\t[TRAIN] epoch=19, iter=15950/20000, loss=0.2019, lr=0.000594, batch_cost=1.1241, reader_cost=0.00009, ips=3.5583 samples/sec | ETA 01:15:52\n",
      "2021-04-13 18:36:18 [INFO]\t[TRAIN] epoch=19, iter=15960/20000, loss=0.2038, lr=0.000593, batch_cost=1.2362, reader_cost=0.00011, ips=3.2357 samples/sec | ETA 01:23:14\n",
      "2021-04-13 18:36:29 [INFO]\t[TRAIN] epoch=19, iter=15970/20000, loss=0.1809, lr=0.000591, batch_cost=1.0867, reader_cost=0.00010, ips=3.6808 samples/sec | ETA 01:12:59\n",
      "2021-04-13 18:36:39 [INFO]\t[TRAIN] epoch=19, iter=15980/20000, loss=0.1786, lr=0.000590, batch_cost=1.0792, reader_cost=0.00011, ips=3.7063 samples/sec | ETA 01:12:18\n",
      "2021-04-13 18:36:51 [INFO]\t[TRAIN] epoch=19, iter=15990/20000, loss=0.1771, lr=0.000589, batch_cost=1.1873, reader_cost=0.00009, ips=3.3690 samples/sec | ETA 01:19:21\n",
      "2021-04-13 18:37:03 [INFO]\t[TRAIN] epoch=19, iter=16000/20000, loss=0.2970, lr=0.000587, batch_cost=1.1268, reader_cost=0.00010, ips=3.5499 samples/sec | ETA 01:15:07\n",
      "2021-04-13 18:37:13 [INFO]\t[TRAIN] epoch=19, iter=16010/20000, loss=0.1781, lr=0.000586, batch_cost=1.0872, reader_cost=0.00009, ips=3.6793 samples/sec | ETA 01:12:17\n",
      "2021-04-13 18:37:25 [INFO]\t[TRAIN] epoch=19, iter=16020/20000, loss=0.1781, lr=0.000585, batch_cost=1.1616, reader_cost=0.00009, ips=3.4436 samples/sec | ETA 01:17:03\n",
      "2021-04-13 18:37:36 [INFO]\t[TRAIN] epoch=19, iter=16030/20000, loss=0.1926, lr=0.000583, batch_cost=1.0812, reader_cost=0.00011, ips=3.6996 samples/sec | ETA 01:11:32\n",
      "2021-04-13 18:37:47 [INFO]\t[TRAIN] epoch=19, iter=16040/20000, loss=0.2070, lr=0.000582, batch_cost=1.1078, reader_cost=0.00009, ips=3.6106 samples/sec | ETA 01:13:07\n",
      "2021-04-13 18:37:59 [INFO]\t[TRAIN] epoch=19, iter=16050/20000, loss=0.2371, lr=0.000581, batch_cost=1.1867, reader_cost=0.00069, ips=3.3706 samples/sec | ETA 01:18:07\n",
      "2021-04-13 18:38:10 [INFO]\t[TRAIN] epoch=19, iter=16060/20000, loss=0.1790, lr=0.000580, batch_cost=1.1229, reader_cost=0.00010, ips=3.5621 samples/sec | ETA 01:13:44\n",
      "2021-04-13 18:38:21 [INFO]\t[TRAIN] epoch=19, iter=16070/20000, loss=0.2010, lr=0.000578, batch_cost=1.1213, reader_cost=0.00010, ips=3.5672 samples/sec | ETA 01:13:26\n",
      "2021-04-13 18:38:33 [INFO]\t[TRAIN] epoch=19, iter=16080/20000, loss=0.2021, lr=0.000577, batch_cost=1.1241, reader_cost=0.00009, ips=3.5584 samples/sec | ETA 01:13:26\n",
      "2021-04-13 18:38:44 [INFO]\t[TRAIN] epoch=19, iter=16090/20000, loss=0.1903, lr=0.000576, batch_cost=1.0965, reader_cost=0.00009, ips=3.6481 samples/sec | ETA 01:11:27\n",
      "2021-04-13 18:38:54 [INFO]\t[TRAIN] epoch=19, iter=16100/20000, loss=0.2028, lr=0.000574, batch_cost=1.0708, reader_cost=0.00010, ips=3.7355 samples/sec | ETA 01:09:36\n",
      "2021-04-13 18:39:05 [INFO]\t[TRAIN] epoch=19, iter=16110/20000, loss=0.1592, lr=0.000573, batch_cost=1.0674, reader_cost=0.00010, ips=3.7476 samples/sec | ETA 01:09:11\n",
      "2021-04-13 18:39:15 [INFO]\t[TRAIN] epoch=19, iter=16120/20000, loss=0.1744, lr=0.000572, batch_cost=1.0093, reader_cost=0.00009, ips=3.9633 samples/sec | ETA 01:05:15\n",
      "2021-04-13 18:39:26 [INFO]\t[TRAIN] epoch=19, iter=16130/20000, loss=0.1827, lr=0.000570, batch_cost=1.1494, reader_cost=0.00010, ips=3.4799 samples/sec | ETA 01:14:08\n",
      "2021-04-13 18:39:38 [INFO]\t[TRAIN] epoch=19, iter=16140/20000, loss=0.1778, lr=0.000569, batch_cost=1.1666, reader_cost=0.00009, ips=3.4289 samples/sec | ETA 01:15:02\n",
      "2021-04-13 18:39:50 [INFO]\t[TRAIN] epoch=19, iter=16150/20000, loss=0.1937, lr=0.000568, batch_cost=1.2233, reader_cost=0.00009, ips=3.2697 samples/sec | ETA 01:18:29\n",
      "2021-04-13 18:40:01 [INFO]\t[TRAIN] epoch=19, iter=16160/20000, loss=0.1819, lr=0.000566, batch_cost=1.0565, reader_cost=0.00009, ips=3.7862 samples/sec | ETA 01:07:36\n",
      "2021-04-13 18:40:13 [INFO]\t[TRAIN] epoch=19, iter=16170/20000, loss=0.2307, lr=0.000565, batch_cost=1.1893, reader_cost=0.00009, ips=3.3633 samples/sec | ETA 01:15:55\n",
      "2021-04-13 18:40:23 [INFO]\t[TRAIN] epoch=19, iter=16180/20000, loss=0.1914, lr=0.000564, batch_cost=1.0268, reader_cost=0.00010, ips=3.8956 samples/sec | ETA 01:05:22\n",
      "2021-04-13 18:40:35 [INFO]\t[TRAIN] epoch=19, iter=16190/20000, loss=0.1943, lr=0.000562, batch_cost=1.1421, reader_cost=0.00010, ips=3.5023 samples/sec | ETA 01:12:31\n",
      "2021-04-13 18:40:46 [INFO]\t[TRAIN] epoch=19, iter=16200/20000, loss=0.1799, lr=0.000561, batch_cost=1.1408, reader_cost=0.00009, ips=3.5064 samples/sec | ETA 01:12:14\n",
      "2021-04-13 18:40:57 [INFO]\t[TRAIN] epoch=19, iter=16210/20000, loss=0.1898, lr=0.000560, batch_cost=1.0786, reader_cost=0.00010, ips=3.7087 samples/sec | ETA 01:08:07\n",
      "2021-04-13 18:41:08 [INFO]\t[TRAIN] epoch=19, iter=16220/20000, loss=0.1690, lr=0.000558, batch_cost=1.0832, reader_cost=0.00009, ips=3.6927 samples/sec | ETA 01:08:14\n",
      "2021-04-13 18:41:19 [INFO]\t[TRAIN] epoch=19, iter=16230/20000, loss=0.1834, lr=0.000557, batch_cost=1.1039, reader_cost=0.00010, ips=3.6234 samples/sec | ETA 01:09:21\n",
      "2021-04-13 18:41:30 [INFO]\t[TRAIN] epoch=19, iter=16240/20000, loss=0.1999, lr=0.000556, batch_cost=1.1417, reader_cost=0.00009, ips=3.5036 samples/sec | ETA 01:11:32\n",
      "2021-04-13 18:41:40 [INFO]\t[TRAIN] epoch=19, iter=16250/20000, loss=0.1775, lr=0.000554, batch_cost=0.9957, reader_cost=0.00009, ips=4.0172 samples/sec | ETA 01:02:13\n",
      "2021-04-13 18:41:53 [INFO]\t[TRAIN] epoch=19, iter=16260/20000, loss=0.1911, lr=0.000553, batch_cost=1.2710, reader_cost=0.08179, ips=3.1472 samples/sec | ETA 01:19:13\n",
      "2021-04-13 18:42:05 [INFO]\t[TRAIN] epoch=19, iter=16270/20000, loss=0.1595, lr=0.000552, batch_cost=1.2481, reader_cost=0.00010, ips=3.2049 samples/sec | ETA 01:17:35\n",
      "2021-04-13 18:42:17 [INFO]\t[TRAIN] epoch=19, iter=16280/20000, loss=0.2056, lr=0.000550, batch_cost=1.1393, reader_cost=0.00009, ips=3.5110 samples/sec | ETA 01:10:38\n",
      "2021-04-13 18:42:28 [INFO]\t[TRAIN] epoch=19, iter=16290/20000, loss=0.2129, lr=0.000549, batch_cost=1.1485, reader_cost=0.00009, ips=3.4828 samples/sec | ETA 01:11:00\n",
      "2021-04-13 18:42:39 [INFO]\t[TRAIN] epoch=19, iter=16300/20000, loss=0.1946, lr=0.000548, batch_cost=1.1371, reader_cost=0.00009, ips=3.5176 samples/sec | ETA 01:10:07\n",
      "2021-04-13 18:42:50 [INFO]\t[TRAIN] epoch=19, iter=16310/20000, loss=0.1735, lr=0.000546, batch_cost=1.0396, reader_cost=0.00009, ips=3.8475 samples/sec | ETA 01:03:56\n",
      "2021-04-13 18:43:01 [INFO]\t[TRAIN] epoch=19, iter=16320/20000, loss=0.1876, lr=0.000545, batch_cost=1.1299, reader_cost=0.00009, ips=3.5400 samples/sec | ETA 01:09:18\n",
      "2021-04-13 18:43:12 [INFO]\t[TRAIN] epoch=19, iter=16330/20000, loss=0.1901, lr=0.000544, batch_cost=1.1024, reader_cost=0.00010, ips=3.6285 samples/sec | ETA 01:07:25\n",
      "2021-04-13 18:43:23 [INFO]\t[TRAIN] epoch=19, iter=16340/20000, loss=0.1967, lr=0.000542, batch_cost=1.0758, reader_cost=0.00009, ips=3.7182 samples/sec | ETA 01:05:37\n",
      "2021-04-13 18:43:34 [INFO]\t[TRAIN] epoch=19, iter=16350/20000, loss=0.2119, lr=0.000541, batch_cost=1.1292, reader_cost=0.00010, ips=3.5425 samples/sec | ETA 01:08:41\n",
      "2021-04-13 18:43:46 [INFO]\t[TRAIN] epoch=19, iter=16360/20000, loss=0.2229, lr=0.000540, batch_cost=1.1774, reader_cost=0.00010, ips=3.3974 samples/sec | ETA 01:11:25\n",
      "2021-04-13 18:43:57 [INFO]\t[TRAIN] epoch=19, iter=16370/20000, loss=0.1812, lr=0.000538, batch_cost=1.0645, reader_cost=0.00010, ips=3.7575 samples/sec | ETA 01:04:24\n",
      "2021-04-13 18:44:08 [INFO]\t[TRAIN] epoch=19, iter=16380/20000, loss=0.2076, lr=0.000537, batch_cost=1.1053, reader_cost=0.00009, ips=3.6190 samples/sec | ETA 01:06:41\n",
      "2021-04-13 18:44:19 [INFO]\t[TRAIN] epoch=19, iter=16390/20000, loss=0.1761, lr=0.000536, batch_cost=1.0988, reader_cost=0.00009, ips=3.6402 samples/sec | ETA 01:06:06\n",
      "2021-04-13 18:44:30 [INFO]\t[TRAIN] epoch=19, iter=16400/20000, loss=0.1921, lr=0.000534, batch_cost=1.1041, reader_cost=0.00010, ips=3.6228 samples/sec | ETA 01:06:14\n",
      "2021-04-13 18:44:41 [INFO]\t[TRAIN] epoch=19, iter=16410/20000, loss=0.1813, lr=0.000533, batch_cost=1.1177, reader_cost=0.00009, ips=3.5787 samples/sec | ETA 01:06:52\n",
      "2021-04-13 18:44:53 [INFO]\t[TRAIN] epoch=19, iter=16420/20000, loss=0.1860, lr=0.000532, batch_cost=1.2037, reader_cost=0.00009, ips=3.3231 samples/sec | ETA 01:11:49\n",
      "2021-04-13 18:45:04 [INFO]\t[TRAIN] epoch=19, iter=16430/20000, loss=0.1912, lr=0.000530, batch_cost=1.0787, reader_cost=0.00009, ips=3.7080 samples/sec | ETA 01:04:11\n",
      "2021-04-13 18:45:15 [INFO]\t[TRAIN] epoch=19, iter=16440/20000, loss=0.2195, lr=0.000529, batch_cost=1.1020, reader_cost=0.00009, ips=3.6299 samples/sec | ETA 01:05:23\n",
      "2021-04-13 18:45:26 [INFO]\t[TRAIN] epoch=19, iter=16450/20000, loss=0.1862, lr=0.000528, batch_cost=1.1564, reader_cost=0.00010, ips=3.4591 samples/sec | ETA 01:08:25\n",
      "2021-04-13 18:45:38 [INFO]\t[TRAIN] epoch=19, iter=16460/20000, loss=0.1970, lr=0.000526, batch_cost=1.1255, reader_cost=0.00011, ips=3.5540 samples/sec | ETA 01:06:24\n",
      "2021-04-13 18:45:49 [INFO]\t[TRAIN] epoch=19, iter=16470/20000, loss=0.1827, lr=0.000525, batch_cost=1.1642, reader_cost=0.00009, ips=3.4357 samples/sec | ETA 01:08:29\n",
      "2021-04-13 18:46:01 [INFO]\t[TRAIN] epoch=19, iter=16480/20000, loss=0.2018, lr=0.000524, batch_cost=1.2296, reader_cost=0.00009, ips=3.2530 samples/sec | ETA 01:12:08\n",
      "2021-04-13 18:46:12 [INFO]\t[TRAIN] epoch=19, iter=16490/20000, loss=0.2045, lr=0.000522, batch_cost=1.0921, reader_cost=0.00011, ips=3.6626 samples/sec | ETA 01:03:53\n",
      "2021-04-13 18:46:23 [INFO]\t[TRAIN] epoch=19, iter=16500/20000, loss=0.2037, lr=0.000521, batch_cost=1.0751, reader_cost=0.00009, ips=3.7204 samples/sec | ETA 01:02:43\n",
      "2021-04-13 18:46:23 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 107s 213ms/step - batch_cost: 0.2129 - reader cost: 0.001\n",
      "2021-04-13 18:48:10 [INFO]\t[EVAL] #Images=500 mIoU=0.2113 Acc=0.9884 Kappa=0.6195 \n",
      "2021-04-13 18:48:10 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9899 0.1771 0.5222 0.477  0.1671 0.3168 0.4191 0.     0.     0.\n",
      " 0.     0.1001 0.     0.     0.    ]\n",
      "2021-04-13 18:48:10 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9942 0.2829 0.7161 0.5569 0.2441 0.4413 0.6274 0.     0.     0.\n",
      " 0.     0.2684 0.     0.     0.    ]\n",
      "2021-04-13 18:48:12 [INFO]\t[EVAL] The model with the best validation mIoU (0.2148) was saved at iter 15750.\n",
      "2021-04-13 18:48:24 [INFO]\t[TRAIN] epoch=19, iter=16510/20000, loss=0.2045, lr=0.000520, batch_cost=1.1963, reader_cost=0.00010, ips=3.3436 samples/sec | ETA 01:09:35\n",
      "2021-04-13 18:48:35 [INFO]\t[TRAIN] epoch=19, iter=16520/20000, loss=0.1711, lr=0.000518, batch_cost=1.1486, reader_cost=0.00010, ips=3.4825 samples/sec | ETA 01:06:37\n",
      "2021-04-13 18:48:46 [INFO]\t[TRAIN] epoch=19, iter=16530/20000, loss=0.1984, lr=0.000517, batch_cost=1.1211, reader_cost=0.00010, ips=3.5679 samples/sec | ETA 01:04:50\n",
      "2021-04-13 18:48:57 [INFO]\t[TRAIN] epoch=19, iter=16540/20000, loss=0.2038, lr=0.000516, batch_cost=1.0846, reader_cost=0.00009, ips=3.6880 samples/sec | ETA 01:02:32\n",
      "2021-04-13 18:49:09 [INFO]\t[TRAIN] epoch=19, iter=16550/20000, loss=0.1973, lr=0.000514, batch_cost=1.1327, reader_cost=0.00010, ips=3.5314 samples/sec | ETA 01:05:07\n",
      "2021-04-13 18:49:20 [INFO]\t[TRAIN] epoch=19, iter=16560/20000, loss=0.3283, lr=0.000513, batch_cost=1.1032, reader_cost=0.00009, ips=3.6259 samples/sec | ETA 01:03:14\n",
      "2021-04-13 18:49:32 [INFO]\t[TRAIN] epoch=19, iter=16570/20000, loss=0.2220, lr=0.000512, batch_cost=1.2172, reader_cost=0.00009, ips=3.2861 samples/sec | ETA 01:09:35\n",
      "2021-04-13 18:49:43 [INFO]\t[TRAIN] epoch=19, iter=16580/20000, loss=0.2052, lr=0.000510, batch_cost=1.1302, reader_cost=0.00009, ips=3.5393 samples/sec | ETA 01:04:25\n",
      "2021-04-13 18:49:56 [INFO]\t[TRAIN] epoch=19, iter=16590/20000, loss=0.2154, lr=0.000509, batch_cost=1.2757, reader_cost=0.00009, ips=3.1355 samples/sec | ETA 01:12:30\n",
      "2021-04-13 18:50:07 [INFO]\t[TRAIN] epoch=19, iter=16600/20000, loss=0.1872, lr=0.000508, batch_cost=1.1348, reader_cost=0.00009, ips=3.5249 samples/sec | ETA 01:04:18\n",
      "2021-04-13 18:50:18 [INFO]\t[TRAIN] epoch=19, iter=16610/20000, loss=0.1708, lr=0.000506, batch_cost=1.0711, reader_cost=0.00010, ips=3.7346 samples/sec | ETA 01:00:30\n",
      "2021-04-13 18:50:29 [INFO]\t[TRAIN] epoch=19, iter=16620/20000, loss=0.1703, lr=0.000505, batch_cost=1.1439, reader_cost=0.00009, ips=3.4967 samples/sec | ETA 01:04:26\n",
      "2021-04-13 18:50:40 [INFO]\t[TRAIN] epoch=20, iter=16630/20000, loss=0.1669, lr=0.000503, batch_cost=1.0651, reader_cost=0.00009, ips=3.7554 samples/sec | ETA 00:59:49\n",
      "2021-04-13 18:50:51 [INFO]\t[TRAIN] epoch=20, iter=16640/20000, loss=0.2071, lr=0.000502, batch_cost=1.0785, reader_cost=0.00009, ips=3.7090 samples/sec | ETA 01:00:23\n",
      "2021-04-13 18:51:02 [INFO]\t[TRAIN] epoch=20, iter=16650/20000, loss=0.1664, lr=0.000501, batch_cost=1.1462, reader_cost=0.00009, ips=3.4898 samples/sec | ETA 01:03:59\n",
      "2021-04-13 18:51:14 [INFO]\t[TRAIN] epoch=20, iter=16660/20000, loss=0.2194, lr=0.000499, batch_cost=1.1858, reader_cost=0.00009, ips=3.3732 samples/sec | ETA 01:06:00\n",
      "2021-04-13 18:51:26 [INFO]\t[TRAIN] epoch=20, iter=16670/20000, loss=0.1936, lr=0.000498, batch_cost=1.1505, reader_cost=0.00009, ips=3.4766 samples/sec | ETA 01:03:51\n",
      "2021-04-13 18:51:37 [INFO]\t[TRAIN] epoch=20, iter=16680/20000, loss=0.1882, lr=0.000497, batch_cost=1.1142, reader_cost=0.00009, ips=3.5899 samples/sec | ETA 01:01:39\n",
      "2021-04-13 18:51:48 [INFO]\t[TRAIN] epoch=20, iter=16690/20000, loss=0.1745, lr=0.000495, batch_cost=1.0978, reader_cost=0.00009, ips=3.6436 samples/sec | ETA 01:00:33\n",
      "2021-04-13 18:51:59 [INFO]\t[TRAIN] epoch=20, iter=16700/20000, loss=0.1643, lr=0.000494, batch_cost=1.1411, reader_cost=0.00009, ips=3.5054 samples/sec | ETA 01:02:45\n",
      "2021-04-13 18:52:10 [INFO]\t[TRAIN] epoch=20, iter=16710/20000, loss=0.2061, lr=0.000493, batch_cost=1.0810, reader_cost=0.00009, ips=3.7002 samples/sec | ETA 00:59:16\n",
      "2021-04-13 18:52:21 [INFO]\t[TRAIN] epoch=20, iter=16720/20000, loss=0.2074, lr=0.000491, batch_cost=1.1356, reader_cost=0.00009, ips=3.5225 samples/sec | ETA 01:02:04\n",
      "2021-04-13 18:52:32 [INFO]\t[TRAIN] epoch=20, iter=16730/20000, loss=0.1981, lr=0.000490, batch_cost=1.0784, reader_cost=0.00010, ips=3.7092 samples/sec | ETA 00:58:46\n",
      "2021-04-13 18:52:44 [INFO]\t[TRAIN] epoch=20, iter=16740/20000, loss=0.2073, lr=0.000489, batch_cost=1.1712, reader_cost=0.00010, ips=3.4154 samples/sec | ETA 01:03:38\n",
      "2021-04-13 18:52:55 [INFO]\t[TRAIN] epoch=20, iter=16750/20000, loss=0.1839, lr=0.000487, batch_cost=1.1278, reader_cost=0.00009, ips=3.5466 samples/sec | ETA 01:01:05\n",
      "2021-04-13 18:53:06 [INFO]\t[TRAIN] epoch=20, iter=16760/20000, loss=0.2210, lr=0.000486, batch_cost=1.1369, reader_cost=0.00009, ips=3.5183 samples/sec | ETA 01:01:23\n",
      "2021-04-13 18:53:18 [INFO]\t[TRAIN] epoch=20, iter=16770/20000, loss=0.1725, lr=0.000485, batch_cost=1.1404, reader_cost=0.00009, ips=3.5075 samples/sec | ETA 01:01:23\n",
      "2021-04-13 18:53:29 [INFO]\t[TRAIN] epoch=20, iter=16780/20000, loss=0.1783, lr=0.000483, batch_cost=1.1212, reader_cost=0.00011, ips=3.5677 samples/sec | ETA 01:00:10\n",
      "2021-04-13 18:53:40 [INFO]\t[TRAIN] epoch=20, iter=16790/20000, loss=0.2022, lr=0.000482, batch_cost=1.1380, reader_cost=0.00011, ips=3.5149 samples/sec | ETA 01:00:52\n",
      "2021-04-13 18:53:52 [INFO]\t[TRAIN] epoch=20, iter=16800/20000, loss=0.1883, lr=0.000481, batch_cost=1.1166, reader_cost=0.00010, ips=3.5823 samples/sec | ETA 00:59:33\n",
      "2021-04-13 18:54:02 [INFO]\t[TRAIN] epoch=20, iter=16810/20000, loss=0.1787, lr=0.000479, batch_cost=1.0275, reader_cost=0.00010, ips=3.8930 samples/sec | ETA 00:54:37\n",
      "2021-04-13 18:54:13 [INFO]\t[TRAIN] epoch=20, iter=16820/20000, loss=0.2062, lr=0.000478, batch_cost=1.0847, reader_cost=0.00009, ips=3.6877 samples/sec | ETA 00:57:29\n",
      "2021-04-13 18:54:24 [INFO]\t[TRAIN] epoch=20, iter=16830/20000, loss=0.2048, lr=0.000477, batch_cost=1.1645, reader_cost=0.00009, ips=3.4349 samples/sec | ETA 01:01:31\n",
      "2021-04-13 18:54:36 [INFO]\t[TRAIN] epoch=20, iter=16840/20000, loss=0.1897, lr=0.000475, batch_cost=1.1742, reader_cost=0.00011, ips=3.4065 samples/sec | ETA 01:01:50\n",
      "2021-04-13 18:54:48 [INFO]\t[TRAIN] epoch=20, iter=16850/20000, loss=0.2098, lr=0.000474, batch_cost=1.1814, reader_cost=0.00010, ips=3.3858 samples/sec | ETA 01:02:01\n",
      "2021-04-13 18:55:00 [INFO]\t[TRAIN] epoch=20, iter=16860/20000, loss=0.1725, lr=0.000472, batch_cost=1.1799, reader_cost=0.00009, ips=3.3901 samples/sec | ETA 01:01:44\n",
      "2021-04-13 18:55:11 [INFO]\t[TRAIN] epoch=20, iter=16870/20000, loss=0.1841, lr=0.000471, batch_cost=1.0811, reader_cost=0.00009, ips=3.6999 samples/sec | ETA 00:56:23\n",
      "2021-04-13 18:55:21 [INFO]\t[TRAIN] epoch=20, iter=16880/20000, loss=0.1563, lr=0.000470, batch_cost=1.0545, reader_cost=0.00009, ips=3.7933 samples/sec | ETA 00:54:50\n",
      "2021-04-13 18:55:33 [INFO]\t[TRAIN] epoch=20, iter=16890/20000, loss=0.1962, lr=0.000468, batch_cost=1.1457, reader_cost=0.00009, ips=3.4913 samples/sec | ETA 00:59:23\n",
      "2021-04-13 18:55:45 [INFO]\t[TRAIN] epoch=20, iter=16900/20000, loss=0.2187, lr=0.000467, batch_cost=1.1991, reader_cost=0.00009, ips=3.3358 samples/sec | ETA 01:01:57\n",
      "2021-04-13 18:55:55 [INFO]\t[TRAIN] epoch=20, iter=16910/20000, loss=0.1647, lr=0.000466, batch_cost=1.0449, reader_cost=0.00010, ips=3.8283 samples/sec | ETA 00:53:48\n",
      "2021-04-13 18:56:06 [INFO]\t[TRAIN] epoch=20, iter=16920/20000, loss=0.1743, lr=0.000464, batch_cost=1.1015, reader_cost=0.00009, ips=3.6313 samples/sec | ETA 00:56:32\n",
      "2021-04-13 18:56:17 [INFO]\t[TRAIN] epoch=20, iter=16930/20000, loss=0.1985, lr=0.000463, batch_cost=1.0639, reader_cost=0.00010, ips=3.7599 samples/sec | ETA 00:54:26\n",
      "2021-04-13 18:56:27 [INFO]\t[TRAIN] epoch=20, iter=16940/20000, loss=0.1936, lr=0.000462, batch_cost=1.0775, reader_cost=0.00009, ips=3.7122 samples/sec | ETA 00:54:57\n",
      "2021-04-13 18:56:39 [INFO]\t[TRAIN] epoch=20, iter=16950/20000, loss=0.2063, lr=0.000460, batch_cost=1.1136, reader_cost=0.00011, ips=3.5920 samples/sec | ETA 00:56:36\n",
      "2021-04-13 18:56:49 [INFO]\t[TRAIN] epoch=20, iter=16960/20000, loss=0.1850, lr=0.000459, batch_cost=1.0699, reader_cost=0.00009, ips=3.7386 samples/sec | ETA 00:54:12\n",
      "2021-04-13 18:57:01 [INFO]\t[TRAIN] epoch=20, iter=16970/20000, loss=0.1729, lr=0.000458, batch_cost=1.1562, reader_cost=0.00009, ips=3.4597 samples/sec | ETA 00:58:23\n",
      "2021-04-13 18:57:11 [INFO]\t[TRAIN] epoch=20, iter=16980/20000, loss=0.1778, lr=0.000456, batch_cost=1.0623, reader_cost=0.00011, ips=3.7656 samples/sec | ETA 00:53:28\n",
      "2021-04-13 18:57:23 [INFO]\t[TRAIN] epoch=20, iter=16990/20000, loss=0.1888, lr=0.000455, batch_cost=1.1099, reader_cost=0.00009, ips=3.6040 samples/sec | ETA 00:55:40\n",
      "2021-04-13 18:57:34 [INFO]\t[TRAIN] epoch=20, iter=17000/20000, loss=0.1939, lr=0.000453, batch_cost=1.1302, reader_cost=0.00010, ips=3.5391 samples/sec | ETA 00:56:30\n",
      "2021-04-13 18:57:45 [INFO]\t[TRAIN] epoch=20, iter=17010/20000, loss=0.2004, lr=0.000452, batch_cost=1.1108, reader_cost=0.00010, ips=3.6011 samples/sec | ETA 00:55:21\n",
      "2021-04-13 18:57:56 [INFO]\t[TRAIN] epoch=20, iter=17020/20000, loss=0.1883, lr=0.000451, batch_cost=1.1048, reader_cost=0.00011, ips=3.6206 samples/sec | ETA 00:54:52\n",
      "2021-04-13 18:58:08 [INFO]\t[TRAIN] epoch=20, iter=17030/20000, loss=0.1832, lr=0.000449, batch_cost=1.1773, reader_cost=0.00009, ips=3.3976 samples/sec | ETA 00:58:16\n",
      "2021-04-13 18:58:18 [INFO]\t[TRAIN] epoch=20, iter=17040/20000, loss=0.1915, lr=0.000448, batch_cost=1.0687, reader_cost=0.00009, ips=3.7429 samples/sec | ETA 00:52:43\n",
      "2021-04-13 18:58:31 [INFO]\t[TRAIN] epoch=20, iter=17050/20000, loss=0.1763, lr=0.000447, batch_cost=1.2162, reader_cost=0.00009, ips=3.2888 samples/sec | ETA 00:59:47\n",
      "2021-04-13 18:58:42 [INFO]\t[TRAIN] epoch=20, iter=17060/20000, loss=0.2222, lr=0.000445, batch_cost=1.1499, reader_cost=0.00009, ips=3.4785 samples/sec | ETA 00:56:20\n",
      "2021-04-13 18:58:54 [INFO]\t[TRAIN] epoch=20, iter=17070/20000, loss=0.2090, lr=0.000444, batch_cost=1.1615, reader_cost=0.00009, ips=3.4440 samples/sec | ETA 00:56:43\n",
      "2021-04-13 18:59:05 [INFO]\t[TRAIN] epoch=20, iter=17080/20000, loss=0.1919, lr=0.000443, batch_cost=1.0916, reader_cost=0.00010, ips=3.6644 samples/sec | ETA 00:53:07\n",
      "2021-04-13 18:59:15 [INFO]\t[TRAIN] epoch=20, iter=17090/20000, loss=0.2128, lr=0.000441, batch_cost=1.0766, reader_cost=0.00010, ips=3.7155 samples/sec | ETA 00:52:12\n",
      "2021-04-13 18:59:26 [INFO]\t[TRAIN] epoch=20, iter=17100/20000, loss=0.2015, lr=0.000440, batch_cost=1.0903, reader_cost=0.00009, ips=3.6689 samples/sec | ETA 00:52:41\n",
      "2021-04-13 18:59:37 [INFO]\t[TRAIN] epoch=20, iter=17110/20000, loss=0.1764, lr=0.000438, batch_cost=1.0684, reader_cost=0.00009, ips=3.7440 samples/sec | ETA 00:51:27\n",
      "2021-04-13 18:59:48 [INFO]\t[TRAIN] epoch=20, iter=17120/20000, loss=0.2076, lr=0.000437, batch_cost=1.1210, reader_cost=0.00009, ips=3.5682 samples/sec | ETA 00:53:48\n",
      "2021-04-13 18:59:59 [INFO]\t[TRAIN] epoch=20, iter=17130/20000, loss=0.2130, lr=0.000436, batch_cost=1.0591, reader_cost=0.09789, ips=3.7768 samples/sec | ETA 00:50:39\n",
      "2021-04-13 19:00:11 [INFO]\t[TRAIN] epoch=20, iter=17140/20000, loss=0.1754, lr=0.000434, batch_cost=1.1739, reader_cost=0.00154, ips=3.4074 samples/sec | ETA 00:55:57\n",
      "2021-04-13 19:00:22 [INFO]\t[TRAIN] epoch=20, iter=17150/20000, loss=0.2178, lr=0.000433, batch_cost=1.1734, reader_cost=0.00009, ips=3.4090 samples/sec | ETA 00:55:44\n",
      "2021-04-13 19:00:33 [INFO]\t[TRAIN] epoch=20, iter=17160/20000, loss=0.1823, lr=0.000432, batch_cost=1.0611, reader_cost=0.00009, ips=3.7696 samples/sec | ETA 00:50:13\n",
      "2021-04-13 19:00:45 [INFO]\t[TRAIN] epoch=20, iter=17170/20000, loss=0.2175, lr=0.000430, batch_cost=1.1883, reader_cost=0.00011, ips=3.3661 samples/sec | ETA 00:56:02\n",
      "2021-04-13 19:00:56 [INFO]\t[TRAIN] epoch=20, iter=17180/20000, loss=0.1782, lr=0.000429, batch_cost=1.1183, reader_cost=0.00010, ips=3.5767 samples/sec | ETA 00:52:33\n",
      "2021-04-13 19:01:07 [INFO]\t[TRAIN] epoch=20, iter=17190/20000, loss=0.1982, lr=0.000428, batch_cost=1.0954, reader_cost=0.00008, ips=3.6517 samples/sec | ETA 00:51:18\n",
      "2021-04-13 19:01:18 [INFO]\t[TRAIN] epoch=20, iter=17200/20000, loss=0.1698, lr=0.000426, batch_cost=1.1367, reader_cost=0.00009, ips=3.5191 samples/sec | ETA 00:53:02\n",
      "2021-04-13 19:01:30 [INFO]\t[TRAIN] epoch=20, iter=17210/20000, loss=0.2354, lr=0.000425, batch_cost=1.1726, reader_cost=0.00101, ips=3.4113 samples/sec | ETA 00:54:31\n",
      "2021-04-13 19:01:41 [INFO]\t[TRAIN] epoch=20, iter=17220/20000, loss=0.1791, lr=0.000423, batch_cost=1.0570, reader_cost=0.00008, ips=3.7844 samples/sec | ETA 00:48:58\n",
      "2021-04-13 19:01:52 [INFO]\t[TRAIN] epoch=20, iter=17230/20000, loss=0.1911, lr=0.000422, batch_cost=1.1099, reader_cost=0.00010, ips=3.6039 samples/sec | ETA 00:51:14\n",
      "2021-04-13 19:02:03 [INFO]\t[TRAIN] epoch=20, iter=17240/20000, loss=0.1950, lr=0.000421, batch_cost=1.1486, reader_cost=0.00010, ips=3.4826 samples/sec | ETA 00:52:50\n",
      "2021-04-13 19:02:14 [INFO]\t[TRAIN] epoch=20, iter=17250/20000, loss=0.1616, lr=0.000419, batch_cost=1.0684, reader_cost=0.00009, ips=3.7439 samples/sec | ETA 00:48:58\n",
      "2021-04-13 19:02:14 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 107s 214ms/step - batch_cost: 0.2137 - reader cost: 0.001\n",
      "2021-04-13 19:04:01 [INFO]\t[EVAL] #Images=500 mIoU=0.2146 Acc=0.9882 Kappa=0.6200 \n",
      "2021-04-13 19:04:01 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9896 0.1828 0.5258 0.4919 0.1708 0.2891 0.4263 0.     0.     0.\n",
      " 0.     0.1432 0.     0.     0.    ]\n",
      "2021-04-13 19:04:01 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9944 0.2828 0.7235 0.6163 0.2657 0.3407 0.5834 0.     0.     0.\n",
      " 0.     0.26   0.     0.     0.    ]\n",
      "2021-04-13 19:04:03 [INFO]\t[EVAL] The model with the best validation mIoU (0.2148) was saved at iter 15750.\n",
      "2021-04-13 19:04:14 [INFO]\t[TRAIN] epoch=20, iter=17260/20000, loss=0.1707, lr=0.000418, batch_cost=1.1148, reader_cost=0.00010, ips=3.5882 samples/sec | ETA 00:50:54\n",
      "2021-04-13 19:04:25 [INFO]\t[TRAIN] epoch=20, iter=17270/20000, loss=0.1890, lr=0.000417, batch_cost=1.1152, reader_cost=0.00009, ips=3.5870 samples/sec | ETA 00:50:44\n",
      "2021-04-13 19:04:36 [INFO]\t[TRAIN] epoch=20, iter=17280/20000, loss=0.1776, lr=0.000415, batch_cost=1.1111, reader_cost=0.00010, ips=3.6001 samples/sec | ETA 00:50:22\n",
      "2021-04-13 19:04:48 [INFO]\t[TRAIN] epoch=20, iter=17290/20000, loss=0.1883, lr=0.000414, batch_cost=1.1238, reader_cost=0.00009, ips=3.5595 samples/sec | ETA 00:50:45\n",
      "2021-04-13 19:04:59 [INFO]\t[TRAIN] epoch=20, iter=17300/20000, loss=0.1738, lr=0.000412, batch_cost=1.1327, reader_cost=0.00010, ips=3.5313 samples/sec | ETA 00:50:58\n",
      "2021-04-13 19:05:10 [INFO]\t[TRAIN] epoch=20, iter=17310/20000, loss=0.1858, lr=0.000411, batch_cost=1.0848, reader_cost=0.00009, ips=3.6874 samples/sec | ETA 00:48:38\n",
      "2021-04-13 19:05:21 [INFO]\t[TRAIN] epoch=20, iter=17320/20000, loss=0.1818, lr=0.000410, batch_cost=1.1126, reader_cost=0.00009, ips=3.5951 samples/sec | ETA 00:49:41\n",
      "2021-04-13 19:05:32 [INFO]\t[TRAIN] epoch=20, iter=17330/20000, loss=0.1743, lr=0.000408, batch_cost=1.0908, reader_cost=0.00009, ips=3.6670 samples/sec | ETA 00:48:32\n",
      "2021-04-13 19:05:43 [INFO]\t[TRAIN] epoch=20, iter=17340/20000, loss=0.1551, lr=0.000407, batch_cost=1.1491, reader_cost=0.00009, ips=3.4811 samples/sec | ETA 00:50:56\n",
      "2021-04-13 19:05:54 [INFO]\t[TRAIN] epoch=20, iter=17350/20000, loss=0.1704, lr=0.000406, batch_cost=1.0993, reader_cost=0.00009, ips=3.6386 samples/sec | ETA 00:48:33\n",
      "2021-04-13 19:06:06 [INFO]\t[TRAIN] epoch=20, iter=17360/20000, loss=0.2204, lr=0.000404, batch_cost=1.2034, reader_cost=0.00009, ips=3.3239 samples/sec | ETA 00:52:56\n",
      "2021-04-13 19:06:18 [INFO]\t[TRAIN] epoch=20, iter=17370/20000, loss=0.1756, lr=0.000403, batch_cost=1.1230, reader_cost=0.00010, ips=3.5619 samples/sec | ETA 00:49:13\n",
      "2021-04-13 19:06:29 [INFO]\t[TRAIN] epoch=20, iter=17380/20000, loss=0.2207, lr=0.000401, batch_cost=1.0962, reader_cost=0.00010, ips=3.6490 samples/sec | ETA 00:47:51\n",
      "2021-04-13 19:06:40 [INFO]\t[TRAIN] epoch=20, iter=17390/20000, loss=0.2042, lr=0.000400, batch_cost=1.1402, reader_cost=0.00009, ips=3.5082 samples/sec | ETA 00:49:35\n",
      "2021-04-13 19:06:51 [INFO]\t[TRAIN] epoch=20, iter=17400/20000, loss=0.1900, lr=0.000399, batch_cost=1.1420, reader_cost=0.00009, ips=3.5025 samples/sec | ETA 00:49:29\n",
      "2021-04-13 19:07:03 [INFO]\t[TRAIN] epoch=20, iter=17410/20000, loss=0.1804, lr=0.000397, batch_cost=1.1296, reader_cost=0.00009, ips=3.5411 samples/sec | ETA 00:48:45\n",
      "2021-04-13 19:07:13 [INFO]\t[TRAIN] epoch=20, iter=17420/20000, loss=0.2204, lr=0.000396, batch_cost=1.0626, reader_cost=0.00008, ips=3.7644 samples/sec | ETA 00:45:41\n",
      "2021-04-13 19:07:25 [INFO]\t[TRAIN] epoch=20, iter=17430/20000, loss=0.1582, lr=0.000395, batch_cost=1.1421, reader_cost=0.00009, ips=3.5023 samples/sec | ETA 00:48:55\n",
      "2021-04-13 19:07:36 [INFO]\t[TRAIN] epoch=20, iter=17440/20000, loss=0.1773, lr=0.000393, batch_cost=1.1369, reader_cost=0.00009, ips=3.5183 samples/sec | ETA 00:48:30\n",
      "2021-04-13 19:07:47 [INFO]\t[TRAIN] epoch=20, iter=17450/20000, loss=0.1627, lr=0.000392, batch_cost=1.1194, reader_cost=0.00010, ips=3.5734 samples/sec | ETA 00:47:34\n",
      "2021-04-13 19:07:58 [INFO]\t[TRAIN] epoch=20, iter=17460/20000, loss=0.1880, lr=0.000390, batch_cost=1.0988, reader_cost=0.00010, ips=3.6403 samples/sec | ETA 00:46:30\n",
      "2021-04-13 19:08:09 [INFO]\t[TRAIN] epoch=20, iter=17470/20000, loss=0.2183, lr=0.000389, batch_cost=1.1224, reader_cost=0.00009, ips=3.5638 samples/sec | ETA 00:47:19\n",
      "2021-04-13 19:08:20 [INFO]\t[TRAIN] epoch=20, iter=17480/20000, loss=0.1953, lr=0.000388, batch_cost=1.0927, reader_cost=0.00010, ips=3.6608 samples/sec | ETA 00:45:53\n",
      "2021-04-13 19:08:32 [INFO]\t[TRAIN] epoch=20, iter=17490/20000, loss=0.2347, lr=0.000386, batch_cost=1.1665, reader_cost=0.00010, ips=3.4290 samples/sec | ETA 00:48:47\n",
      "2021-04-13 19:08:43 [INFO]\t[TRAIN] epoch=20, iter=17500/20000, loss=0.1831, lr=0.000385, batch_cost=1.0921, reader_cost=0.00011, ips=3.6628 samples/sec | ETA 00:45:30\n",
      "2021-04-13 19:08:54 [INFO]\t[TRAIN] epoch=21, iter=17510/20000, loss=0.2038, lr=0.000383, batch_cost=1.1130, reader_cost=0.00009, ips=3.5938 samples/sec | ETA 00:46:11\n",
      "2021-04-13 19:09:06 [INFO]\t[TRAIN] epoch=21, iter=17520/20000, loss=0.1663, lr=0.000382, batch_cost=1.1468, reader_cost=0.00009, ips=3.4881 samples/sec | ETA 00:47:23\n",
      "2021-04-13 19:09:17 [INFO]\t[TRAIN] epoch=21, iter=17530/20000, loss=0.1929, lr=0.000381, batch_cost=1.1199, reader_cost=0.00009, ips=3.5717 samples/sec | ETA 00:46:06\n",
      "2021-04-13 19:09:28 [INFO]\t[TRAIN] epoch=21, iter=17540/20000, loss=0.2099, lr=0.000379, batch_cost=1.1550, reader_cost=0.00009, ips=3.4631 samples/sec | ETA 00:47:21\n",
      "2021-04-13 19:09:40 [INFO]\t[TRAIN] epoch=21, iter=17550/20000, loss=0.2202, lr=0.000378, batch_cost=1.1157, reader_cost=0.00010, ips=3.5852 samples/sec | ETA 00:45:33\n",
      "2021-04-13 19:09:51 [INFO]\t[TRAIN] epoch=21, iter=17560/20000, loss=0.1821, lr=0.000377, batch_cost=1.1327, reader_cost=0.00009, ips=3.5313 samples/sec | ETA 00:46:03\n",
      "2021-04-13 19:10:02 [INFO]\t[TRAIN] epoch=21, iter=17570/20000, loss=0.2081, lr=0.000375, batch_cost=1.1655, reader_cost=0.00010, ips=3.4319 samples/sec | ETA 00:47:12\n",
      "2021-04-13 19:10:14 [INFO]\t[TRAIN] epoch=21, iter=17580/20000, loss=0.1889, lr=0.000374, batch_cost=1.1129, reader_cost=0.00009, ips=3.5943 samples/sec | ETA 00:44:53\n",
      "2021-04-13 19:10:25 [INFO]\t[TRAIN] epoch=21, iter=17590/20000, loss=0.1716, lr=0.000372, batch_cost=1.1288, reader_cost=0.00009, ips=3.5434 samples/sec | ETA 00:45:20\n",
      "2021-04-13 19:10:37 [INFO]\t[TRAIN] epoch=21, iter=17600/20000, loss=0.1596, lr=0.000371, batch_cost=1.2128, reader_cost=0.00010, ips=3.2982 samples/sec | ETA 00:48:30\n",
      "2021-04-13 19:10:47 [INFO]\t[TRAIN] epoch=21, iter=17610/20000, loss=0.1907, lr=0.000370, batch_cost=1.0453, reader_cost=0.00010, ips=3.8265 samples/sec | ETA 00:41:38\n",
      "2021-04-13 19:10:58 [INFO]\t[TRAIN] epoch=21, iter=17620/20000, loss=0.1997, lr=0.000368, batch_cost=1.0590, reader_cost=0.00009, ips=3.7771 samples/sec | ETA 00:42:00\n",
      "2021-04-13 19:11:09 [INFO]\t[TRAIN] epoch=21, iter=17630/20000, loss=0.1803, lr=0.000367, batch_cost=1.1228, reader_cost=0.00009, ips=3.5626 samples/sec | ETA 00:44:20\n",
      "2021-04-13 19:11:20 [INFO]\t[TRAIN] epoch=21, iter=17640/20000, loss=0.2028, lr=0.000365, batch_cost=1.1142, reader_cost=0.00010, ips=3.5901 samples/sec | ETA 00:43:49\n",
      "2021-04-13 19:11:32 [INFO]\t[TRAIN] epoch=21, iter=17650/20000, loss=0.1591, lr=0.000364, batch_cost=1.1724, reader_cost=0.00010, ips=3.4118 samples/sec | ETA 00:45:55\n",
      "2021-04-13 19:11:43 [INFO]\t[TRAIN] epoch=21, iter=17660/20000, loss=0.1767, lr=0.000363, batch_cost=1.0614, reader_cost=0.00008, ips=3.7685 samples/sec | ETA 00:41:23\n",
      "2021-04-13 19:11:54 [INFO]\t[TRAIN] epoch=21, iter=17670/20000, loss=0.2169, lr=0.000361, batch_cost=1.1455, reader_cost=0.00009, ips=3.4920 samples/sec | ETA 00:44:28\n",
      "2021-04-13 19:12:06 [INFO]\t[TRAIN] epoch=21, iter=17680/20000, loss=0.2042, lr=0.000360, batch_cost=1.1541, reader_cost=0.00009, ips=3.4659 samples/sec | ETA 00:44:37\n",
      "2021-04-13 19:12:17 [INFO]\t[TRAIN] epoch=21, iter=17690/20000, loss=0.2669, lr=0.000358, batch_cost=1.1289, reader_cost=0.00011, ips=3.5434 samples/sec | ETA 00:43:27\n",
      "2021-04-13 19:12:28 [INFO]\t[TRAIN] epoch=21, iter=17700/20000, loss=0.2012, lr=0.000357, batch_cost=1.1078, reader_cost=0.00010, ips=3.6106 samples/sec | ETA 00:42:28\n",
      "2021-04-13 19:12:40 [INFO]\t[TRAIN] epoch=21, iter=17710/20000, loss=0.2134, lr=0.000356, batch_cost=1.1686, reader_cost=0.00010, ips=3.4228 samples/sec | ETA 00:44:36\n",
      "2021-04-13 19:12:51 [INFO]\t[TRAIN] epoch=21, iter=17720/20000, loss=0.2069, lr=0.000354, batch_cost=1.0825, reader_cost=0.00011, ips=3.6953 samples/sec | ETA 00:41:07\n",
      "2021-04-13 19:13:02 [INFO]\t[TRAIN] epoch=21, iter=17730/20000, loss=0.2272, lr=0.000353, batch_cost=1.1774, reader_cost=0.00010, ips=3.3972 samples/sec | ETA 00:44:32\n",
      "2021-04-13 19:13:14 [INFO]\t[TRAIN] epoch=21, iter=17740/20000, loss=0.1859, lr=0.000351, batch_cost=1.1125, reader_cost=0.00009, ips=3.5955 samples/sec | ETA 00:41:54\n",
      "2021-04-13 19:13:24 [INFO]\t[TRAIN] epoch=21, iter=17750/20000, loss=0.2016, lr=0.000350, batch_cost=1.0305, reader_cost=0.00009, ips=3.8815 samples/sec | ETA 00:38:38\n",
      "2021-04-13 19:13:36 [INFO]\t[TRAIN] epoch=21, iter=17760/20000, loss=0.1765, lr=0.000349, batch_cost=1.2000, reader_cost=0.00009, ips=3.3333 samples/sec | ETA 00:44:48\n",
      "2021-04-13 19:13:48 [INFO]\t[TRAIN] epoch=21, iter=17770/20000, loss=0.2305, lr=0.000347, batch_cost=1.2229, reader_cost=0.00010, ips=3.2709 samples/sec | ETA 00:45:27\n",
      "2021-04-13 19:13:59 [INFO]\t[TRAIN] epoch=21, iter=17780/20000, loss=0.1632, lr=0.000346, batch_cost=1.0792, reader_cost=0.00009, ips=3.7064 samples/sec | ETA 00:39:55\n",
      "2021-04-13 19:14:11 [INFO]\t[TRAIN] epoch=21, iter=17790/20000, loss=0.1875, lr=0.000344, batch_cost=1.2181, reader_cost=0.00009, ips=3.2837 samples/sec | ETA 00:44:52\n",
      "2021-04-13 19:14:23 [INFO]\t[TRAIN] epoch=21, iter=17800/20000, loss=0.1867, lr=0.000343, batch_cost=1.1500, reader_cost=0.00009, ips=3.4782 samples/sec | ETA 00:42:10\n",
      "2021-04-13 19:14:33 [INFO]\t[TRAIN] epoch=21, iter=17810/20000, loss=0.2115, lr=0.000342, batch_cost=1.0774, reader_cost=0.00008, ips=3.7127 samples/sec | ETA 00:39:19\n",
      "2021-04-13 19:14:44 [INFO]\t[TRAIN] epoch=21, iter=17820/20000, loss=0.1950, lr=0.000340, batch_cost=1.0836, reader_cost=0.00009, ips=3.6915 samples/sec | ETA 00:39:22\n",
      "2021-04-13 19:14:55 [INFO]\t[TRAIN] epoch=21, iter=17830/20000, loss=0.1907, lr=0.000339, batch_cost=1.0582, reader_cost=0.00009, ips=3.7801 samples/sec | ETA 00:38:16\n",
      "2021-04-13 19:15:06 [INFO]\t[TRAIN] epoch=21, iter=17840/20000, loss=0.1878, lr=0.000337, batch_cost=1.1062, reader_cost=0.00009, ips=3.6158 samples/sec | ETA 00:39:49\n",
      "2021-04-13 19:15:18 [INFO]\t[TRAIN] epoch=21, iter=17850/20000, loss=0.2286, lr=0.000336, batch_cost=1.1931, reader_cost=0.00009, ips=3.3526 samples/sec | ETA 00:42:45\n",
      "2021-04-13 19:15:29 [INFO]\t[TRAIN] epoch=21, iter=17860/20000, loss=0.1869, lr=0.000335, batch_cost=1.0965, reader_cost=0.00009, ips=3.6480 samples/sec | ETA 00:39:06\n",
      "2021-04-13 19:15:41 [INFO]\t[TRAIN] epoch=21, iter=17870/20000, loss=0.2079, lr=0.000333, batch_cost=1.2059, reader_cost=0.00008, ips=3.3170 samples/sec | ETA 00:42:48\n",
      "2021-04-13 19:15:51 [INFO]\t[TRAIN] epoch=21, iter=17880/20000, loss=0.1875, lr=0.000332, batch_cost=1.0672, reader_cost=0.00008, ips=3.7480 samples/sec | ETA 00:37:42\n",
      "2021-04-13 19:16:03 [INFO]\t[TRAIN] epoch=21, iter=17890/20000, loss=0.1694, lr=0.000330, batch_cost=1.1581, reader_cost=0.00009, ips=3.4539 samples/sec | ETA 00:40:43\n",
      "2021-04-13 19:16:14 [INFO]\t[TRAIN] epoch=21, iter=17900/20000, loss=0.1769, lr=0.000329, batch_cost=1.0920, reader_cost=0.00010, ips=3.6629 samples/sec | ETA 00:38:13\n",
      "2021-04-13 19:16:26 [INFO]\t[TRAIN] epoch=21, iter=17910/20000, loss=0.2045, lr=0.000328, batch_cost=1.1620, reader_cost=0.00009, ips=3.4423 samples/sec | ETA 00:40:28\n",
      "2021-04-13 19:16:37 [INFO]\t[TRAIN] epoch=21, iter=17920/20000, loss=0.1557, lr=0.000326, batch_cost=1.1173, reader_cost=0.00011, ips=3.5800 samples/sec | ETA 00:38:44\n",
      "2021-04-13 19:16:48 [INFO]\t[TRAIN] epoch=21, iter=17930/20000, loss=0.1826, lr=0.000325, batch_cost=1.1244, reader_cost=0.00010, ips=3.5573 samples/sec | ETA 00:38:47\n",
      "2021-04-13 19:16:59 [INFO]\t[TRAIN] epoch=21, iter=17940/20000, loss=0.1746, lr=0.000323, batch_cost=1.1218, reader_cost=0.00010, ips=3.5658 samples/sec | ETA 00:38:30\n",
      "2021-04-13 19:17:10 [INFO]\t[TRAIN] epoch=21, iter=17950/20000, loss=0.1804, lr=0.000322, batch_cost=1.1075, reader_cost=0.00009, ips=3.6117 samples/sec | ETA 00:37:50\n",
      "2021-04-13 19:17:22 [INFO]\t[TRAIN] epoch=21, iter=17960/20000, loss=0.1814, lr=0.000321, batch_cost=1.1325, reader_cost=0.00009, ips=3.5319 samples/sec | ETA 00:38:30\n",
      "2021-04-13 19:17:34 [INFO]\t[TRAIN] epoch=21, iter=17970/20000, loss=0.1793, lr=0.000319, batch_cost=1.1919, reader_cost=0.00012, ips=3.3560 samples/sec | ETA 00:40:19\n",
      "2021-04-13 19:17:45 [INFO]\t[TRAIN] epoch=21, iter=17980/20000, loss=0.1770, lr=0.000318, batch_cost=1.1623, reader_cost=0.00010, ips=3.4416 samples/sec | ETA 00:39:07\n",
      "2021-04-13 19:17:56 [INFO]\t[TRAIN] epoch=21, iter=17990/20000, loss=0.1589, lr=0.000316, batch_cost=1.1039, reader_cost=0.00009, ips=3.6235 samples/sec | ETA 00:36:58\n",
      "2021-04-13 19:18:06 [INFO]\t[TRAIN] epoch=21, iter=18000/20000, loss=0.1714, lr=0.000315, batch_cost=1.0132, reader_cost=0.00009, ips=3.9477 samples/sec | ETA 00:33:46\n",
      "2021-04-13 19:18:07 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 106s 213ms/step - batch_cost: 0.2126 - reader cost: 0.00\n",
      "2021-04-13 19:19:53 [INFO]\t[EVAL] #Images=500 mIoU=0.2192 Acc=0.9887 Kappa=0.6272 \n",
      "2021-04-13 19:19:53 [INFO]\t[EVAL] Class IoU: \n",
      "[0.99   0.2012 0.5306 0.5001 0.1706 0.319  0.4292 0.     0.     0.\n",
      " 0.     0.1474 0.     0.     0.    ]\n",
      "2021-04-13 19:19:53 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9941 0.3125 0.7169 0.6286 0.2943 0.4223 0.6074 0.     0.     0.\n",
      " 0.     0.3321 0.     0.     0.    ]\n",
      "2021-04-13 19:19:56 [INFO]\t[EVAL] The model with the best validation mIoU (0.2192) was saved at iter 18000.\n",
      "2021-04-13 19:20:09 [INFO]\t[TRAIN] epoch=21, iter=18010/20000, loss=0.2012, lr=0.000313, batch_cost=1.3212, reader_cost=0.09298, ips=3.0276 samples/sec | ETA 00:43:49\n",
      "2021-04-13 19:20:20 [INFO]\t[TRAIN] epoch=21, iter=18020/20000, loss=0.1772, lr=0.000312, batch_cost=1.1101, reader_cost=0.00010, ips=3.6033 samples/sec | ETA 00:36:37\n",
      "2021-04-13 19:20:31 [INFO]\t[TRAIN] epoch=21, iter=18030/20000, loss=0.2108, lr=0.000311, batch_cost=1.1107, reader_cost=0.00008, ips=3.6014 samples/sec | ETA 00:36:28\n",
      "2021-04-13 19:20:43 [INFO]\t[TRAIN] epoch=21, iter=18040/20000, loss=0.1834, lr=0.000309, batch_cost=1.1091, reader_cost=0.00009, ips=3.6066 samples/sec | ETA 00:36:13\n",
      "2021-04-13 19:20:54 [INFO]\t[TRAIN] epoch=21, iter=18050/20000, loss=0.1945, lr=0.000308, batch_cost=1.1302, reader_cost=0.00009, ips=3.5393 samples/sec | ETA 00:36:43\n",
      "2021-04-13 19:21:06 [INFO]\t[TRAIN] epoch=21, iter=18060/20000, loss=0.1840, lr=0.000306, batch_cost=1.2638, reader_cost=0.00106, ips=3.1651 samples/sec | ETA 00:40:51\n",
      "2021-04-13 19:21:18 [INFO]\t[TRAIN] epoch=21, iter=18070/20000, loss=0.1744, lr=0.000305, batch_cost=1.1068, reader_cost=0.00009, ips=3.6140 samples/sec | ETA 00:35:36\n",
      "2021-04-13 19:21:29 [INFO]\t[TRAIN] epoch=21, iter=18080/20000, loss=0.1943, lr=0.000304, batch_cost=1.1522, reader_cost=0.00010, ips=3.4716 samples/sec | ETA 00:36:52\n",
      "2021-04-13 19:21:40 [INFO]\t[TRAIN] epoch=21, iter=18090/20000, loss=0.1990, lr=0.000302, batch_cost=1.1045, reader_cost=0.00010, ips=3.6216 samples/sec | ETA 00:35:09\n",
      "2021-04-13 19:21:52 [INFO]\t[TRAIN] epoch=21, iter=18100/20000, loss=0.2669, lr=0.000301, batch_cost=1.1487, reader_cost=0.00010, ips=3.4822 samples/sec | ETA 00:36:22\n",
      "2021-04-13 19:22:03 [INFO]\t[TRAIN] epoch=21, iter=18110/20000, loss=0.1752, lr=0.000299, batch_cost=1.1499, reader_cost=0.00009, ips=3.4784 samples/sec | ETA 00:36:13\n",
      "2021-04-13 19:22:15 [INFO]\t[TRAIN] epoch=21, iter=18120/20000, loss=0.1712, lr=0.000298, batch_cost=1.2064, reader_cost=0.00010, ips=3.3156 samples/sec | ETA 00:37:48\n",
      "2021-04-13 19:22:26 [INFO]\t[TRAIN] epoch=21, iter=18130/20000, loss=0.1623, lr=0.000296, batch_cost=1.0857, reader_cost=0.00008, ips=3.6843 samples/sec | ETA 00:33:50\n",
      "2021-04-13 19:22:37 [INFO]\t[TRAIN] epoch=21, iter=18140/20000, loss=0.2484, lr=0.000295, batch_cost=1.0817, reader_cost=0.00010, ips=3.6979 samples/sec | ETA 00:33:31\n",
      "2021-04-13 19:22:48 [INFO]\t[TRAIN] epoch=21, iter=18150/20000, loss=0.1982, lr=0.000294, batch_cost=1.1084, reader_cost=0.00087, ips=3.6087 samples/sec | ETA 00:34:10\n",
      "2021-04-13 19:23:00 [INFO]\t[TRAIN] epoch=21, iter=18160/20000, loss=0.1969, lr=0.000292, batch_cost=1.1778, reader_cost=0.00009, ips=3.3962 samples/sec | ETA 00:36:07\n",
      "2021-04-13 19:23:11 [INFO]\t[TRAIN] epoch=21, iter=18170/20000, loss=0.2323, lr=0.000291, batch_cost=1.1155, reader_cost=0.00021, ips=3.5858 samples/sec | ETA 00:34:01\n",
      "2021-04-13 19:23:22 [INFO]\t[TRAIN] epoch=21, iter=18180/20000, loss=0.1790, lr=0.000289, batch_cost=1.0902, reader_cost=0.00010, ips=3.6690 samples/sec | ETA 00:33:04\n",
      "2021-04-13 19:23:34 [INFO]\t[TRAIN] epoch=21, iter=18190/20000, loss=0.1827, lr=0.000288, batch_cost=1.2161, reader_cost=0.00010, ips=3.2893 samples/sec | ETA 00:36:41\n",
      "2021-04-13 19:23:45 [INFO]\t[TRAIN] epoch=21, iter=18200/20000, loss=0.1632, lr=0.000286, batch_cost=1.0928, reader_cost=0.00011, ips=3.6602 samples/sec | ETA 00:32:47\n",
      "2021-04-13 19:23:56 [INFO]\t[TRAIN] epoch=21, iter=18210/20000, loss=0.1781, lr=0.000285, batch_cost=1.1025, reader_cost=0.00010, ips=3.6280 samples/sec | ETA 00:32:53\n",
      "2021-04-13 19:24:07 [INFO]\t[TRAIN] epoch=21, iter=18220/20000, loss=0.1723, lr=0.000284, batch_cost=1.1289, reader_cost=0.00008, ips=3.5432 samples/sec | ETA 00:33:29\n",
      "2021-04-13 19:24:19 [INFO]\t[TRAIN] epoch=21, iter=18230/20000, loss=0.1896, lr=0.000282, batch_cost=1.2195, reader_cost=0.00009, ips=3.2799 samples/sec | ETA 00:35:58\n",
      "2021-04-13 19:24:31 [INFO]\t[TRAIN] epoch=21, iter=18240/20000, loss=0.1958, lr=0.000281, batch_cost=1.1164, reader_cost=0.00009, ips=3.5830 samples/sec | ETA 00:32:44\n",
      "2021-04-13 19:24:42 [INFO]\t[TRAIN] epoch=21, iter=18250/20000, loss=0.2743, lr=0.000279, batch_cost=1.1614, reader_cost=0.00009, ips=3.4442 samples/sec | ETA 00:33:52\n",
      "2021-04-13 19:24:54 [INFO]\t[TRAIN] epoch=21, iter=18260/20000, loss=0.1794, lr=0.000278, batch_cost=1.1834, reader_cost=0.00009, ips=3.3802 samples/sec | ETA 00:34:19\n",
      "2021-04-13 19:25:05 [INFO]\t[TRAIN] epoch=21, iter=18270/20000, loss=0.1727, lr=0.000276, batch_cost=1.1489, reader_cost=0.00009, ips=3.4815 samples/sec | ETA 00:33:07\n",
      "2021-04-13 19:25:16 [INFO]\t[TRAIN] epoch=21, iter=18280/20000, loss=0.1646, lr=0.000275, batch_cost=1.0795, reader_cost=0.00009, ips=3.7054 samples/sec | ETA 00:30:56\n",
      "2021-04-13 19:25:27 [INFO]\t[TRAIN] epoch=21, iter=18290/20000, loss=0.1413, lr=0.000273, batch_cost=1.0722, reader_cost=0.00009, ips=3.7308 samples/sec | ETA 00:30:33\n",
      "2021-04-13 19:25:38 [INFO]\t[TRAIN] epoch=21, iter=18300/20000, loss=0.2118, lr=0.000272, batch_cost=1.1492, reader_cost=0.00009, ips=3.4807 samples/sec | ETA 00:32:33\n",
      "2021-04-13 19:25:50 [INFO]\t[TRAIN] epoch=21, iter=18310/20000, loss=0.1702, lr=0.000271, batch_cost=1.1368, reader_cost=0.00009, ips=3.5185 samples/sec | ETA 00:32:01\n",
      "2021-04-13 19:26:00 [INFO]\t[TRAIN] epoch=21, iter=18320/20000, loss=0.1728, lr=0.000269, batch_cost=1.0612, reader_cost=0.00010, ips=3.7693 samples/sec | ETA 00:29:42\n",
      "2021-04-13 19:26:12 [INFO]\t[TRAIN] epoch=21, iter=18330/20000, loss=0.1856, lr=0.000268, batch_cost=1.1348, reader_cost=0.00010, ips=3.5248 samples/sec | ETA 00:31:35\n",
      "2021-04-13 19:26:23 [INFO]\t[TRAIN] epoch=21, iter=18340/20000, loss=0.2023, lr=0.000266, batch_cost=1.1382, reader_cost=0.00011, ips=3.5142 samples/sec | ETA 00:31:29\n",
      "2021-04-13 19:26:35 [INFO]\t[TRAIN] epoch=21, iter=18350/20000, loss=0.1708, lr=0.000265, batch_cost=1.1984, reader_cost=0.00010, ips=3.3378 samples/sec | ETA 00:32:57\n",
      "2021-04-13 19:26:46 [INFO]\t[TRAIN] epoch=21, iter=18360/20000, loss=0.1941, lr=0.000263, batch_cost=1.1073, reader_cost=0.00010, ips=3.6124 samples/sec | ETA 00:30:15\n",
      "2021-04-13 19:26:58 [INFO]\t[TRAIN] epoch=21, iter=18370/20000, loss=0.1737, lr=0.000262, batch_cost=1.1525, reader_cost=0.00008, ips=3.4708 samples/sec | ETA 00:31:18\n",
      "2021-04-13 19:27:09 [INFO]\t[TRAIN] epoch=22, iter=18380/20000, loss=0.1808, lr=0.000261, batch_cost=1.0874, reader_cost=0.00009, ips=3.6784 samples/sec | ETA 00:29:21\n",
      "2021-04-13 19:27:19 [INFO]\t[TRAIN] epoch=22, iter=18390/20000, loss=0.1845, lr=0.000259, batch_cost=1.0768, reader_cost=0.00009, ips=3.7148 samples/sec | ETA 00:28:53\n",
      "2021-04-13 19:27:31 [INFO]\t[TRAIN] epoch=22, iter=18400/20000, loss=0.2286, lr=0.000258, batch_cost=1.1228, reader_cost=0.00009, ips=3.5625 samples/sec | ETA 00:29:56\n",
      "2021-04-13 19:27:42 [INFO]\t[TRAIN] epoch=22, iter=18410/20000, loss=0.1835, lr=0.000256, batch_cost=1.1019, reader_cost=0.00009, ips=3.6301 samples/sec | ETA 00:29:11\n",
      "2021-04-13 19:27:53 [INFO]\t[TRAIN] epoch=22, iter=18420/20000, loss=0.2108, lr=0.000255, batch_cost=1.0903, reader_cost=0.00009, ips=3.6687 samples/sec | ETA 00:28:42\n",
      "2021-04-13 19:28:03 [INFO]\t[TRAIN] epoch=22, iter=18430/20000, loss=0.1754, lr=0.000253, batch_cost=1.0734, reader_cost=0.00009, ips=3.7264 samples/sec | ETA 00:28:05\n",
      "2021-04-13 19:28:14 [INFO]\t[TRAIN] epoch=22, iter=18440/20000, loss=0.1956, lr=0.000252, batch_cost=1.0877, reader_cost=0.00009, ips=3.6776 samples/sec | ETA 00:28:16\n",
      "2021-04-13 19:28:26 [INFO]\t[TRAIN] epoch=22, iter=18450/20000, loss=0.1690, lr=0.000250, batch_cost=1.1554, reader_cost=0.00009, ips=3.4621 samples/sec | ETA 00:29:50\n",
      "2021-04-13 19:28:37 [INFO]\t[TRAIN] epoch=22, iter=18460/20000, loss=0.1863, lr=0.000249, batch_cost=1.1162, reader_cost=0.00009, ips=3.5836 samples/sec | ETA 00:28:38\n",
      "2021-04-13 19:28:49 [INFO]\t[TRAIN] epoch=22, iter=18470/20000, loss=0.1759, lr=0.000247, batch_cost=1.1846, reader_cost=0.00010, ips=3.3767 samples/sec | ETA 00:30:12\n",
      "2021-04-13 19:29:00 [INFO]\t[TRAIN] epoch=22, iter=18480/20000, loss=0.2229, lr=0.000246, batch_cost=1.1503, reader_cost=0.00010, ips=3.4773 samples/sec | ETA 00:29:08\n",
      "2021-04-13 19:29:11 [INFO]\t[TRAIN] epoch=22, iter=18490/20000, loss=0.1718, lr=0.000245, batch_cost=1.0956, reader_cost=0.00009, ips=3.6510 samples/sec | ETA 00:27:34\n",
      "2021-04-13 19:29:23 [INFO]\t[TRAIN] epoch=22, iter=18500/20000, loss=0.2272, lr=0.000243, batch_cost=1.2108, reader_cost=0.00009, ips=3.3035 samples/sec | ETA 00:30:16\n",
      "2021-04-13 19:29:34 [INFO]\t[TRAIN] epoch=22, iter=18510/20000, loss=0.1759, lr=0.000242, batch_cost=1.0713, reader_cost=0.00012, ips=3.7339 samples/sec | ETA 00:26:36\n",
      "2021-04-13 19:29:46 [INFO]\t[TRAIN] epoch=22, iter=18520/20000, loss=0.1672, lr=0.000240, batch_cost=1.1789, reader_cost=0.00010, ips=3.3930 samples/sec | ETA 00:29:04\n",
      "2021-04-13 19:29:58 [INFO]\t[TRAIN] epoch=22, iter=18530/20000, loss=0.1934, lr=0.000239, batch_cost=1.1832, reader_cost=0.00009, ips=3.3806 samples/sec | ETA 00:28:59\n",
      "2021-04-13 19:30:09 [INFO]\t[TRAIN] epoch=22, iter=18540/20000, loss=0.2269, lr=0.000237, batch_cost=1.1576, reader_cost=0.00010, ips=3.4554 samples/sec | ETA 00:28:10\n",
      "2021-04-13 19:30:20 [INFO]\t[TRAIN] epoch=22, iter=18550/20000, loss=0.2009, lr=0.000236, batch_cost=1.1163, reader_cost=0.00009, ips=3.5833 samples/sec | ETA 00:26:58\n",
      "2021-04-13 19:30:32 [INFO]\t[TRAIN] epoch=22, iter=18560/20000, loss=0.1745, lr=0.000234, batch_cost=1.1281, reader_cost=0.00009, ips=3.5457 samples/sec | ETA 00:27:04\n",
      "2021-04-13 19:30:43 [INFO]\t[TRAIN] epoch=22, iter=18570/20000, loss=0.1875, lr=0.000233, batch_cost=1.1736, reader_cost=0.00009, ips=3.4085 samples/sec | ETA 00:27:58\n",
      "2021-04-13 19:30:56 [INFO]\t[TRAIN] epoch=22, iter=18580/20000, loss=0.1687, lr=0.000231, batch_cost=1.2322, reader_cost=0.00010, ips=3.2463 samples/sec | ETA 00:29:09\n",
      "2021-04-13 19:31:07 [INFO]\t[TRAIN] epoch=22, iter=18590/20000, loss=0.1400, lr=0.000230, batch_cost=1.1028, reader_cost=0.00010, ips=3.6270 samples/sec | ETA 00:25:55\n",
      "2021-04-13 19:31:18 [INFO]\t[TRAIN] epoch=22, iter=18600/20000, loss=0.1715, lr=0.000228, batch_cost=1.0783, reader_cost=0.00009, ips=3.7094 samples/sec | ETA 00:25:09\n",
      "2021-04-13 19:31:29 [INFO]\t[TRAIN] epoch=22, iter=18610/20000, loss=0.2146, lr=0.000227, batch_cost=1.1270, reader_cost=0.00010, ips=3.5492 samples/sec | ETA 00:26:06\n",
      "2021-04-13 19:31:40 [INFO]\t[TRAIN] epoch=22, iter=18620/20000, loss=0.1884, lr=0.000226, batch_cost=1.1189, reader_cost=0.00009, ips=3.5750 samples/sec | ETA 00:25:44\n",
      "2021-04-13 19:31:51 [INFO]\t[TRAIN] epoch=22, iter=18630/20000, loss=0.2165, lr=0.000224, batch_cost=1.1235, reader_cost=0.00009, ips=3.5604 samples/sec | ETA 00:25:39\n",
      "2021-04-13 19:32:03 [INFO]\t[TRAIN] epoch=22, iter=18640/20000, loss=0.1652, lr=0.000223, batch_cost=1.1600, reader_cost=0.00010, ips=3.4482 samples/sec | ETA 00:26:17\n",
      "2021-04-13 19:32:14 [INFO]\t[TRAIN] epoch=22, iter=18650/20000, loss=0.1981, lr=0.000221, batch_cost=1.1151, reader_cost=0.00009, ips=3.5871 samples/sec | ETA 00:25:05\n",
      "2021-04-13 19:32:25 [INFO]\t[TRAIN] epoch=22, iter=18660/20000, loss=0.2139, lr=0.000220, batch_cost=1.0857, reader_cost=0.00009, ips=3.6841 samples/sec | ETA 00:24:14\n",
      "2021-04-13 19:32:36 [INFO]\t[TRAIN] epoch=22, iter=18670/20000, loss=0.1936, lr=0.000218, batch_cost=1.1283, reader_cost=0.00009, ips=3.5452 samples/sec | ETA 00:25:00\n",
      "2021-04-13 19:32:47 [INFO]\t[TRAIN] epoch=22, iter=18680/20000, loss=0.1805, lr=0.000217, batch_cost=1.0408, reader_cost=0.00009, ips=3.8433 samples/sec | ETA 00:22:53\n",
      "2021-04-13 19:32:57 [INFO]\t[TRAIN] epoch=22, iter=18690/20000, loss=0.1577, lr=0.000215, batch_cost=1.0727, reader_cost=0.00010, ips=3.7288 samples/sec | ETA 00:23:25\n",
      "2021-04-13 19:33:08 [INFO]\t[TRAIN] epoch=22, iter=18700/20000, loss=0.1599, lr=0.000214, batch_cost=1.1041, reader_cost=0.00009, ips=3.6229 samples/sec | ETA 00:23:55\n",
      "2021-04-13 19:33:19 [INFO]\t[TRAIN] epoch=22, iter=18710/20000, loss=0.2054, lr=0.000212, batch_cost=1.0612, reader_cost=0.00009, ips=3.7692 samples/sec | ETA 00:22:48\n",
      "2021-04-13 19:33:30 [INFO]\t[TRAIN] epoch=22, iter=18720/20000, loss=0.1887, lr=0.000211, batch_cost=1.1031, reader_cost=0.00009, ips=3.6262 samples/sec | ETA 00:23:31\n",
      "2021-04-13 19:33:41 [INFO]\t[TRAIN] epoch=22, iter=18730/20000, loss=0.1846, lr=0.000209, batch_cost=1.1069, reader_cost=0.00009, ips=3.6138 samples/sec | ETA 00:23:25\n",
      "2021-04-13 19:33:53 [INFO]\t[TRAIN] epoch=22, iter=18740/20000, loss=0.1802, lr=0.000208, batch_cost=1.1552, reader_cost=0.00009, ips=3.4625 samples/sec | ETA 00:24:15\n",
      "2021-04-13 19:34:04 [INFO]\t[TRAIN] epoch=22, iter=18750/20000, loss=0.2148, lr=0.000206, batch_cost=1.1146, reader_cost=0.00009, ips=3.5887 samples/sec | ETA 00:23:13\n",
      "2021-04-13 19:34:04 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 105s 211ms/step - batch_cost: 0.2103 - reader cost: 0.00\n",
      "2021-04-13 19:35:49 [INFO]\t[EVAL] #Images=500 mIoU=0.2184 Acc=0.9884 Kappa=0.6246 \n",
      "2021-04-13 19:35:49 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9899 0.1915 0.5257 0.5034 0.1731 0.3128 0.4285 0.     0.     0.\n",
      " 0.     0.1518 0.     0.     0.    ]\n",
      "2021-04-13 19:35:49 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9943 0.2724 0.7122 0.6453 0.2578 0.3953 0.6135 0.     0.     0.\n",
      " 0.     0.2964 0.     0.     0.    ]\n",
      "2021-04-13 19:35:51 [INFO]\t[EVAL] The model with the best validation mIoU (0.2192) was saved at iter 18000.\n",
      "2021-04-13 19:36:03 [INFO]\t[TRAIN] epoch=22, iter=18760/20000, loss=0.2174, lr=0.000205, batch_cost=1.2348, reader_cost=0.00009, ips=3.2394 samples/sec | ETA 00:25:31\n",
      "2021-04-13 19:36:15 [INFO]\t[TRAIN] epoch=22, iter=18770/20000, loss=0.2088, lr=0.000203, batch_cost=1.1878, reader_cost=0.00011, ips=3.3676 samples/sec | ETA 00:24:20\n",
      "2021-04-13 19:36:27 [INFO]\t[TRAIN] epoch=22, iter=18780/20000, loss=0.1936, lr=0.000202, batch_cost=1.1413, reader_cost=0.00011, ips=3.5049 samples/sec | ETA 00:23:12\n",
      "2021-04-13 19:36:39 [INFO]\t[TRAIN] epoch=22, iter=18790/20000, loss=0.1559, lr=0.000200, batch_cost=1.1802, reader_cost=0.00010, ips=3.3892 samples/sec | ETA 00:23:48\n",
      "2021-04-13 19:36:51 [INFO]\t[TRAIN] epoch=22, iter=18800/20000, loss=0.1895, lr=0.000199, batch_cost=1.1914, reader_cost=0.00010, ips=3.3573 samples/sec | ETA 00:23:49\n",
      "2021-04-13 19:37:02 [INFO]\t[TRAIN] epoch=22, iter=18810/20000, loss=0.1777, lr=0.000197, batch_cost=1.1307, reader_cost=0.00009, ips=3.5377 samples/sec | ETA 00:22:25\n",
      "2021-04-13 19:37:13 [INFO]\t[TRAIN] epoch=22, iter=18820/20000, loss=0.1860, lr=0.000196, batch_cost=1.1101, reader_cost=0.00010, ips=3.6031 samples/sec | ETA 00:21:49\n",
      "2021-04-13 19:37:25 [INFO]\t[TRAIN] epoch=22, iter=18830/20000, loss=0.1877, lr=0.000194, batch_cost=1.1636, reader_cost=0.00009, ips=3.4375 samples/sec | ETA 00:22:41\n",
      "2021-04-13 19:37:35 [INFO]\t[TRAIN] epoch=22, iter=18840/20000, loss=0.2137, lr=0.000193, batch_cost=1.0854, reader_cost=0.00009, ips=3.6854 samples/sec | ETA 00:20:59\n",
      "2021-04-13 19:37:47 [INFO]\t[TRAIN] epoch=22, iter=18850/20000, loss=0.2921, lr=0.000191, batch_cost=1.1602, reader_cost=0.00010, ips=3.4478 samples/sec | ETA 00:22:14\n",
      "2021-04-13 19:37:58 [INFO]\t[TRAIN] epoch=22, iter=18860/20000, loss=0.2264, lr=0.000190, batch_cost=1.1208, reader_cost=0.00009, ips=3.5687 samples/sec | ETA 00:21:17\n",
      "2021-04-13 19:38:10 [INFO]\t[TRAIN] epoch=22, iter=18870/20000, loss=0.1934, lr=0.000188, batch_cost=1.1336, reader_cost=0.00010, ips=3.5286 samples/sec | ETA 00:21:20\n",
      "2021-04-13 19:38:21 [INFO]\t[TRAIN] epoch=22, iter=18880/20000, loss=0.1845, lr=0.000187, batch_cost=1.1505, reader_cost=0.09078, ips=3.4768 samples/sec | ETA 00:21:28\n",
      "2021-04-13 19:38:33 [INFO]\t[TRAIN] epoch=22, iter=18890/20000, loss=0.1618, lr=0.000185, batch_cost=1.1510, reader_cost=0.00041, ips=3.4752 samples/sec | ETA 00:21:17\n",
      "2021-04-13 19:38:45 [INFO]\t[TRAIN] epoch=22, iter=18900/20000, loss=0.1843, lr=0.000184, batch_cost=1.2253, reader_cost=0.00010, ips=3.2644 samples/sec | ETA 00:22:27\n",
      "2021-04-13 19:38:56 [INFO]\t[TRAIN] epoch=22, iter=18910/20000, loss=0.1690, lr=0.000182, batch_cost=1.1395, reader_cost=0.00010, ips=3.5105 samples/sec | ETA 00:20:42\n",
      "2021-04-13 19:39:08 [INFO]\t[TRAIN] epoch=22, iter=18920/20000, loss=0.2026, lr=0.000181, batch_cost=1.1570, reader_cost=0.00009, ips=3.4572 samples/sec | ETA 00:20:49\n",
      "2021-04-13 19:39:19 [INFO]\t[TRAIN] epoch=22, iter=18930/20000, loss=0.1906, lr=0.000179, batch_cost=1.0821, reader_cost=0.00009, ips=3.6966 samples/sec | ETA 00:19:17\n",
      "2021-04-13 19:39:30 [INFO]\t[TRAIN] epoch=22, iter=18940/20000, loss=0.2032, lr=0.000178, batch_cost=1.0955, reader_cost=0.00009, ips=3.6512 samples/sec | ETA 00:19:21\n",
      "2021-04-13 19:39:40 [INFO]\t[TRAIN] epoch=22, iter=18950/20000, loss=0.1633, lr=0.000176, batch_cost=1.0767, reader_cost=0.00008, ips=3.7152 samples/sec | ETA 00:18:50\n",
      "2021-04-13 19:39:51 [INFO]\t[TRAIN] epoch=22, iter=18960/20000, loss=0.1761, lr=0.000175, batch_cost=1.1031, reader_cost=0.00009, ips=3.6263 samples/sec | ETA 00:19:07\n",
      "2021-04-13 19:40:02 [INFO]\t[TRAIN] epoch=22, iter=18970/20000, loss=0.1834, lr=0.000173, batch_cost=1.0935, reader_cost=0.00009, ips=3.6579 samples/sec | ETA 00:18:46\n",
      "2021-04-13 19:40:14 [INFO]\t[TRAIN] epoch=22, iter=18980/20000, loss=0.2008, lr=0.000172, batch_cost=1.1728, reader_cost=0.00010, ips=3.4107 samples/sec | ETA 00:19:56\n",
      "2021-04-13 19:40:26 [INFO]\t[TRAIN] epoch=22, iter=18990/20000, loss=0.1602, lr=0.000170, batch_cost=1.2152, reader_cost=0.00009, ips=3.2917 samples/sec | ETA 00:20:27\n",
      "2021-04-13 19:40:38 [INFO]\t[TRAIN] epoch=22, iter=19000/20000, loss=0.1804, lr=0.000169, batch_cost=1.1418, reader_cost=0.00010, ips=3.5033 samples/sec | ETA 00:19:01\n",
      "2021-04-13 19:40:50 [INFO]\t[TRAIN] epoch=22, iter=19010/20000, loss=0.1854, lr=0.000167, batch_cost=1.1895, reader_cost=0.00010, ips=3.3627 samples/sec | ETA 00:19:37\n",
      "2021-04-13 19:41:01 [INFO]\t[TRAIN] epoch=22, iter=19020/20000, loss=0.1766, lr=0.000166, batch_cost=1.1887, reader_cost=0.00010, ips=3.3651 samples/sec | ETA 00:19:24\n",
      "2021-04-13 19:41:13 [INFO]\t[TRAIN] epoch=22, iter=19030/20000, loss=0.2535, lr=0.000164, batch_cost=1.1913, reader_cost=0.00010, ips=3.3578 samples/sec | ETA 00:19:15\n",
      "2021-04-13 19:41:24 [INFO]\t[TRAIN] epoch=22, iter=19040/20000, loss=0.1741, lr=0.000163, batch_cost=1.1123, reader_cost=0.00009, ips=3.5961 samples/sec | ETA 00:17:47\n",
      "2021-04-13 19:41:36 [INFO]\t[TRAIN] epoch=22, iter=19050/20000, loss=0.1947, lr=0.000161, batch_cost=1.1148, reader_cost=0.00009, ips=3.5882 samples/sec | ETA 00:17:39\n",
      "2021-04-13 19:41:47 [INFO]\t[TRAIN] epoch=22, iter=19060/20000, loss=0.1892, lr=0.000160, batch_cost=1.1050, reader_cost=0.00008, ips=3.6198 samples/sec | ETA 00:17:18\n",
      "2021-04-13 19:41:58 [INFO]\t[TRAIN] epoch=22, iter=19070/20000, loss=0.1751, lr=0.000158, batch_cost=1.1075, reader_cost=0.00009, ips=3.6116 samples/sec | ETA 00:17:10\n",
      "2021-04-13 19:42:09 [INFO]\t[TRAIN] epoch=22, iter=19080/20000, loss=0.2246, lr=0.000157, batch_cost=1.1257, reader_cost=0.00009, ips=3.5534 samples/sec | ETA 00:17:15\n",
      "2021-04-13 19:42:20 [INFO]\t[TRAIN] epoch=22, iter=19090/20000, loss=0.1824, lr=0.000155, batch_cost=1.1294, reader_cost=0.00009, ips=3.5417 samples/sec | ETA 00:17:07\n",
      "2021-04-13 19:42:31 [INFO]\t[TRAIN] epoch=22, iter=19100/20000, loss=0.1688, lr=0.000154, batch_cost=1.0635, reader_cost=0.00009, ips=3.7611 samples/sec | ETA 00:15:57\n",
      "2021-04-13 19:42:42 [INFO]\t[TRAIN] epoch=22, iter=19110/20000, loss=0.1849, lr=0.000152, batch_cost=1.0773, reader_cost=0.00009, ips=3.7131 samples/sec | ETA 00:15:58\n",
      "2021-04-13 19:42:53 [INFO]\t[TRAIN] epoch=22, iter=19120/20000, loss=0.1784, lr=0.000150, batch_cost=1.1819, reader_cost=0.00009, ips=3.3844 samples/sec | ETA 00:17:20\n",
      "2021-04-13 19:43:05 [INFO]\t[TRAIN] epoch=22, iter=19130/20000, loss=0.1758, lr=0.000149, batch_cost=1.1451, reader_cost=0.00009, ips=3.4933 samples/sec | ETA 00:16:36\n",
      "2021-04-13 19:43:16 [INFO]\t[TRAIN] epoch=22, iter=19140/20000, loss=0.2022, lr=0.000147, batch_cost=1.1071, reader_cost=0.00010, ips=3.6132 samples/sec | ETA 00:15:52\n",
      "2021-04-13 19:43:27 [INFO]\t[TRAIN] epoch=22, iter=19150/20000, loss=0.1692, lr=0.000146, batch_cost=1.0755, reader_cost=0.00009, ips=3.7191 samples/sec | ETA 00:15:14\n",
      "2021-04-13 19:43:38 [INFO]\t[TRAIN] epoch=22, iter=19160/20000, loss=0.1854, lr=0.000144, batch_cost=1.1458, reader_cost=0.00011, ips=3.4911 samples/sec | ETA 00:16:02\n",
      "2021-04-13 19:43:49 [INFO]\t[TRAIN] epoch=22, iter=19170/20000, loss=0.1653, lr=0.000143, batch_cost=1.0662, reader_cost=0.00010, ips=3.7516 samples/sec | ETA 00:14:44\n",
      "2021-04-13 19:43:59 [INFO]\t[TRAIN] epoch=22, iter=19180/20000, loss=0.1714, lr=0.000141, batch_cost=1.0589, reader_cost=0.00009, ips=3.7776 samples/sec | ETA 00:14:28\n",
      "2021-04-13 19:44:11 [INFO]\t[TRAIN] epoch=22, iter=19190/20000, loss=0.1876, lr=0.000140, batch_cost=1.1283, reader_cost=0.00009, ips=3.5452 samples/sec | ETA 00:15:13\n",
      "2021-04-13 19:44:22 [INFO]\t[TRAIN] epoch=22, iter=19200/20000, loss=0.1826, lr=0.000138, batch_cost=1.0950, reader_cost=0.00009, ips=3.6529 samples/sec | ETA 00:14:36\n",
      "2021-04-13 19:44:33 [INFO]\t[TRAIN] epoch=22, iter=19210/20000, loss=0.1907, lr=0.000137, batch_cost=1.1055, reader_cost=0.00009, ips=3.6182 samples/sec | ETA 00:14:33\n",
      "2021-04-13 19:44:44 [INFO]\t[TRAIN] epoch=22, iter=19220/20000, loss=0.2101, lr=0.000135, batch_cost=1.1015, reader_cost=0.00008, ips=3.6313 samples/sec | ETA 00:14:19\n",
      "2021-04-13 19:44:55 [INFO]\t[TRAIN] epoch=22, iter=19230/20000, loss=0.1996, lr=0.000133, batch_cost=1.1003, reader_cost=0.00009, ips=3.6353 samples/sec | ETA 00:14:07\n",
      "2021-04-13 19:45:06 [INFO]\t[TRAIN] epoch=22, iter=19240/20000, loss=0.1974, lr=0.000132, batch_cost=1.1261, reader_cost=0.00009, ips=3.5522 samples/sec | ETA 00:14:15\n",
      "2021-04-13 19:45:17 [INFO]\t[TRAIN] epoch=22, iter=19250/20000, loss=0.2354, lr=0.000130, batch_cost=1.1255, reader_cost=0.00010, ips=3.5539 samples/sec | ETA 00:14:04\n",
      "2021-04-13 19:45:28 [INFO]\t[TRAIN] epoch=23, iter=19260/20000, loss=0.1669, lr=0.000129, batch_cost=1.0884, reader_cost=0.00012, ips=3.6751 samples/sec | ETA 00:13:25\n",
      "2021-04-13 19:45:40 [INFO]\t[TRAIN] epoch=23, iter=19270/20000, loss=0.2090, lr=0.000127, batch_cost=1.1328, reader_cost=0.00008, ips=3.5310 samples/sec | ETA 00:13:46\n",
      "2021-04-13 19:45:50 [INFO]\t[TRAIN] epoch=23, iter=19280/20000, loss=0.1868, lr=0.000126, batch_cost=1.0909, reader_cost=0.00008, ips=3.6666 samples/sec | ETA 00:13:05\n",
      "2021-04-13 19:46:02 [INFO]\t[TRAIN] epoch=23, iter=19290/20000, loss=0.1987, lr=0.000124, batch_cost=1.1332, reader_cost=0.00009, ips=3.5299 samples/sec | ETA 00:13:24\n",
      "2021-04-13 19:46:13 [INFO]\t[TRAIN] epoch=23, iter=19300/20000, loss=0.2035, lr=0.000123, batch_cost=1.1132, reader_cost=0.00009, ips=3.5933 samples/sec | ETA 00:12:59\n",
      "2021-04-13 19:46:23 [INFO]\t[TRAIN] epoch=23, iter=19310/20000, loss=0.1906, lr=0.000121, batch_cost=1.0286, reader_cost=0.00008, ips=3.8887 samples/sec | ETA 00:11:49\n",
      "2021-04-13 19:46:34 [INFO]\t[TRAIN] epoch=23, iter=19320/20000, loss=0.1730, lr=0.000119, batch_cost=1.0656, reader_cost=0.00009, ips=3.7538 samples/sec | ETA 00:12:04\n",
      "2021-04-13 19:46:46 [INFO]\t[TRAIN] epoch=23, iter=19330/20000, loss=0.1568, lr=0.000118, batch_cost=1.1966, reader_cost=0.00008, ips=3.3427 samples/sec | ETA 00:13:21\n",
      "2021-04-13 19:46:57 [INFO]\t[TRAIN] epoch=23, iter=19340/20000, loss=0.1850, lr=0.000116, batch_cost=1.0924, reader_cost=0.00008, ips=3.6618 samples/sec | ETA 00:12:00\n",
      "2021-04-13 19:47:08 [INFO]\t[TRAIN] epoch=23, iter=19350/20000, loss=0.2219, lr=0.000115, batch_cost=1.1368, reader_cost=0.00008, ips=3.5186 samples/sec | ETA 00:12:18\n",
      "2021-04-13 19:47:19 [INFO]\t[TRAIN] epoch=23, iter=19360/20000, loss=0.1779, lr=0.000113, batch_cost=1.1281, reader_cost=0.00009, ips=3.5456 samples/sec | ETA 00:12:02\n",
      "2021-04-13 19:47:30 [INFO]\t[TRAIN] epoch=23, iter=19370/20000, loss=0.1958, lr=0.000111, batch_cost=1.0862, reader_cost=0.00010, ips=3.6824 samples/sec | ETA 00:11:24\n",
      "2021-04-13 19:47:41 [INFO]\t[TRAIN] epoch=23, iter=19380/20000, loss=0.1806, lr=0.000110, batch_cost=1.1151, reader_cost=0.00009, ips=3.5872 samples/sec | ETA 00:11:31\n",
      "2021-04-13 19:47:52 [INFO]\t[TRAIN] epoch=23, iter=19390/20000, loss=0.1661, lr=0.000108, batch_cost=1.0508, reader_cost=0.00009, ips=3.8068 samples/sec | ETA 00:10:40\n",
      "2021-04-13 19:48:03 [INFO]\t[TRAIN] epoch=23, iter=19400/20000, loss=0.2052, lr=0.000107, batch_cost=1.1395, reader_cost=0.00009, ips=3.5102 samples/sec | ETA 00:11:23\n",
      "2021-04-13 19:48:16 [INFO]\t[TRAIN] epoch=23, iter=19410/20000, loss=0.1536, lr=0.000105, batch_cost=1.2392, reader_cost=0.00010, ips=3.2278 samples/sec | ETA 00:12:11\n",
      "2021-04-13 19:48:27 [INFO]\t[TRAIN] epoch=23, iter=19420/20000, loss=0.1384, lr=0.000103, batch_cost=1.1016, reader_cost=0.00010, ips=3.6310 samples/sec | ETA 00:10:38\n",
      "2021-04-13 19:48:38 [INFO]\t[TRAIN] epoch=23, iter=19430/20000, loss=0.2039, lr=0.000102, batch_cost=1.0906, reader_cost=0.00010, ips=3.6677 samples/sec | ETA 00:10:21\n",
      "2021-04-13 19:48:48 [INFO]\t[TRAIN] epoch=23, iter=19440/20000, loss=0.1528, lr=0.000100, batch_cost=1.0540, reader_cost=0.00009, ips=3.7949 samples/sec | ETA 00:09:50\n",
      "2021-04-13 19:48:59 [INFO]\t[TRAIN] epoch=23, iter=19450/20000, loss=0.2304, lr=0.000099, batch_cost=1.0929, reader_cost=0.00010, ips=3.6601 samples/sec | ETA 00:10:01\n",
      "2021-04-13 19:49:11 [INFO]\t[TRAIN] epoch=23, iter=19460/20000, loss=0.2018, lr=0.000097, batch_cost=1.1820, reader_cost=0.00009, ips=3.3842 samples/sec | ETA 00:10:38\n",
      "2021-04-13 19:49:22 [INFO]\t[TRAIN] epoch=23, iter=19470/20000, loss=0.1960, lr=0.000095, batch_cost=1.1410, reader_cost=0.00008, ips=3.5057 samples/sec | ETA 00:10:04\n",
      "2021-04-13 19:49:34 [INFO]\t[TRAIN] epoch=23, iter=19480/20000, loss=0.1690, lr=0.000094, batch_cost=1.1238, reader_cost=0.00010, ips=3.5592 samples/sec | ETA 00:09:44\n",
      "2021-04-13 19:49:45 [INFO]\t[TRAIN] epoch=23, iter=19490/20000, loss=0.1931, lr=0.000092, batch_cost=1.1147, reader_cost=0.00012, ips=3.5883 samples/sec | ETA 00:09:28\n",
      "2021-04-13 19:49:56 [INFO]\t[TRAIN] epoch=23, iter=19500/20000, loss=0.2054, lr=0.000091, batch_cost=1.1381, reader_cost=0.00010, ips=3.5145 samples/sec | ETA 00:09:29\n",
      "2021-04-13 19:49:56 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 106s 213ms/step - batch_cost: 0.2125 - reader cost: 0.001\n",
      "2021-04-13 19:51:43 [INFO]\t[EVAL] #Images=500 mIoU=0.2213 Acc=0.9885 Kappa=0.6290 \n",
      "2021-04-13 19:51:43 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9899 0.2045 0.5309 0.5099 0.1732 0.3145 0.4355 0.     0.     0.\n",
      " 0.     0.161  0.     0.     0.    ]\n",
      "2021-04-13 19:51:43 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9944 0.3066 0.7081 0.6616 0.2835 0.4026 0.5962 0.     0.     0.\n",
      " 0.     0.2803 0.     0.     0.    ]\n",
      "2021-04-13 19:51:46 [INFO]\t[EVAL] The model with the best validation mIoU (0.2213) was saved at iter 19500.\n",
      "2021-04-13 19:51:57 [INFO]\t[TRAIN] epoch=23, iter=19510/20000, loss=0.2054, lr=0.000089, batch_cost=1.1445, reader_cost=0.00010, ips=3.4949 samples/sec | ETA 00:09:20\n",
      "2021-04-13 19:52:09 [INFO]\t[TRAIN] epoch=23, iter=19520/20000, loss=0.1871, lr=0.000087, batch_cost=1.1151, reader_cost=0.00009, ips=3.5871 samples/sec | ETA 00:08:55\n",
      "2021-04-13 19:52:20 [INFO]\t[TRAIN] epoch=23, iter=19530/20000, loss=0.1846, lr=0.000086, batch_cost=1.1901, reader_cost=0.00010, ips=3.3610 samples/sec | ETA 00:09:19\n",
      "2021-04-13 19:52:32 [INFO]\t[TRAIN] epoch=23, iter=19540/20000, loss=0.1847, lr=0.000084, batch_cost=1.1749, reader_cost=0.00009, ips=3.4045 samples/sec | ETA 00:09:00\n",
      "2021-04-13 19:52:43 [INFO]\t[TRAIN] epoch=23, iter=19550/20000, loss=0.2124, lr=0.000082, batch_cost=1.0730, reader_cost=0.00115, ips=3.7280 samples/sec | ETA 00:08:02\n",
      "2021-04-13 19:52:54 [INFO]\t[TRAIN] epoch=23, iter=19560/20000, loss=0.1749, lr=0.000081, batch_cost=1.0888, reader_cost=0.00009, ips=3.6738 samples/sec | ETA 00:07:59\n",
      "2021-04-13 19:53:05 [INFO]\t[TRAIN] epoch=23, iter=19570/20000, loss=0.1888, lr=0.000079, batch_cost=1.1479, reader_cost=0.00010, ips=3.4847 samples/sec | ETA 00:08:13\n",
      "2021-04-13 19:53:17 [INFO]\t[TRAIN] epoch=23, iter=19580/20000, loss=0.1735, lr=0.000077, batch_cost=1.1418, reader_cost=0.00009, ips=3.5032 samples/sec | ETA 00:07:59\n",
      "2021-04-13 19:53:28 [INFO]\t[TRAIN] epoch=23, iter=19590/20000, loss=0.1826, lr=0.000076, batch_cost=1.0958, reader_cost=0.00010, ips=3.6501 samples/sec | ETA 00:07:29\n",
      "2021-04-13 19:53:39 [INFO]\t[TRAIN] epoch=23, iter=19600/20000, loss=0.1709, lr=0.000074, batch_cost=1.1579, reader_cost=0.00011, ips=3.4546 samples/sec | ETA 00:07:43\n",
      "2021-04-13 19:53:51 [INFO]\t[TRAIN] epoch=23, iter=19610/20000, loss=0.2015, lr=0.000072, batch_cost=1.1929, reader_cost=0.00010, ips=3.3532 samples/sec | ETA 00:07:45\n",
      "2021-04-13 19:54:02 [INFO]\t[TRAIN] epoch=23, iter=19620/20000, loss=0.1417, lr=0.000071, batch_cost=1.0988, reader_cost=0.00009, ips=3.6404 samples/sec | ETA 00:06:57\n",
      "2021-04-13 19:54:14 [INFO]\t[TRAIN] epoch=23, iter=19630/20000, loss=0.1878, lr=0.000069, batch_cost=1.2105, reader_cost=0.00011, ips=3.3045 samples/sec | ETA 00:07:27\n",
      "2021-04-13 19:54:26 [INFO]\t[TRAIN] epoch=23, iter=19640/20000, loss=0.1762, lr=0.000067, batch_cost=1.1779, reader_cost=0.00010, ips=3.3959 samples/sec | ETA 00:07:04\n",
      "2021-04-13 19:54:37 [INFO]\t[TRAIN] epoch=23, iter=19650/20000, loss=0.1770, lr=0.000066, batch_cost=1.0915, reader_cost=0.00009, ips=3.6646 samples/sec | ETA 00:06:22\n",
      "2021-04-13 19:54:48 [INFO]\t[TRAIN] epoch=23, iter=19660/20000, loss=0.1990, lr=0.000064, batch_cost=1.1479, reader_cost=0.00009, ips=3.4845 samples/sec | ETA 00:06:30\n",
      "2021-04-13 19:55:01 [INFO]\t[TRAIN] epoch=23, iter=19670/20000, loss=0.1879, lr=0.000062, batch_cost=1.2117, reader_cost=0.00010, ips=3.3012 samples/sec | ETA 00:06:39\n",
      "2021-04-13 19:55:12 [INFO]\t[TRAIN] epoch=23, iter=19680/20000, loss=0.2099, lr=0.000061, batch_cost=1.1125, reader_cost=0.00009, ips=3.5956 samples/sec | ETA 00:05:55\n",
      "2021-04-13 19:55:22 [INFO]\t[TRAIN] epoch=23, iter=19690/20000, loss=0.1886, lr=0.000059, batch_cost=1.0782, reader_cost=0.00010, ips=3.7098 samples/sec | ETA 00:05:34\n",
      "2021-04-13 19:55:34 [INFO]\t[TRAIN] epoch=23, iter=19700/20000, loss=0.1499, lr=0.000057, batch_cost=1.1155, reader_cost=0.00010, ips=3.5859 samples/sec | ETA 00:05:34\n",
      "2021-04-13 19:55:45 [INFO]\t[TRAIN] epoch=23, iter=19710/20000, loss=0.2010, lr=0.000056, batch_cost=1.0914, reader_cost=0.00009, ips=3.6651 samples/sec | ETA 00:05:16\n",
      "2021-04-13 19:55:56 [INFO]\t[TRAIN] epoch=23, iter=19720/20000, loss=0.1764, lr=0.000054, batch_cost=1.1149, reader_cost=0.00009, ips=3.5878 samples/sec | ETA 00:05:12\n",
      "2021-04-13 19:56:06 [INFO]\t[TRAIN] epoch=23, iter=19730/20000, loss=0.1960, lr=0.000052, batch_cost=1.0455, reader_cost=0.00009, ips=3.8258 samples/sec | ETA 00:04:42\n",
      "2021-04-13 19:56:17 [INFO]\t[TRAIN] epoch=23, iter=19740/20000, loss=0.1995, lr=0.000050, batch_cost=1.1141, reader_cost=0.00009, ips=3.5904 samples/sec | ETA 00:04:49\n",
      "2021-04-13 19:56:27 [INFO]\t[TRAIN] epoch=23, iter=19750/20000, loss=0.1943, lr=0.000049, batch_cost=1.0006, reader_cost=0.00010, ips=3.9977 samples/sec | ETA 00:04:10\n",
      "2021-04-13 19:56:41 [INFO]\t[TRAIN] epoch=23, iter=19760/20000, loss=0.1690, lr=0.000047, batch_cost=1.3368, reader_cost=0.08217, ips=2.9922 samples/sec | ETA 00:05:20\n",
      "2021-04-13 19:56:52 [INFO]\t[TRAIN] epoch=23, iter=19770/20000, loss=0.2305, lr=0.000045, batch_cost=1.1137, reader_cost=0.00009, ips=3.5916 samples/sec | ETA 00:04:16\n",
      "2021-04-13 19:57:03 [INFO]\t[TRAIN] epoch=23, iter=19780/20000, loss=0.1785, lr=0.000043, batch_cost=1.1202, reader_cost=0.00010, ips=3.5709 samples/sec | ETA 00:04:06\n",
      "2021-04-13 19:57:14 [INFO]\t[TRAIN] epoch=23, iter=19790/20000, loss=0.1793, lr=0.000042, batch_cost=1.1226, reader_cost=0.00009, ips=3.5631 samples/sec | ETA 00:03:55\n",
      "2021-04-13 19:57:26 [INFO]\t[TRAIN] epoch=23, iter=19800/20000, loss=0.1905, lr=0.000040, batch_cost=1.1495, reader_cost=0.00009, ips=3.4798 samples/sec | ETA 00:03:49\n",
      "2021-04-13 19:57:37 [INFO]\t[TRAIN] epoch=23, iter=19810/20000, loss=0.2143, lr=0.000038, batch_cost=1.1102, reader_cost=0.00010, ips=3.6030 samples/sec | ETA 00:03:30\n",
      "2021-04-13 19:57:48 [INFO]\t[TRAIN] epoch=23, iter=19820/20000, loss=0.2316, lr=0.000036, batch_cost=1.1453, reader_cost=0.00008, ips=3.4925 samples/sec | ETA 00:03:26\n",
      "2021-04-13 19:57:58 [INFO]\t[TRAIN] epoch=23, iter=19830/20000, loss=0.1914, lr=0.000034, batch_cost=1.0242, reader_cost=0.00009, ips=3.9054 samples/sec | ETA 00:02:54\n",
      "2021-04-13 19:58:09 [INFO]\t[TRAIN] epoch=23, iter=19840/20000, loss=0.1572, lr=0.000033, batch_cost=1.0301, reader_cost=0.00010, ips=3.8830 samples/sec | ETA 00:02:44\n",
      "2021-04-13 19:58:19 [INFO]\t[TRAIN] epoch=23, iter=19850/20000, loss=0.1959, lr=0.000031, batch_cost=1.0506, reader_cost=0.00010, ips=3.8073 samples/sec | ETA 00:02:37\n",
      "2021-04-13 19:58:31 [INFO]\t[TRAIN] epoch=23, iter=19860/20000, loss=0.1875, lr=0.000029, batch_cost=1.1786, reader_cost=0.00009, ips=3.3938 samples/sec | ETA 00:02:45\n",
      "2021-04-13 19:58:42 [INFO]\t[TRAIN] epoch=23, iter=19870/20000, loss=0.2037, lr=0.000027, batch_cost=1.1154, reader_cost=0.00009, ips=3.5862 samples/sec | ETA 00:02:24\n",
      "2021-04-13 19:58:53 [INFO]\t[TRAIN] epoch=23, iter=19880/20000, loss=0.1967, lr=0.000025, batch_cost=1.0951, reader_cost=0.00009, ips=3.6525 samples/sec | ETA 00:02:11\n",
      "2021-04-13 19:59:04 [INFO]\t[TRAIN] epoch=23, iter=19890/20000, loss=0.1966, lr=0.000023, batch_cost=1.1258, reader_cost=0.00011, ips=3.5530 samples/sec | ETA 00:02:03\n",
      "2021-04-13 19:59:15 [INFO]\t[TRAIN] epoch=23, iter=19900/20000, loss=0.1745, lr=0.000021, batch_cost=1.0835, reader_cost=0.00010, ips=3.6919 samples/sec | ETA 00:01:48\n",
      "2021-04-13 19:59:27 [INFO]\t[TRAIN] epoch=23, iter=19910/20000, loss=0.1789, lr=0.000020, batch_cost=1.2121, reader_cost=0.00009, ips=3.3001 samples/sec | ETA 00:01:49\n",
      "2021-04-13 19:59:38 [INFO]\t[TRAIN] epoch=23, iter=19920/20000, loss=0.1861, lr=0.000018, batch_cost=1.1053, reader_cost=0.00092, ips=3.6188 samples/sec | ETA 00:01:28\n",
      "2021-04-13 19:59:50 [INFO]\t[TRAIN] epoch=23, iter=19930/20000, loss=0.1753, lr=0.000016, batch_cost=1.1310, reader_cost=0.00009, ips=3.5368 samples/sec | ETA 00:01:19\n",
      "2021-04-13 20:00:01 [INFO]\t[TRAIN] epoch=23, iter=19940/20000, loss=0.1757, lr=0.000014, batch_cost=1.1146, reader_cost=0.00008, ips=3.5887 samples/sec | ETA 00:01:06\n",
      "2021-04-13 20:00:13 [INFO]\t[TRAIN] epoch=23, iter=19950/20000, loss=0.1588, lr=0.000012, batch_cost=1.1707, reader_cost=0.00010, ips=3.4167 samples/sec | ETA 00:00:58\n",
      "2021-04-13 20:00:24 [INFO]\t[TRAIN] epoch=23, iter=19960/20000, loss=0.1972, lr=0.000010, batch_cost=1.1129, reader_cost=0.00011, ips=3.5942 samples/sec | ETA 00:00:44\n",
      "2021-04-13 20:00:35 [INFO]\t[TRAIN] epoch=23, iter=19970/20000, loss=0.1771, lr=0.000007, batch_cost=1.1197, reader_cost=0.00010, ips=3.5724 samples/sec | ETA 00:00:33\n",
      "2021-04-13 20:00:47 [INFO]\t[TRAIN] epoch=23, iter=19980/20000, loss=0.2070, lr=0.000005, batch_cost=1.1942, reader_cost=0.00010, ips=3.3496 samples/sec | ETA 00:00:23\n",
      "2021-04-13 20:00:59 [INFO]\t[TRAIN] epoch=23, iter=19990/20000, loss=0.1609, lr=0.000003, batch_cost=1.1719, reader_cost=0.00010, ips=3.4133 samples/sec | ETA 00:00:11\n",
      "2021-04-13 20:01:10 [INFO]\t[TRAIN] epoch=23, iter=20000/20000, loss=0.1564, lr=0.000000, batch_cost=1.1041, reader_cost=0.00010, ips=3.6228 samples/sec | ETA 00:00:00\n",
      "2021-04-13 20:01:10 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "500/500 [==============================] - 108s 216ms/step - batch_cost: 0.2158 - reader cost: 0.001\n",
      "2021-04-13 20:02:58 [INFO]\t[EVAL] #Images=500 mIoU=0.2195 Acc=0.9885 Kappa=0.6262 \n",
      "2021-04-13 20:02:58 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9899 0.1984 0.529  0.5056 0.1689 0.3098 0.4318 0.     0.     0.\n",
      " 0.     0.1585 0.     0.     0.    ]\n",
      "2021-04-13 20:02:58 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9943 0.2885 0.7156 0.6507 0.2787 0.3954 0.5912 0.     0.     0.\n",
      " 0.     0.2804 0.     0.     0.    ]\n",
      "2021-04-13 20:03:00 [INFO]\t[EVAL] The model with the best validation mIoU (0.2213) was saved at iter 19500.\n",
      "<class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted\n",
      "Customize Function has been applied to <class 'paddle.nn.layer.norm.SyncBatchNorm'>\n",
      "Cannot find suitable count function for <class 'paddleseg.models.ocrnet.SpatialGatherBlock'>. Treat it as zero FLOPs.\n",
      "Cannot find suitable count function for <class 'paddle.nn.layer.common.Dropout2D'>. Treat it as zero FLOPs.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.FP32, but right dtype is VarType.INT32, the right dtype will convert to VarType.FP32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "Total Flops: 106278256640     Total Params: 12120094\n"
     ]
    }
   ],
   "source": [
    "# 也可以在项目提供的6000轮训练结果上继续训练\n",
    "!python train.py \\\n",
    "       --config configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml \\\n",
    "       --resume_model output/iter_12750 \\\n",
    "       --do_eval \\\n",
    "       --use_vdl \\\n",
    "       --save_interval 750 \\\n",
    "       --save_dir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2021-04-13 20:20:05 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "SOLVER:\n",
      "  CROSS_ENTROPY_WEIGHT: dynamic\n",
      "  LR: 0.005\n",
      "  LR_POLICY: poly\n",
      "  NUM_EPOCHS: 40\n",
      "  OPTIMIZER: sgd\n",
      "batch_size: 4\n",
      "iters: 20000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.0025\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  - 0.4\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "    type: HRNet_W18\n",
      "  backbone_indices:\n",
      "  - 0\n",
      "  type: OCRNet\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: train\n",
      "  num_classes: 15\n",
      "  train_path: /home/aistudio/train_list.txt\n",
      "  transforms:\n",
      "  - max_scale_factor: 2.0\n",
      "    min_scale_factor: 0.5\n",
      "    scale_step_size: 0.25\n",
      "    type: ResizeStepScaling\n",
      "  - max_rotation: 30\n",
      "    type: RandomRotation\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - crop_size:\n",
      "    - 1024\n",
      "    - 512\n",
      "    type: RandomPaddingCrop\n",
      "  - type: RandomBlur\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: val\n",
      "  num_classes: 15\n",
      "  transforms:\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/val_list.txt\n",
      "------------------------------------------------\n",
      "W0413 20:20:05.982918  9506 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0413 20:20:05.982966  9506 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-04-13 20:20:09 [INFO]\tLoading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "2021-04-13 20:20:09,857 - INFO - Lock 139793140909904 acquired on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:20:09,857 - INFO - Lock 139793140909904 released on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:20:11 [INFO]\tThere are 1525/1525 variables loaded into HRNet.\n",
      "2021-04-13 20:20:11 [INFO]\tLoading pretrained model from output/best_model/model.pdparams\n",
      "2021-04-13 20:20:12 [INFO]\tThere are 1583/1583 variables loaded into OCRNet.\n",
      "2021-04-13 20:20:12 [INFO]\tLoaded trained params of model successfully\n",
      "2021-04-13 20:20:12 [INFO]\tStart evaluating (total_samples=500, total_iters=500)...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT32, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT64, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "500/500 [==============================] - 108s 216ms/step - batch_cost: 0.2156 - reader cost: 0.055\n",
      "2021-04-13 20:22:00 [INFO]\t[EVAL] #Images=500 mIoU=0.2213 Acc=0.9885 Kappa=0.6290 \n",
      "2021-04-13 20:22:00 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9899 0.2045 0.5309 0.5099 0.1732 0.3145 0.4355 0.     0.     0.\n",
      " 0.     0.161  0.     0.     0.    ]\n",
      "2021-04-13 20:22:00 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9944 0.3066 0.7081 0.6616 0.2835 0.4026 0.5962 0.     0.     0.\n",
      " 0.     0.2803 0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "!python val.py \\\n",
    "       --config configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml \\\n",
    "       --model_path output/best_model/model.pdparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2021-04-13 20:29:06 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "SOLVER:\n",
      "  CROSS_ENTROPY_WEIGHT: dynamic\n",
      "  LR: 0.005\n",
      "  LR_POLICY: poly\n",
      "  NUM_EPOCHS: 40\n",
      "  OPTIMIZER: sgd\n",
      "batch_size: 4\n",
      "iters: 20000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.0025\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  - 0.4\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "    type: HRNet_W18\n",
      "  backbone_indices:\n",
      "  - 0\n",
      "  type: OCRNet\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: train\n",
      "  num_classes: 15\n",
      "  train_path: /home/aistudio/train_list.txt\n",
      "  transforms:\n",
      "  - max_scale_factor: 2.0\n",
      "    min_scale_factor: 0.5\n",
      "    scale_step_size: 0.25\n",
      "    type: ResizeStepScaling\n",
      "  - max_rotation: 30\n",
      "    type: RandomRotation\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - crop_size:\n",
      "    - 1024\n",
      "    - 512\n",
      "    type: RandomPaddingCrop\n",
      "  - type: RandomBlur\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: val\n",
      "  num_classes: 15\n",
      "  transforms:\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/val_list.txt\n",
      "------------------------------------------------\n",
      "W0413 20:29:06.974445  9973 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0413 20:29:06.974503  9973 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-04-13 20:29:10 [INFO]\tLoading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "2021-04-13 20:29:10,897 - INFO - Lock 139676744877968 acquired on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:29:10,898 - INFO - Lock 139676744877968 released on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:29:12 [INFO]\tThere are 1525/1525 variables loaded into HRNet.\n",
      "2021-04-13 20:29:12 [INFO]\tNumber of predict images = 1\n",
      "2021-04-13 20:29:12 [INFO]\tLoading pretrained model from output/best_model/model.pdparams\n",
      "2021-04-13 20:29:13 [INFO]\tThere are 1583/1583 variables loaded into OCRNet.\n",
      "2021-04-13 20:29:13 [INFO]\tStart to predict...\n",
      "1/1 [==============================] - 1s 620ms/step\n"
     ]
    }
   ],
   "source": [
    "!python predict.py \\\n",
    "       --config configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml \\\n",
    "       --model_path output/best_model/model.pdparams \\\n",
    "       --image_path ../infer/4346.png \\\n",
    "       --save_dir output/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 导出静态图模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "W0413 20:30:27.097597 10098 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0413 20:30:27.102846 10098 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-04-13 20:30:31 [INFO]\tLoading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "2021-04-13 20:30:31,020 - INFO - Lock 140131666467024 acquired on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:30:31,021 - INFO - Lock 140131666467024 released on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:30:32 [INFO]\tThere are 1525/1525 variables loaded into HRNet.\n",
      "2021-04-13 20:30:32 [INFO]\tLoaded trained params of model successfully.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp_v_r9u9i.py:26\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpm1iy8uqs.py:26\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpu1jo33om.py:26\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmparx3qj0k.py:26\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpu33aiiv8.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp_w_q4pqf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpqfk253d6.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpqlvzcfax.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpdronyhwn.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpw_jttyoe.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmprwvbs52i.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpdfr3owpa.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpk73ra14q.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpk73ra14q.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpo8xxvrxr.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp0t_ds7ny.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmptdj6pm3v.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpbp4r_88b.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpq5gagjuw.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmphwol6i16.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpvfl_gex8.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpz_mygokv.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp5pvnaqox.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmphnin9uvf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpnqm1lxpb.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpdu363iy3.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp7n3upk_f.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp7n3upk_f.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpabp0t6of.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp2cudk1k5.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpbocxbkde.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmprlkzyp38.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpwruzfzlo.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpm_nlbwq4.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpgveg22rc.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpz2tr8q9z.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp2qqa1hln.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp6psgh33o.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp7fnzaqq1.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpscc5jl3k.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp4gov3q0s.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp4gov3q0s.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp3430etku.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpri65v8ex.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpyq7bq7tx.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpexxo8q9m.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpuhmq09a5.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp0ltm0ke_.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpu8grqvwf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpeoi9hvmu.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpb34gihsh.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpfc35a2vv.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpq3zqfhgb.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpyq90e4zb.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpvzvxlo5v.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpvzvxlo5v.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpqgofudub.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp4jj04sex.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpv_25cupr.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmprvnuqgr9.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmplkxbzhx9.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpxhq7c8l0.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp66ftv446.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpbw_o_qhk.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmphgeifob1.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp61u8jgqf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp1474jytf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpdhlomefh.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpm570xzzh.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpm570xzzh.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpmts6enyj.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpvh6wji_h.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpnw3qg4cd.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpvp_h85u7.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp2brt2q4f.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp1lsdgdtf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpo76zx__b.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpld64lo_7.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpu7rnv5zq.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp1a1ro798.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp6219lt4w.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpy8oefu4b.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpcgrzddno.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpsjv7lj4n.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpg2kqgw9w.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpc3wujdne.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpda2rqyzc.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpda2rqyzc.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmprcibftes.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpkj8xgn9r.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpf3lp592g.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpphv9bmps.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpep_tpp2y.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp_6lgzalh.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpnkiws32f.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpy0itthfs.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpk5_pdvsg.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpru1vgf0k.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp0k1rjdyt.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpl8g_uf0_.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpz5lc1a1s.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpzvhhn0ls.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpo5_v6nni.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpoa71xt_8.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmppmy3egrd.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmppmy3egrd.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp0fkhvrba.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpm8tn7f93.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpxjdwp53w.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpj32fixte.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmp8g6lff1k.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpfq6h0n6c.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpp4_i2wi1.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpnju00vnx.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpgdxwyx8u.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpcj94t7kh.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpi6xpt4q0.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpiw71kxx3.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmparsu18kr.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpndoyweqn.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpwrl_lruy.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmpwfw5zpbf.py:25\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmph9fgqvux.py:34\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /tmp/tmph9fgqvux.py:56\n",
      "The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.\n",
      "  op_type, op_type, EXPRESSION_MAP[method_name]))\n",
      "2021-04-13 20:30:45 [INFO]\tModel is saved in ./output.\n"
     ]
    }
   ],
   "source": [
    "!python export.py \\\n",
    "       --config configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml \\\n",
    "       --model_path output/best_model/model.pdparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2021-04-13 20:38:16 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "SOLVER:\n",
      "  CROSS_ENTROPY_WEIGHT: dynamic\n",
      "  LR: 0.005\n",
      "  LR_POLICY: poly\n",
      "  NUM_EPOCHS: 40\n",
      "  OPTIMIZER: sgd\n",
      "batch_size: 4\n",
      "iters: 20000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.0025\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  - 0.4\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "  - coef:\n",
      "    - 0.8\n",
      "    - 0.2\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: LovaszSoftmaxLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "    type: HRNet_W18\n",
      "  backbone_indices:\n",
      "  - 0\n",
      "  type: OCRNet\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: train\n",
      "  num_classes: 15\n",
      "  train_path: /home/aistudio/train_list.txt\n",
      "  transforms:\n",
      "  - max_scale_factor: 2.0\n",
      "    min_scale_factor: 0.5\n",
      "    scale_step_size: 0.25\n",
      "    type: ResizeStepScaling\n",
      "  - max_rotation: 30\n",
      "    type: RandomRotation\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - crop_size:\n",
      "    - 1024\n",
      "    - 512\n",
      "    type: RandomPaddingCrop\n",
      "  - type: RandomBlur\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/\n",
      "  mode: val\n",
      "  num_classes: 15\n",
      "  transforms:\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/val_list.txt\n",
      "------------------------------------------------\n",
      "W0413 20:38:16.764972 10456 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0413 20:38:16.765028 10456 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-04-13 20:38:20 [INFO]\tLoading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
      "2021-04-13 20:38:20,639 - INFO - Lock 140642940987024 acquired on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:38:20,639 - INFO - Lock 140642940987024 released on /home/aistudio/.paddleseg/tmp/hrnet_w18_ssld\n",
      "2021-04-13 20:38:21 [INFO]\tThere are 1525/1525 variables loaded into HRNet.\n",
      "2021-04-13 20:38:21 [INFO]\tNumber of predict images = 1000\n",
      "2021-04-13 20:38:21 [INFO]\tLoading pretrained model from PaddleSeg/output/best_model/model.pdparams\n",
      "2021-04-13 20:38:22 [INFO]\tThere are 1583/1583 variables loaded into OCRNet.\n",
      "2021-04-13 20:38:23 [INFO]\tStart to predict...\n",
      "1000/1000 [==============================] - 539s 539ms/st\n"
     ]
    }
   ],
   "source": [
    "# 进行推理，其中--model_path后跟自己训练好的模型文件路径，一般选择bast_model或iter后最大值的参数文件\r\n",
    "!python PaddleSeg/predict.py --config PaddleSeg/configs/ocrnet/ocrnet_hrnetw18_cityscapes_1024x512_160k_lovasz_softmax.yml --model_path PaddleSeg/output/best_model/model.pdparams --image_path infer --save_dir output/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\r"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/output/result/pseudo_color_prediction\r\n",
    "!zip -r -o /home/aistudio/predict.zip ./\r\n",
    "%cd /home/aistudio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
