{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 快来选一顿好吃的年夜饭：看看如何自定义数据集，实现文本分类中的情感分析任务\n",
    "\n",
    "情感分析是自然语言处理领域一个老生常谈的任务。句子情感分析目的是为了判别说者的情感倾向，比如在某些话题上给出的的态度明确的观点，或者反映的情绪状态等。情感分析有着广泛应用，比如电商评论分析、舆情分析等。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/febb8a1478e34258953e56611ddc76cd20b412fec89845b0a4a2e6b9f8aae774\" hspace='10'/> <br />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 题目：\n",
    "将lstm网络替换成其他网络。可参考[seq2vec介绍](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "\n",
    "提示位置：self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 环境介绍\n",
    "\n",
    "- PaddlePaddle框架，AI Studio平台已经默认安装最新版2.0。\n",
    "\n",
    "- PaddleNLP，深度兼容框架2.0，是飞桨框架2.0在NLP领域的最佳实践。\n",
    "\n",
    "这里使用的是beta版本，马上也会发布rc版哦。AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp==2.0.0b4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/de/9ca615db516f438bae269b457f320ac0cbfa6c90242e80e29da8e2f5491c/paddlenlp-2.0.0b4-py3-none-any.whl (164kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 125kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.0.0b4) (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.0b4) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.0b4) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.0b4) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.0b4) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.0b4) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.0b4) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (7.2.0)\n",
      "\u001b[31mERROR: paddlehub 2.0.4 has requirement paddlenlp>=2.0.0rc5, but you'll have paddlenlp 2.0.0b4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.0rc7\n",
      "    Uninstalling paddlenlp-2.0.0rc7:\n",
      "      Successfully uninstalled paddlenlp-2.0.0rc7\n",
      "Successfully installed paddlenlp-2.0.0b4\n"
     ]
    }
   ],
   "source": [
    "# 下载paddlenlp\n",
    "!pip install --upgrade paddlenlp==2.0.0b4 -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "查看安装的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1 2.0.0b4\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddlenlp\n",
    "\n",
    "print(paddle.__version__, paddlenlp.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PaddleNLP和Paddle框架是什么关系？\n",
    "\n",
    "![]()\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/165924e86d9f4b5fa5d6fdee9e8496bf01be524e61f341b3879aceba48ae80fb\" width = \"300\" height = \"250\"  hspace='10'/> <br />\n",
    "</p><br></br>\n",
    "\n",
    "- Paddle框架是基础底座，提供深度学习任务全流程API。PaddleNLP基于Paddle框架开发，适用于NLP任务。\n",
    "\n",
    "PaddleNLP中数据处理、数据集、组网单元等API未来会沉淀到框架`paddle.text`中。\n",
    "\n",
    "\n",
    "- 代码中继承\n",
    "`class TSVDataset(paddle.io.Dataset)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用飞桨完成深度学习任务的通用流程\n",
    "\n",
    "- 数据集和数据处理  \n",
    "paddle.io.Dataset   \n",
    "paddle.io.DataLoader   \n",
    "paddlenlp.data   \n",
    "\n",
    "- 组网和网络配置\n",
    "\n",
    "paddle.nn.Embedding   \n",
    "paddlenlp.seq2vec\n",
    "paddle.nn.Linear   \n",
    "paddle.tanh\n",
    "\n",
    "paddle.nn.CrossEntropyLoss    \n",
    "paddle.metric.Accuracy   \n",
    "paddle.optimizer   \n",
    "\n",
    "model.prepare    \n",
    "\n",
    "- 网络训练和评估   \n",
    "model.fit   \n",
    "model.evaluate   \n",
    "\n",
    "- 预测\n",
    "model.predict   \n",
    "\n",
    "注意：建议在GPU下运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.datasets import MapDatasetWrapper\n",
    "\n",
    "from utils import load_vocab, convert_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据集和数据处理\n",
    "\n",
    "## 自定义数据集\n",
    "\n",
    "映射式(map-style)数据集需要继承`paddle.io.Dataset`\n",
    "\n",
    "- `__getitem__`: 根据给定索引获取数据集中指定样本，在 paddle.io.DataLoader 中需要使用此函数通过下标获取样本。\n",
    "\n",
    "- `__len__`: 返回数据集样本个数， paddle.io.BatchSampler 中需要样本个数生成下标序列。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SelfDefinedDataset(paddle.io.Dataset):\r\n",
    "    def __init__(self, data):\r\n",
    "        super(SelfDefinedDataset, self).__init__()\r\n",
    "        self.data = data\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.data[idx]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "        \r\n",
    "    def get_labels(self):\r\n",
    "        return [\"0\", \"1\",\"2\"]\r\n",
    "\r\n",
    "def txt_to_list(file_name):\r\n",
    "    res_list = []\r\n",
    "    for line in open(file_name):\r\n",
    "        res_list.append(line.strip().split('\\t'))\r\n",
    "    return res_list\r\n",
    "\r\n",
    "trainlst = txt_to_list('train.txt')\r\n",
    "devlst = txt_to_list('dev.txt')\r\n",
    "testlst = txt_to_list('test.txt')\r\n",
    "\r\n",
    "# 通过get_datasets()函数，将list数据转换为dataset。\r\n",
    "# get_datasets()可接收[list]参数，或[str]参数，根据自定义数据集的写法自由选择。\r\n",
    "# train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets(['train', 'dev', 'test'])\r\n",
    "train_ds, dev_ds, test_ds = SelfDefinedDataset.get_datasets([trainlst, devlst, testlst])\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "看看数据长什么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2']\n",
      "['环境不错，叉烧包小孩爱吃，三色煎糕很一般', '2']\n",
      "['刚来的时候让我们等位置，我们就在门口等了十分钟左右，没有见到有人离开，然后有工作人员让我们上来二楼，上来后看到有十来桌是没有人的，既然有位置，为什么非得让我们在门口等位！！！本以为有座位后就可以马上吃饭了，让人内心崩溃的是点菜就等了十分钟才有人有空过来理我们。上菜更是郁闷，端来了一锅猪肚鸡，但是没人开火，要点调味料也没人管，服务质量太差，之前来过一次觉得还可以，这次让我再也不想来这家店了！真心失望', '0']\n",
      "['口味还可以服务真的差到爆啊我来过45次真的次次都只给差评东西确实不错但你看看你们的服务还收服务费我的天干蒸什么的60块比太古汇翠园还贵主要是没人收台没人倒水谁还要自己倒我的天给你服务费还什么都自己干我接受不了钱花了服务没有实在不行', '0']\n",
      "['出品不错老字号就是好有山有水有树有鱼赞', '1']\n",
      "['在江南大道这间～服务态度好差，食物出品平凡，应该唔会再去了', '1']\n",
      "['我是个挺食得咸的人，但那个黑糖叉烧真是太咸了。招牌的菠萝包，分量好大，可是味道呢，只不过是普通酥皮包里面加了陷，而且那个陷，食不到菠萝，食不到菠萝味，只是一堆糖酱。水鬼重和粉丝煲味道算OK，不过那个鱼腐太硬。必须吐槽一下服务！一个稍胖的女服务员爱理不理的，解释菜式时一副不屑的样子！', '0']\n",
      "['Theplacewasgoodwasat29floorofplazaThefoodwetried:beeffriedrice..veryplaindoesn’thaveanyspecialtastenotevenanegginsidePorkbelly:nyummyworthtotryDurianpuffpastry:TOOsweet!Beefbraised:wasunderexpectationVegetable:so..sotheservicewasTERRIBLEThewaitressnotkind,doesn’thavewarmgreetingtoforeigner..Underexpectation..', '0']\n",
      "['早上要11点才开门，排队又要在广场上排，大热天，等死人了，为什么不可以让人进入大堂去等候呢?须要改进', '0']\n",
      "['应小伙伴的邀请，我评价一下＂空中一号＂吧，两个---失望（无奈），我两从一进去一看那茶杯就开始吐槽，期待着菜肴能好点，结果一道道的失望[难过][难过]。不好吃！！想来想去，它也就能边吃边看看小蛮腰吧，如此罢了。餐厅玻璃还很脏！！', '0']\n",
      "['菜色一般，价格高了点而且上碟量少，服务还可以。', '0']\n"
     ]
    }
   ],
   "source": [
    "label_list = train_ds.get_labels()\n",
    "print(label_list)\n",
    "\n",
    "for i in range(10):\n",
    "    print (train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "为了将原始数据处理成模型可以读入的格式，本项目将对数据作以下处理：\n",
    "\n",
    "- 首先使用jieba切词，之后将jieba切完后的单词映射词表中单词id。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c538bbd04cb2489ab4ff260133247fa7ab8fb0da17874179bc320d773977cb5d)\n",
    "\n",
    "- 使用`paddle.io.DataLoader`接口多线程异步加载数据。\n",
    "\n",
    "其中用到了PaddleNLP中关于数据处理的API。PaddleNLP提供了许多关于NLP任务中构建有效的数据pipeline的常用API\n",
    "\n",
    "| API                             | 简介                                       |\n",
    "| ------------------------------- | :----------------------------------------- |\n",
    "| `paddlenlp.data.Stack`          | 堆叠N个具有相同shape的输入数据来构建一个batch，它的输入必须具有相同的shape，输出便是这些输入的堆叠组成的batch数据。 |\n",
    "| `paddlenlp.data.Pad`            | 堆叠N个输入数据来构建一个batch，每个输入数据将会被padding到N个输入数据中最大的长度 |\n",
    "| `paddlenlp.data.Tuple`          | 将多个组batch的函数包装在一起 |\n",
    "\n",
    "更多数据处理操作详见： [https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-22 19:44:22--  https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt\n",
      "Resolving paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c00:6c21:10ad:0:ff:b00e:67d\n",
      "Connecting to paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14600150 (14M) [text/plain]\n",
      "Saving to: ‘senta_word_dict.txt’\n",
      "\n",
      "senta_word_dict.txt 100%[===================>]  13.92M  29.7MB/s    in 0.5s    \n",
      "\n",
      "2021-03-22 19:44:23 (29.7 MB/s) - ‘senta_word_dict.txt’ saved [14600150/14600150]\n",
      "\n",
      "[PAD] 0\n"
     ]
    }
   ],
   "source": [
    "# 下载词汇表文件word_dict.txt，用于构造词-id映射关系。\n",
    "!wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt\n",
    "\n",
    "# 加载词表\n",
    "vocab = load_vocab('./senta_word_dict.txt')\n",
    "\n",
    "for k, v in vocab.items():\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 构造dataloder\n",
    "\n",
    "下面的`create_data_loader`函数用于创建运行和预测时所需要的`DataLoader`对象。\n",
    "\n",
    "* `paddle.io.DataLoader`返回一个迭代器，该迭代器根据`batch_sampler`指定的顺序迭代返回dataset数据。异步加载数据。\n",
    "\n",
    "* `batch_sampler`：DataLoader通过 batch_sampler 产生的mini-batch索引列表来 dataset 中索引样本并组成mini-batch\n",
    "\n",
    "* `collate_fn`：指定如何将样本列表组合为mini-batch数据。传给它参数需要是一个callable对象，需要实现对组建的batch的处理逻辑，并返回每个batch的数据。在这里传入的是`prepare_input`函数，对产生的数据进行pad操作，并返回实际长度等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reads data and generates mini-batches.\n",
    "def create_dataloader(dataset,\n",
    "                      trans_function=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      pad_token_id=0,\n",
    "                      batchify_fn=None):\n",
    "    if trans_function:\n",
    "        dataset = dataset.apply(trans_function, lazy=True)\n",
    "\n",
    "    # return_list 数据是否以list形式返回\n",
    "    # collate_fn  指定如何将样本列表组合为mini-batch数据。传给它参数需要是一个callable对象，需要实现对组建的batch的处理逻辑，并返回每个batch的数据。在这里传入的是`prepare_input`函数，对产生的数据进行pad操作，并返回实际长度等。\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        return_list=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=batchify_fn)\n",
    "        \n",
    "    return dataloader\n",
    "\n",
    "# python中的偏函数partial，把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。\n",
    "trans_function = partial(\n",
    "    convert_example,\n",
    "    vocab=vocab,\n",
    "    unk_token_id=vocab.get('[UNK]', 1),\n",
    "    is_test=False)\n",
    "\n",
    "# 将读入的数据batch化处理，便于模型batch化运算。\n",
    "# batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。\n",
    "# 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=vocab['[PAD]']),  # input_ids\n",
    "    Stack(dtype=\"int64\"),  # seq len\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='train',\n",
    "    batchify_fn=batchify_fn)\n",
    "dev_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='validation',\n",
    "    batchify_fn=batchify_fn)\n",
    "test_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='test',\n",
    "    batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型搭建\n",
    "\n",
    "使用`LSTMencoder`搭建一个BiLSTM模型用于进行句子建模，得到句子的向量表示。\n",
    "\n",
    "然后接一个线性变换层，完成二分类任务。\n",
    "\n",
    "- `paddle.nn.Embedding`组建word-embedding层\n",
    "- `ppnlp.seq2vec.LSTMEncoder`组建句子建模层\n",
    "- `paddle.nn.Linear`构造二分类器\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecf309c20e5347399c55f1e067821daa088842fa46ad49be90de4933753cd3cf\" width = \"800\" height = \"450\"  hspace='10'/> <br />\n",
    "</p><br><center>图1：seq2vec示意图</center></br>\n",
    "\n",
    "* 除LSTM外，`seq2vec`还提供了许多语义表征方法，详细可参考：[seq2vec介绍](https://aistudio.baidu.com/aistudio/projectdetail/1283423)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "\n",
    "        # 首先将输入word id 查表后映射成 word embedding\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        # 将word embedding经过LSTMEncoder变换到文本语义表征空间中\n",
    "        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        # LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "\n",
    "        # 最后的分类器\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # text shape: (batch_size, num_tokens)\n",
    "        # print('input :', text.shape)\n",
    "        \n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # print('after word-embeding:', embedded_text.shape)\n",
    "\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "        # print('after lstm:', text_repr.shape)\n",
    "\n",
    "\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "        # print('after Linear classifier:', fc_out.shape)\n",
    "\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        # print('output:', logits.shape)\n",
    "        \n",
    "        # probs 分类概率值\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        # print('output probability:', probs.shape)\n",
    "        return probs\n",
    "\n",
    "model= LSTMModel(\n",
    "        len(vocab),\n",
    "        len(label_list),\n",
    "        direction='bidirectional',\n",
    "        padding_idx=vocab['[PAD]'])\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 vocab_size,\r\n",
    "                 num_classes,\r\n",
    "                 emb_dim=128,\r\n",
    "                 padding_idx=0,\r\n",
    "                 gru_hidden_size=198,\r\n",
    "                 direction='forward',\r\n",
    "                 gru_layers=1,\r\n",
    "                 dropout_rate=0,\r\n",
    "                 pooling_type=None,\r\n",
    "                 fc_hidden_size=96):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # 首先将输入word id 查表后映射成 word embedding\r\n",
    "        self.embedder = nn.Embedding(\r\n",
    "            num_embeddings=vocab_size,\r\n",
    "            embedding_dim=emb_dim,\r\n",
    "            padding_idx=padding_idx)\r\n",
    "\r\n",
    "        # 将word embedding经过GRUEncoder变换到文本语义表征空间中\r\n",
    "        self.gru_encoder = ppnlp.seq2vec.GRUEncoder(\r\n",
    "            emb_dim,\r\n",
    "            gru_hidden_size,\r\n",
    "            num_layers=gru_layers,\r\n",
    "            direction=direction,\r\n",
    "            dropout=dropout_rate,\r\n",
    "            pooling_type=pooling_type)\r\n",
    "\r\n",
    "        # GRUEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\r\n",
    "        self.fc = nn.Linear(self.gru_encoder.get_output_dim(), fc_hidden_size)\r\n",
    "\r\n",
    "        # 最后的分类器\r\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, text, seq_len):\r\n",
    "        # text shape: (batch_size, num_tokens)\r\n",
    "        # print('input :', text.shape)\r\n",
    "        \r\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\r\n",
    "        embedded_text = self.embedder(text)\r\n",
    "        # print('after word-embeding:', embedded_text.shape)\r\n",
    "\r\n",
    "        # Shape: (batch_size, num_tokens, num_directions*gru_hidden_size)\r\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\r\n",
    "        text_repr = self.gru_encoder(embedded_text, sequence_length=seq_len)\r\n",
    "        # print('after lstm:', text_repr.shape)\r\n",
    "\r\n",
    "\r\n",
    "        # Shape: (batch_size, fc_hidden_size)\r\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\r\n",
    "        # print('after Linear classifier:', fc_out.shape)\r\n",
    "\r\n",
    "        # Shape: (batch_size, num_classes)\r\n",
    "        logits = self.output_layer(fc_out)\r\n",
    "        # print('output:', logits.shape)\r\n",
    "        \r\n",
    "        # probs 分类概率值\r\n",
    "        probs = F.softmax(logits, axis=-1)\r\n",
    "        # print('output probability:', probs.shape)\r\n",
    "        return probs\r\n",
    "\r\n",
    "model= GRUModel(\r\n",
    "        len(vocab),\r\n",
    "        len(label_list),\r\n",
    "        direction='bidirectional',\r\n",
    "        padding_idx=vocab['[PAD]'])\r\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型配置和训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(\r\n",
    "        parameters=model.parameters(), learning_rate=5e-5)\r\n",
    "\r\n",
    "loss = paddle.nn.CrossEntropyLoss()\r\n",
    "metric = paddle.metric.Accuracy()\r\n",
    "\r\n",
    "model.prepare(optimizer, loss, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 设置visualdl路径\n",
    "log_dir = './visualdl'\n",
    "callback = paddle.callbacks.VisualDL(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练\n",
    "\n",
    "训练过程中会输出loss、acc等信息。这里设置了10个epoch，在训练集上准确率约97%。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/254cc9f80f474181a0f7fd00bb6f431502efdfdf54e54989a26549bc3abbe3c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2021-03-22 19:45:52,963 - DEBUG - Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "2021-03-22 19:45:53,883 - DEBUG - Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.907 seconds.\n",
      "2021-03-22 19:45:53,941 - DEBUG - Loading model cost 0.907 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2021-03-22 19:45:53,943 - DEBUG - Prefix dict has been built successfully.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/251 - loss: 1.0981 - acc: 0.3477 - 239ms/step\n",
      "step  20/251 - loss: 1.0957 - acc: 0.3477 - 167ms/step\n",
      "step  30/251 - loss: 1.0974 - acc: 0.3453 - 139ms/step\n",
      "step  40/251 - loss: 1.1007 - acc: 0.3385 - 127ms/step\n",
      "step  50/251 - loss: 1.0972 - acc: 0.3377 - 126ms/step\n",
      "step  60/251 - loss: 1.0997 - acc: 0.3367 - 122ms/step\n",
      "step  70/251 - loss: 1.0964 - acc: 0.3363 - 119ms/step\n",
      "step  80/251 - loss: 1.0973 - acc: 0.3394 - 117ms/step\n",
      "step  90/251 - loss: 1.0952 - acc: 0.3407 - 114ms/step\n",
      "step 100/251 - loss: 1.0980 - acc: 0.3416 - 113ms/step\n",
      "step 110/251 - loss: 1.0964 - acc: 0.3413 - 112ms/step\n",
      "step 120/251 - loss: 1.0966 - acc: 0.3432 - 111ms/step\n",
      "step 130/251 - loss: 1.0931 - acc: 0.3424 - 112ms/step\n",
      "step 140/251 - loss: 1.0932 - acc: 0.3429 - 111ms/step\n",
      "step 150/251 - loss: 1.0974 - acc: 0.3427 - 110ms/step\n",
      "step 160/251 - loss: 1.0886 - acc: 0.3438 - 109ms/step\n",
      "step 170/251 - loss: 1.0929 - acc: 0.3433 - 109ms/step\n",
      "step 180/251 - loss: 1.0814 - acc: 0.3450 - 109ms/step\n",
      "step 190/251 - loss: 1.0886 - acc: 0.3468 - 108ms/step\n",
      "step 200/251 - loss: 1.0799 - acc: 0.3477 - 107ms/step\n",
      "step 210/251 - loss: 1.0779 - acc: 0.3544 - 107ms/step\n",
      "step 220/251 - loss: 1.0582 - acc: 0.3638 - 107ms/step\n",
      "step 230/251 - loss: 1.0454 - acc: 0.3718 - 106ms/step\n",
      "step 240/251 - loss: 1.0287 - acc: 0.3778 - 106ms/step\n",
      "step 250/251 - loss: 0.9922 - acc: 0.3853 - 106ms/step\n",
      "step 251/251 - loss: 0.9537 - acc: 0.3861 - 106ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.9508 - acc: 0.5805 - 96ms/step\n",
      "step 20/31 - loss: 0.9194 - acc: 0.5836 - 81ms/step\n",
      "step 30/31 - loss: 0.9164 - acc: 0.5872 - 72ms/step\n",
      "step 31/31 - loss: 0.9792 - acc: 0.5869 - 70ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 2/10\n",
      "step  10/251 - loss: 0.9462 - acc: 0.5711 - 127ms/step\n",
      "step  20/251 - loss: 0.9778 - acc: 0.5730 - 113ms/step\n",
      "step  30/251 - loss: 0.9609 - acc: 0.5823 - 107ms/step\n",
      "step  40/251 - loss: 0.8833 - acc: 0.5912 - 106ms/step\n",
      "step  50/251 - loss: 0.8780 - acc: 0.5938 - 104ms/step\n",
      "step  60/251 - loss: 0.9401 - acc: 0.5951 - 103ms/step\n",
      "step  70/251 - loss: 0.8626 - acc: 0.5973 - 102ms/step\n",
      "step  80/251 - loss: 0.8475 - acc: 0.6014 - 101ms/step\n",
      "step  90/251 - loss: 0.9403 - acc: 0.6030 - 101ms/step\n",
      "step 100/251 - loss: 0.8681 - acc: 0.6063 - 100ms/step\n",
      "step 110/251 - loss: 0.9321 - acc: 0.6075 - 100ms/step\n",
      "step 120/251 - loss: 0.9074 - acc: 0.6115 - 100ms/step\n",
      "step 130/251 - loss: 0.8762 - acc: 0.6125 - 100ms/step\n",
      "step 140/251 - loss: 0.8732 - acc: 0.6158 - 100ms/step\n",
      "step 150/251 - loss: 0.9090 - acc: 0.6173 - 101ms/step\n",
      "step 160/251 - loss: 0.8506 - acc: 0.6207 - 101ms/step\n",
      "step 170/251 - loss: 0.8322 - acc: 0.6234 - 101ms/step\n",
      "step 180/251 - loss: 0.8528 - acc: 0.6279 - 101ms/step\n",
      "step 190/251 - loss: 0.8922 - acc: 0.6306 - 101ms/step\n",
      "step 200/251 - loss: 0.8531 - acc: 0.6338 - 101ms/step\n",
      "step 210/251 - loss: 0.8708 - acc: 0.6375 - 101ms/step\n",
      "step 220/251 - loss: 0.8309 - acc: 0.6412 - 100ms/step\n",
      "step 230/251 - loss: 0.7924 - acc: 0.6450 - 101ms/step\n",
      "step 240/251 - loss: 0.8573 - acc: 0.6471 - 101ms/step\n",
      "step 250/251 - loss: 0.8519 - acc: 0.6497 - 100ms/step\n",
      "step 251/251 - loss: 0.8075 - acc: 0.6501 - 100ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.8616 - acc: 0.6883 - 100ms/step\n",
      "step 20/31 - loss: 0.8264 - acc: 0.7031 - 83ms/step\n",
      "step 30/31 - loss: 0.8454 - acc: 0.7052 - 72ms/step\n",
      "step 31/31 - loss: 0.8705 - acc: 0.7041 - 71ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 3/10\n",
      "step  10/251 - loss: 0.8394 - acc: 0.7047 - 129ms/step\n",
      "step  20/251 - loss: 0.8806 - acc: 0.7105 - 113ms/step\n",
      "step  30/251 - loss: 0.8162 - acc: 0.7143 - 108ms/step\n",
      "step  40/251 - loss: 0.8055 - acc: 0.7170 - 104ms/step\n",
      "step  50/251 - loss: 0.7944 - acc: 0.7192 - 102ms/step\n",
      "step  60/251 - loss: 0.8259 - acc: 0.7167 - 101ms/step\n",
      "step  70/251 - loss: 0.7895 - acc: 0.7161 - 100ms/step\n",
      "step  80/251 - loss: 0.7826 - acc: 0.7193 - 100ms/step\n",
      "step  90/251 - loss: 0.8318 - acc: 0.7207 - 99ms/step\n",
      "step 100/251 - loss: 0.7801 - acc: 0.7234 - 99ms/step\n",
      "step 110/251 - loss: 0.8569 - acc: 0.7225 - 100ms/step\n",
      "step 120/251 - loss: 0.8330 - acc: 0.7230 - 100ms/step\n",
      "step 130/251 - loss: 0.8141 - acc: 0.7231 - 99ms/step\n",
      "step 140/251 - loss: 0.8031 - acc: 0.7241 - 100ms/step\n",
      "step 150/251 - loss: 0.8293 - acc: 0.7236 - 100ms/step\n",
      "step 160/251 - loss: 0.7749 - acc: 0.7250 - 100ms/step\n",
      "step 170/251 - loss: 0.7760 - acc: 0.7253 - 100ms/step\n",
      "step 180/251 - loss: 0.7707 - acc: 0.7267 - 101ms/step\n",
      "step 190/251 - loss: 0.8455 - acc: 0.7269 - 100ms/step\n",
      "step 200/251 - loss: 0.7888 - acc: 0.7277 - 100ms/step\n",
      "step 210/251 - loss: 0.8155 - acc: 0.7286 - 100ms/step\n",
      "step 220/251 - loss: 0.7696 - acc: 0.7308 - 100ms/step\n",
      "step 230/251 - loss: 0.7375 - acc: 0.7326 - 100ms/step\n",
      "step 240/251 - loss: 0.7954 - acc: 0.7326 - 99ms/step\n",
      "step 250/251 - loss: 0.8328 - acc: 0.7331 - 99ms/step\n",
      "step 251/251 - loss: 0.7689 - acc: 0.7333 - 98ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.8204 - acc: 0.7102 - 96ms/step\n",
      "step 20/31 - loss: 0.7835 - acc: 0.7176 - 81ms/step\n",
      "step 30/31 - loss: 0.8092 - acc: 0.7219 - 70ms/step\n",
      "step 31/31 - loss: 0.8472 - acc: 0.7210 - 69ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 4/10\n",
      "step  10/251 - loss: 0.8136 - acc: 0.7266 - 121ms/step\n",
      "step  20/251 - loss: 0.8422 - acc: 0.7422 - 107ms/step\n",
      "step  30/251 - loss: 0.7670 - acc: 0.7497 - 102ms/step\n",
      "step  40/251 - loss: 0.7741 - acc: 0.7521 - 94ms/step\n",
      "step  50/251 - loss: 0.7540 - acc: 0.7542 - 96ms/step\n",
      "step  60/251 - loss: 0.8157 - acc: 0.7523 - 97ms/step\n",
      "step  70/251 - loss: 0.7393 - acc: 0.7540 - 99ms/step\n",
      "step  80/251 - loss: 0.7402 - acc: 0.7572 - 101ms/step\n",
      "step  90/251 - loss: 0.7945 - acc: 0.7577 - 103ms/step\n",
      "step 100/251 - loss: 0.7549 - acc: 0.7608 - 103ms/step\n",
      "step 110/251 - loss: 0.8294 - acc: 0.7604 - 102ms/step\n",
      "step 120/251 - loss: 0.8016 - acc: 0.7616 - 100ms/step\n",
      "step 130/251 - loss: 0.7852 - acc: 0.7612 - 102ms/step\n",
      "step 140/251 - loss: 0.7667 - acc: 0.7616 - 101ms/step\n",
      "step 150/251 - loss: 0.8080 - acc: 0.7605 - 101ms/step\n",
      "step 160/251 - loss: 0.7462 - acc: 0.7614 - 102ms/step\n",
      "step 170/251 - loss: 0.7474 - acc: 0.7618 - 103ms/step\n",
      "step 180/251 - loss: 0.7591 - acc: 0.7629 - 103ms/step\n",
      "step 190/251 - loss: 0.8006 - acc: 0.7630 - 102ms/step\n",
      "step 200/251 - loss: 0.7465 - acc: 0.7647 - 102ms/step\n",
      "step 210/251 - loss: 0.7788 - acc: 0.7654 - 102ms/step\n",
      "step 220/251 - loss: 0.7423 - acc: 0.7669 - 103ms/step\n",
      "step 230/251 - loss: 0.7165 - acc: 0.7686 - 103ms/step\n",
      "step 240/251 - loss: 0.7660 - acc: 0.7685 - 103ms/step\n",
      "step 250/251 - loss: 0.8159 - acc: 0.7689 - 102ms/step\n",
      "step 251/251 - loss: 0.7557 - acc: 0.7690 - 102ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.8044 - acc: 0.7289 - 113ms/step\n",
      "step 20/31 - loss: 0.7700 - acc: 0.7340 - 116ms/step\n",
      "step 30/31 - loss: 0.7892 - acc: 0.7370 - 108ms/step\n",
      "step 31/31 - loss: 0.8417 - acc: 0.7356 - 106ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 5/10\n",
      "step  10/251 - loss: 0.7869 - acc: 0.7648 - 127ms/step\n",
      "step  20/251 - loss: 0.8308 - acc: 0.7723 - 109ms/step\n",
      "step  30/251 - loss: 0.7375 - acc: 0.7773 - 104ms/step\n",
      "step  40/251 - loss: 0.7605 - acc: 0.7799 - 102ms/step\n",
      "step  50/251 - loss: 0.7360 - acc: 0.7798 - 104ms/step\n",
      "step  60/251 - loss: 0.7764 - acc: 0.7781 - 106ms/step\n",
      "step  70/251 - loss: 0.7113 - acc: 0.7787 - 106ms/step\n",
      "step  80/251 - loss: 0.7313 - acc: 0.7804 - 104ms/step\n",
      "step  90/251 - loss: 0.7698 - acc: 0.7820 - 103ms/step\n",
      "step 100/251 - loss: 0.7127 - acc: 0.7853 - 102ms/step\n",
      "step 110/251 - loss: 0.7949 - acc: 0.7855 - 102ms/step\n",
      "step 120/251 - loss: 0.8062 - acc: 0.7859 - 102ms/step\n",
      "step 130/251 - loss: 0.7819 - acc: 0.7856 - 101ms/step\n",
      "step 140/251 - loss: 0.7477 - acc: 0.7862 - 102ms/step\n",
      "step 150/251 - loss: 0.7477 - acc: 0.7865 - 101ms/step\n",
      "step 160/251 - loss: 0.7238 - acc: 0.7875 - 100ms/step\n",
      "step 170/251 - loss: 0.7192 - acc: 0.7883 - 103ms/step\n",
      "step 180/251 - loss: 0.7410 - acc: 0.7893 - 103ms/step\n",
      "step 190/251 - loss: 0.7658 - acc: 0.7891 - 103ms/step\n",
      "step 200/251 - loss: 0.7211 - acc: 0.7905 - 102ms/step\n",
      "step 210/251 - loss: 0.7572 - acc: 0.7914 - 102ms/step\n",
      "step 220/251 - loss: 0.7203 - acc: 0.7925 - 102ms/step\n",
      "step 230/251 - loss: 0.6812 - acc: 0.7942 - 102ms/step\n",
      "step 240/251 - loss: 0.7623 - acc: 0.7937 - 101ms/step\n",
      "step 250/251 - loss: 0.7874 - acc: 0.7941 - 100ms/step\n",
      "step 251/251 - loss: 0.7551 - acc: 0.7941 - 100ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.8051 - acc: 0.7375 - 104ms/step\n",
      "step 20/31 - loss: 0.7658 - acc: 0.7418 - 84ms/step\n",
      "step 30/31 - loss: 0.7774 - acc: 0.7453 - 72ms/step\n",
      "step 31/31 - loss: 0.8197 - acc: 0.7447 - 70ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 6/10\n",
      "step  10/251 - loss: 0.7889 - acc: 0.7852 - 122ms/step\n",
      "step  20/251 - loss: 0.8212 - acc: 0.7789 - 110ms/step\n",
      "step  30/251 - loss: 0.7183 - acc: 0.7875 - 104ms/step\n",
      "step  40/251 - loss: 0.7229 - acc: 0.7934 - 101ms/step\n",
      "step  50/251 - loss: 0.7396 - acc: 0.7927 - 100ms/step\n",
      "step  60/251 - loss: 0.7576 - acc: 0.7917 - 99ms/step\n",
      "step  70/251 - loss: 0.7131 - acc: 0.7920 - 99ms/step\n",
      "step  80/251 - loss: 0.7350 - acc: 0.7939 - 98ms/step\n",
      "step  90/251 - loss: 0.7511 - acc: 0.7968 - 98ms/step\n",
      "step 100/251 - loss: 0.6932 - acc: 0.8005 - 98ms/step\n",
      "step 110/251 - loss: 0.7848 - acc: 0.8007 - 98ms/step\n",
      "step 120/251 - loss: 0.7663 - acc: 0.8021 - 99ms/step\n",
      "step 130/251 - loss: 0.7558 - acc: 0.8029 - 100ms/step\n",
      "step 140/251 - loss: 0.7303 - acc: 0.8033 - 100ms/step\n",
      "step 150/251 - loss: 0.7311 - acc: 0.8039 - 99ms/step\n",
      "step 160/251 - loss: 0.7042 - acc: 0.8044 - 99ms/step\n",
      "step 170/251 - loss: 0.7029 - acc: 0.8049 - 101ms/step\n",
      "step 180/251 - loss: 0.7372 - acc: 0.8059 - 101ms/step\n",
      "step 190/251 - loss: 0.7459 - acc: 0.8066 - 101ms/step\n",
      "step 200/251 - loss: 0.7433 - acc: 0.8072 - 101ms/step\n",
      "step 210/251 - loss: 0.8159 - acc: 0.8043 - 101ms/step\n",
      "step 220/251 - loss: 0.7881 - acc: 0.8016 - 101ms/step\n",
      "step 230/251 - loss: 0.6876 - acc: 0.8006 - 101ms/step\n",
      "step 240/251 - loss: 0.7781 - acc: 0.8000 - 101ms/step\n",
      "step 250/251 - loss: 0.7671 - acc: 0.8008 - 100ms/step\n",
      "step 251/251 - loss: 0.7287 - acc: 0.8009 - 100ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/5\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7964 - acc: 0.7422 - 95ms/step\n",
      "step 20/31 - loss: 0.7612 - acc: 0.7461 - 82ms/step\n",
      "step 30/31 - loss: 0.7823 - acc: 0.7495 - 70ms/step\n",
      "step 31/31 - loss: 0.8443 - acc: 0.7470 - 68ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 7/10\n",
      "step  10/251 - loss: 0.7540 - acc: 0.7937 - 126ms/step\n",
      "step  20/251 - loss: 0.8040 - acc: 0.7988 - 111ms/step\n",
      "step  30/251 - loss: 0.7173 - acc: 0.8039 - 108ms/step\n",
      "step  40/251 - loss: 0.7163 - acc: 0.8082 - 112ms/step\n",
      "step  50/251 - loss: 0.7062 - acc: 0.8077 - 109ms/step\n",
      "step  60/251 - loss: 0.7524 - acc: 0.8064 - 108ms/step\n",
      "step  70/251 - loss: 0.7096 - acc: 0.8075 - 107ms/step\n",
      "step  80/251 - loss: 0.7243 - acc: 0.8102 - 106ms/step\n",
      "step  90/251 - loss: 0.7427 - acc: 0.8124 - 105ms/step\n",
      "step 100/251 - loss: 0.6876 - acc: 0.8161 - 105ms/step\n",
      "step 110/251 - loss: 0.7664 - acc: 0.8159 - 106ms/step\n",
      "step 120/251 - loss: 0.7565 - acc: 0.8171 - 108ms/step\n",
      "step 130/251 - loss: 0.7391 - acc: 0.8177 - 107ms/step\n",
      "step 140/251 - loss: 0.7364 - acc: 0.8177 - 107ms/step\n",
      "step 150/251 - loss: 0.7215 - acc: 0.8177 - 107ms/step\n",
      "step 160/251 - loss: 0.6838 - acc: 0.8183 - 107ms/step\n",
      "step 170/251 - loss: 0.7025 - acc: 0.8189 - 106ms/step\n",
      "step 180/251 - loss: 0.7374 - acc: 0.8197 - 106ms/step\n",
      "step 190/251 - loss: 0.7501 - acc: 0.8196 - 105ms/step\n",
      "step 200/251 - loss: 0.6985 - acc: 0.8209 - 105ms/step\n",
      "step 210/251 - loss: 0.7323 - acc: 0.8212 - 104ms/step\n",
      "step 220/251 - loss: 0.6903 - acc: 0.8228 - 104ms/step\n",
      "step 230/251 - loss: 0.6622 - acc: 0.8242 - 104ms/step\n",
      "step 240/251 - loss: 0.7311 - acc: 0.8240 - 103ms/step\n",
      "step 250/251 - loss: 0.7440 - acc: 0.8246 - 103ms/step\n",
      "step 251/251 - loss: 0.7331 - acc: 0.8246 - 102ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7833 - acc: 0.7555 - 94ms/step\n",
      "step 20/31 - loss: 0.7570 - acc: 0.7547 - 80ms/step\n",
      "step 30/31 - loss: 0.7704 - acc: 0.7568 - 69ms/step\n",
      "step 31/31 - loss: 0.8309 - acc: 0.7548 - 68ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 8/10\n",
      "step  10/251 - loss: 0.7367 - acc: 0.8320 - 125ms/step\n",
      "step  20/251 - loss: 0.7885 - acc: 0.8254 - 106ms/step\n",
      "step  30/251 - loss: 0.7068 - acc: 0.8284 - 105ms/step\n",
      "step  40/251 - loss: 0.7082 - acc: 0.8305 - 102ms/step\n",
      "step  50/251 - loss: 0.6843 - acc: 0.8302 - 101ms/step\n",
      "step  60/251 - loss: 0.7323 - acc: 0.8276 - 102ms/step\n",
      "step  70/251 - loss: 0.7009 - acc: 0.8276 - 101ms/step\n",
      "step  80/251 - loss: 0.6998 - acc: 0.8291 - 100ms/step\n",
      "step  90/251 - loss: 0.7248 - acc: 0.8320 - 100ms/step\n",
      "step 100/251 - loss: 0.6764 - acc: 0.8343 - 100ms/step\n",
      "step 110/251 - loss: 0.7516 - acc: 0.8335 - 99ms/step\n",
      "step 120/251 - loss: 0.7372 - acc: 0.8345 - 99ms/step\n",
      "step 130/251 - loss: 0.7253 - acc: 0.8352 - 101ms/step\n",
      "step 140/251 - loss: 0.7273 - acc: 0.8352 - 102ms/step\n",
      "step 150/251 - loss: 0.7188 - acc: 0.8347 - 102ms/step\n",
      "step 160/251 - loss: 0.6847 - acc: 0.8345 - 102ms/step\n",
      "step 170/251 - loss: 0.6768 - acc: 0.8341 - 102ms/step\n",
      "step 180/251 - loss: 0.7263 - acc: 0.8345 - 101ms/step\n",
      "step 190/251 - loss: 0.7302 - acc: 0.8352 - 101ms/step\n",
      "step 200/251 - loss: 0.6780 - acc: 0.8366 - 101ms/step\n",
      "step 210/251 - loss: 0.7204 - acc: 0.8369 - 101ms/step\n",
      "step 220/251 - loss: 0.6643 - acc: 0.8382 - 101ms/step\n",
      "step 230/251 - loss: 0.6435 - acc: 0.8395 - 100ms/step\n",
      "step 240/251 - loss: 0.7221 - acc: 0.8392 - 100ms/step\n",
      "step 250/251 - loss: 0.7203 - acc: 0.8398 - 99ms/step\n",
      "step 251/251 - loss: 0.7128 - acc: 0.8398 - 99ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7853 - acc: 0.7508 - 84ms/step\n",
      "step 20/31 - loss: 0.7617 - acc: 0.7512 - 87ms/step\n",
      "step 30/31 - loss: 0.7712 - acc: 0.7573 - 74ms/step\n",
      "step 31/31 - loss: 0.8281 - acc: 0.7555 - 73ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 9/10\n",
      "step  10/251 - loss: 0.7239 - acc: 0.8398 - 128ms/step\n",
      "step  20/251 - loss: 0.7728 - acc: 0.8371 - 114ms/step\n",
      "step  30/251 - loss: 0.6961 - acc: 0.8411 - 108ms/step\n",
      "step  40/251 - loss: 0.6880 - acc: 0.8434 - 105ms/step\n",
      "step  50/251 - loss: 0.6842 - acc: 0.8431 - 104ms/step\n",
      "step  60/251 - loss: 0.7287 - acc: 0.8406 - 103ms/step\n",
      "step  70/251 - loss: 0.6957 - acc: 0.8391 - 102ms/step\n",
      "step  80/251 - loss: 0.6931 - acc: 0.8397 - 101ms/step\n",
      "step  90/251 - loss: 0.7191 - acc: 0.8425 - 101ms/step\n",
      "step 100/251 - loss: 0.6640 - acc: 0.8453 - 101ms/step\n",
      "step 110/251 - loss: 0.7418 - acc: 0.8442 - 101ms/step\n",
      "step 120/251 - loss: 0.7270 - acc: 0.8448 - 101ms/step\n",
      "step 130/251 - loss: 0.7160 - acc: 0.8460 - 101ms/step\n",
      "step 140/251 - loss: 0.7465 - acc: 0.8455 - 100ms/step\n",
      "step 150/251 - loss: 0.7028 - acc: 0.8446 - 100ms/step\n",
      "step 160/251 - loss: 0.7027 - acc: 0.8409 - 101ms/step\n",
      "step 170/251 - loss: 0.7227 - acc: 0.8369 - 102ms/step\n",
      "step 180/251 - loss: 0.7708 - acc: 0.8350 - 101ms/step\n",
      "step 190/251 - loss: 0.7576 - acc: 0.8340 - 101ms/step\n",
      "step 200/251 - loss: 0.6930 - acc: 0.8337 - 101ms/step\n",
      "step 210/251 - loss: 0.7304 - acc: 0.8332 - 101ms/step\n",
      "step 220/251 - loss: 0.6819 - acc: 0.8337 - 101ms/step\n",
      "step 230/251 - loss: 0.6598 - acc: 0.8345 - 101ms/step\n",
      "step 240/251 - loss: 0.7296 - acc: 0.8340 - 101ms/step\n",
      "step 250/251 - loss: 0.7361 - acc: 0.8343 - 101ms/step\n",
      "step 251/251 - loss: 0.7373 - acc: 0.8343 - 101ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7918 - acc: 0.7305 - 97ms/step\n",
      "step 20/31 - loss: 0.7632 - acc: 0.7363 - 81ms/step\n",
      "step 30/31 - loss: 0.7533 - acc: 0.7438 - 70ms/step\n",
      "step 31/31 - loss: 0.8495 - acc: 0.7419 - 69ms/step\n",
      "Eval samples: 3968\n",
      "Epoch 10/10\n",
      "step  10/251 - loss: 0.7258 - acc: 0.8414 - 122ms/step\n",
      "step  20/251 - loss: 0.7754 - acc: 0.8371 - 106ms/step\n",
      "step  30/251 - loss: 0.7024 - acc: 0.8357 - 102ms/step\n",
      "step  40/251 - loss: 0.6817 - acc: 0.8389 - 97ms/step\n",
      "step  50/251 - loss: 0.6731 - acc: 0.8397 - 98ms/step\n",
      "step  60/251 - loss: 0.7146 - acc: 0.8389 - 98ms/step\n",
      "step  70/251 - loss: 0.6878 - acc: 0.8390 - 97ms/step\n",
      "step  80/251 - loss: 0.6816 - acc: 0.8419 - 100ms/step\n",
      "step  90/251 - loss: 0.7162 - acc: 0.8446 - 99ms/step\n",
      "step 100/251 - loss: 0.6605 - acc: 0.8472 - 99ms/step\n",
      "step 110/251 - loss: 0.7352 - acc: 0.8471 - 100ms/step\n",
      "step 120/251 - loss: 0.7143 - acc: 0.8488 - 100ms/step\n",
      "step 130/251 - loss: 0.7051 - acc: 0.8501 - 100ms/step\n",
      "step 140/251 - loss: 0.7252 - acc: 0.8503 - 100ms/step\n",
      "step 150/251 - loss: 0.7158 - acc: 0.8498 - 100ms/step\n",
      "step 160/251 - loss: 0.6723 - acc: 0.8494 - 100ms/step\n",
      "step 170/251 - loss: 0.6661 - acc: 0.8492 - 100ms/step\n",
      "step 180/251 - loss: 0.7101 - acc: 0.8498 - 100ms/step\n",
      "step 190/251 - loss: 0.7284 - acc: 0.8497 - 100ms/step\n",
      "step 200/251 - loss: 0.6919 - acc: 0.8504 - 99ms/step\n",
      "step 210/251 - loss: 0.7132 - acc: 0.8508 - 99ms/step\n",
      "step 220/251 - loss: 0.6373 - acc: 0.8518 - 99ms/step\n",
      "step 230/251 - loss: 0.6593 - acc: 0.8529 - 99ms/step\n",
      "step 240/251 - loss: 0.7084 - acc: 0.8527 - 99ms/step\n",
      "step 250/251 - loss: 0.7082 - acc: 0.8530 - 99ms/step\n",
      "step 251/251 - loss: 0.6909 - acc: 0.8531 - 98ms/step\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7828 - acc: 0.7445 - 98ms/step\n",
      "step 20/31 - loss: 0.7629 - acc: 0.7457 - 84ms/step\n",
      "step 30/31 - loss: 0.7550 - acc: 0.7536 - 73ms/step\n",
      "step 31/31 - loss: 0.8319 - acc: 0.7523 - 72ms/step\n",
      "Eval samples: 3968\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, \r\n",
    "            dev_loader,\r\n",
    "            epochs=10, \r\n",
    "            save_dir='./checkpoints',\r\n",
    "            save_freq=5, \r\n",
    "            callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 启动VisualDL查看训练过程可视化结果\n",
    "启动步骤：\n",
    "- 1、切换到本界面左侧「可视化」\n",
    "- 2、日志文件路径选择 'visualdl'\n",
    "- 3、点击「启动VisualDL」后点击「打开VisualDL」，即可查看可视化结果：\n",
    "Accuracy和Loss的实时变化趋势如下：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cb5dff1e17e2407f91b83a1faabd09b4aaa7daac50f44d74a903a576452fbd09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/31 - loss: 0.7828 - acc: 0.7445 - 93ms/step\n",
      "step 20/31 - loss: 0.7629 - acc: 0.7457 - 79ms/step\n",
      "step 30/31 - loss: 0.7550 - acc: 0.7536 - 68ms/step\n",
      "step 31/31 - loss: 0.8319 - acc: 0.7523 - 66ms/step\n",
      "Eval samples: 3968\n",
      "Finally test acc: 0.75227\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dev_loader)\r\n",
    "print(\"Finally test acc: %.5f\" % results['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 32/32 [==============================] - ETA: 3s - 127ms/st - ETA: 3s - 119ms/st - ETA: 2s - 104ms/st - ETA: 2s - 107ms/st - ETA: 2s - 106ms/st - ETA: 2s - 102ms/st - ETA: 1s - 97ms/step - ETA: 1s - 94ms/ste - ETA: 1s - 91ms/ste - ETA: 1s - 88ms/ste - ETA: 0s - 86ms/ste - ETA: 0s - 84ms/ste - ETA: 0s - 83ms/ste - ETA: 0s - 79ms/ste - ETA: 0s - 76ms/ste - 72ms/step          \n",
      "Predict samples: 3986\n",
      "Data: 出品与环境都算可以吧，服务亦过得去。 \t Label: neutral\n",
      "Data: 菜真是一般般滴，除了贵没啥优点… \t Label: negative\n",
      "Data: 第一次是朋友约的喝茶，两层楼都是坐满满客人，我们一行四个人，东西味道很正宗的广式茶点，最爱红米肠，还有乳鸽...网上团购真心觉得划算，在惠福路靠近北京路步行街总觉得东西会很贵，买单时让我感觉很意外四个人才一百多两百，旁边的一些客人也在说好划算哦，因为味道正宗，服务好，性价比高，这就是它为什么现在很多人选择它的原因，朋友家人聚餐的又一个不错的选择... \t Label: neutral\n",
      "Data: 抱住期待来…有d失望咯…点左bb猪，乳鸽松，上汤豆苗，薄撑，燕窝鹧鸪粥bb猪一般，无好好吃乳鸽松其实不错，不过实在太咸啦！！！！而且好多味精！吃完点饮水都唔够！上汤豆苗d豆苗唔知系乜豆苗，d汤底几好，不过都系咸左小小薄撑边系得得地啊，系唔多得！皮不够烟韧，又唔够脆，特別系在上边的，馅糖太多，花生系咸的！！！！！燕窝粥我无食…环境ok，就系有点嘈。服务一般，不太值10%。总结一句无乜动力令我再帮衬，实在太咸太甜啦！！！！！我宜家仲想买支水一口气喝掉半瓶！！！！！ \t Label: neutral\n",
      "Data: 点了一个小时的单，还没有上，上了的东西还是生的，跟服务员理论还一脸臭相。差！ \t Label: negative\n",
      "Data: 服务贴心。不过鲍鱼没有想象中的那么大，团购划算不少！每人一份感觉挺丰盛的。已经团了两次了，自己觉得好还带家人去尝试。 \t Label: positive\n",
      "Data: 环境可以就是吃饭时间人比较多，出品比较精致分量男生可能少点 \t Label: neutral\n",
      "Data: 差评！非繁忙时段，两位不可以坐卡座，卡座还有一大片是空着的，二人桌只剩一张而且对着门口不想坐，看到别人两位可以坐卡座，就我们两位不可以坐！差评！果断走！不是只有你一间吃饭的！ \t Label: negative\n",
      "Data: 好久都没去过广州塔和珠江边附近走走了，又刚好有个灯光节，在加上抽奖抽中了穿粤传奇的【4D魔幻灯光秀】，就必须要过去看看咯，感受一下灯光节的夜景??不知不觉看完4D魔幻灯光秀都快到九点了，就不如来个宵夜吧。走着走着看到了广州塔下面，有一件叫做赏点点心喝茶的好地方。哈哈…于是我们两个人决定去那吃宵夜咯，增肥的节奏哇！一进去很有广州喝茶的感觉，连装修风格也很像之前茶酒的感觉，好喜欢这种风格哦，不错不错。我们一坐下就看了菜单，看到价格都不是很贵，还是挺实惠的。早上是十一点前埋单，还有得打折呢。我们两个人一共点了五样食物。说真的，他家的出品还真的不错，和我经常去的“点都德”相比来说，他家的份量多点，味道也好点，有一些价格上还便宜先。不管白天还是晚上，喝完茶以后，走走珠江边，看看广州塔，还真是个不错的选择。 \t Label: positive\n",
      "Data: 晚市点了一条鱼，一份翡翠饺子，一个咖喱牛筋。饺子很一般很一般，鱼很难吃，牛筋少且难吃。两个人花了178没一道菜及格的也是醉了，因为太难吃了到最后菜都没吃完，不知道是不是我们太***丝了到这里都要点上百的菜。反正除了环境好，感觉就只剩招牌了。 \t Label: negative\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: 'negative', 1: 'neutral',2:'positive'}\r\n",
    "results = model.predict(test_loader, batch_size=128)[0]\r\n",
    "predictions = []\r\n",
    "\r\n",
    "for batch_probs in results:\r\n",
    "    # 映射分类label\r\n",
    "    idx = np.argmax(batch_probs, axis=-1)\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [label_map[i] for i in idx]\r\n",
    "    predictions.extend(labels)\r\n",
    "\r\n",
    "# 看看预测数据前5个样例分类结果\r\n",
    "for idx, data in enumerate(test_ds.data[:10]):\r\n",
    "    print('Data: {} \\t Label: {}'.format(data[0], predictions[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里只采用了一个基础的模型，就得到了较高的的准确率。\n",
    "\n",
    "可以试试预训练模型，能得到更好的效果！参考[如何通过预训练模型Fine-tune下游任务](https://aistudio.baidu.com/aistudio/projectdetail/1294333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PaddleNLP 更多项目\n",
    "\n",
    " - [瞧瞧怎么使用PaddleNLP内置数据集-基于seq2vec的情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    " - [如何通过预训练模型Fine-tune下游任务](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    " - [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    " - [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    " - [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    " - [使用预训练模型ERNIE-GEN实现智能写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    " - [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    " - [使用预训练模型完成阅读理解](https://aistudio.baidu.com/aistudio/projectdetail/1339612)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
